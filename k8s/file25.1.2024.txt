* 
* ==> Audit <==
* |---------|--------------------------------------------------------------|------------|----------|---------|-------------------------------|-------------------------------|
| Command |                             Args                             |  Profile   |   User   | Version |          Start Time           |           End Time            |
|---------|--------------------------------------------------------------|------------|----------|---------|-------------------------------|-------------------------------|
| delete  | --profile my-cluster                                         | my-cluster | BM\bm10' | v1.24.0 | Mon, 22 Jan 2024 13:14:17 +03 | Mon, 22 Jan 2024 13:14:33 +03 |
| start   |                                                              | minikube   | BM\bm10' | v1.24.0 | Mon, 22 Jan 2024 13:14:44 +03 | Mon, 22 Jan 2024 13:15:47 +03 |
| node    | add --worker                                                 | minikube   | BM\bm10' | v1.24.0 | Mon, 22 Jan 2024 13:19:01 +03 | Mon, 22 Jan 2024 13:19:54 +03 |
| node    | add --worker                                                 | minikube   | BM\bm10' | v1.24.0 | Mon, 22 Jan 2024 13:20:08 +03 | Mon, 22 Jan 2024 13:21:00 +03 |
| stop    |                                                              | minikube   | BM\bm10' | v1.24.0 | Mon, 22 Jan 2024 14:16:38 +03 | Mon, 22 Jan 2024 14:17:01 +03 |
| stop    |                                                              | minikube   | BM\bm10' | v1.24.0 | Mon, 22 Jan 2024 15:27:18 +03 | Mon, 22 Jan 2024 15:27:22 +03 |
| start   |                                                              | minikube   | BM\bm10' | v1.24.0 | Mon, 22 Jan 2024 20:09:17 +03 | Mon, 22 Jan 2024 20:10:44 +03 |
| stop    |                                                              | minikube   | BM\bm10' | v1.24.0 | Tue, 23 Jan 2024 01:32:16 +03 | Tue, 23 Jan 2024 01:32:43 +03 |
| start   |                                                              | minikube   | BM\bm10' | v1.24.0 | Tue, 23 Jan 2024 07:39:06 +03 | Tue, 23 Jan 2024 07:40:15 +03 |
| addons  | list                                                         | minikube   | BM\bm10' | v1.24.0 | Tue, 23 Jan 2024 08:20:47 +03 | Tue, 23 Jan 2024 08:20:48 +03 |
| addons  | enable ingress                                               | minikube   | BM\bm10' | v1.24.0 | Tue, 23 Jan 2024 09:32:40 +03 | Tue, 23 Jan 2024 09:32:41 +03 |
| addons  | list                                                         | minikube   | BM\bm10' | v1.24.0 | Tue, 23 Jan 2024 09:32:53 +03 | Tue, 23 Jan 2024 09:32:53 +03 |
| addons  | enable ingress-dns                                           | minikube   | BM\bm10' | v1.24.0 | Tue, 23 Jan 2024 09:33:18 +03 | Tue, 23 Jan 2024 09:33:18 +03 |
| addons  | list                                                         | minikube   | BM\bm10' | v1.24.0 | Tue, 23 Jan 2024 09:33:22 +03 | Tue, 23 Jan 2024 09:33:22 +03 |
| tunnel  |                                                              | minikube   | BM\bm10' | v1.24.0 | Tue, 23 Jan 2024 09:35:32 +03 | Tue, 23 Jan 2024 09:39:00 +03 |
| start   |                                                              | minikube   | BM\bm10' | v1.24.0 | Tue, 23 Jan 2024 18:45:14 +03 | Tue, 23 Jan 2024 18:49:39 +03 |
| ip      |                                                              | minikube   | BM\bm10' | v1.24.0 | Tue, 23 Jan 2024 23:00:15 +03 | Tue, 23 Jan 2024 23:00:16 +03 |
| tunnel  |                                                              | minikube   | BM\bm10' | v1.24.0 | Tue, 23 Jan 2024 23:24:14 +03 | Tue, 23 Jan 2024 23:24:16 +03 |
| tunnel  |                                                              | minikube   | BM\bm10' | v1.24.0 | Tue, 23 Jan 2024 23:23:17 +03 | Wed, 24 Jan 2024 00:47:02 +03 |
| stop    |                                                              | minikube   | BM\bm10' | v1.24.0 | Wed, 24 Jan 2024 01:42:48 +03 | Wed, 24 Jan 2024 01:43:22 +03 |
| start   |                                                              | minikube   | BM\bm10' | v1.24.0 | Wed, 24 Jan 2024 11:36:09 +03 | Wed, 24 Jan 2024 11:37:48 +03 |
| ssh     |                                                              | minikube   | BM\bm10' | v1.24.0 | Wed, 24 Jan 2024 12:10:31 +03 | Wed, 24 Jan 2024 12:12:45 +03 |
| ssh     |                                                              | minikube   | BM\bm10' | v1.24.0 | Wed, 24 Jan 2024 12:14:47 +03 | Wed, 24 Jan 2024 12:16:55 +03 |
| ssh     |                                                              | minikube   | BM\bm10' | v1.24.0 | Wed, 24 Jan 2024 12:24:25 +03 | Wed, 24 Jan 2024 12:25:13 +03 |
| ssh     |                                                              | minikube   | BM\bm10' | v1.24.0 | Wed, 24 Jan 2024 13:25:28 +03 | Wed, 24 Jan 2024 13:28:12 +03 |
| ssh     | -- sudo touch                                                | minikube   | BM\bm10' | v1.24.0 | Wed, 24 Jan 2024 13:29:35 +03 | Wed, 24 Jan 2024 13:29:36 +03 |
|         | /tmp/hostpath-provisioner/default/database-persistent-volume |            |          |         |                               |                               |
| ssh     |                                                              | minikube   | BM\bm10' | v1.24.0 | Wed, 24 Jan 2024 13:33:48 +03 | Wed, 24 Jan 2024 13:36:22 +03 |
| ssh     |                                                              | minikube   | BM\bm10' | v1.24.0 | Wed, 24 Jan 2024 13:39:20 +03 | Wed, 24 Jan 2024 13:40:14 +03 |
| ssh     |                                                              | minikube   | BM\bm10' | v1.24.0 | Wed, 24 Jan 2024 13:45:49 +03 | Wed, 24 Jan 2024 13:46:38 +03 |
| addons  | list                                                         | minikube   | BM\bm10' | v1.24.0 | Wed, 24 Jan 2024 14:34:38 +03 | Wed, 24 Jan 2024 14:34:38 +03 |
| ssh     |                                                              | minikube   | BM\bm10' | v1.24.0 | Wed, 24 Jan 2024 14:35:35 +03 | Wed, 24 Jan 2024 14:35:58 +03 |
| addons  | list                                                         | minikube   | BM\bm10' | v1.24.0 | Wed, 24 Jan 2024 14:36:38 +03 | Wed, 24 Jan 2024 14:36:39 +03 |
| logs    | --file=logs.txt                                              | minikube   | BM\bm10' | v1.24.0 | Wed, 24 Jan 2024 15:27:24 +03 | Wed, 24 Jan 2024 15:27:30 +03 |
| stop    |                                                              | minikube   | BM\bm10' | v1.24.0 | Wed, 24 Jan 2024 15:30:07 +03 | Wed, 24 Jan 2024 15:30:38 +03 |
| start   |                                                              | minikube   | BM\bm10' | v1.24.0 | Wed, 24 Jan 2024 15:32:53 +03 | Wed, 24 Jan 2024 15:34:33 +03 |
| stop    |                                                              | minikube   | BM\bm10' | v1.24.0 | Wed, 24 Jan 2024 15:55:39 +03 | Wed, 24 Jan 2024 15:56:10 +03 |
| start   |                                                              | minikube   | BM\bm10' | v1.24.0 | Wed, 24 Jan 2024 16:34:11 +03 | Wed, 24 Jan 2024 16:41:32 +03 |
| stop    |                                                              | minikube   | BM\bm10' | v1.24.0 | Wed, 24 Jan 2024 16:54:45 +03 | Wed, 24 Jan 2024 16:55:03 +03 |
| stop    |                                                              | minikube   | BM\bm10' | v1.24.0 | Wed, 24 Jan 2024 17:09:38 +03 | Wed, 24 Jan 2024 17:09:43 +03 |
| start   | --cpus 2 --memory 3586                                       | minikube   | BM\bm10' | v1.24.0 | Wed, 24 Jan 2024 17:26:57 +03 | Wed, 24 Jan 2024 17:34:31 +03 |
| kubectl | -- get pods -A                                               | minikube   | BM\bm10' | v1.24.0 | Wed, 24 Jan 2024 17:34:49 +03 | Wed, 24 Jan 2024 17:34:52 +03 |
| kubectl | -- get pods -A                                               | minikube   | BM\bm10' | v1.24.0 | Wed, 24 Jan 2024 17:35:35 +03 | Wed, 24 Jan 2024 17:35:35 +03 |
| kubectl | -- get pods -n ingress-nginx                                 | minikube   | BM\bm10' | v1.24.0 | Wed, 24 Jan 2024 17:42:49 +03 | Wed, 24 Jan 2024 17:42:50 +03 |
| kubectl | -- get pods -n ingress-nginx                                 | minikube   | BM\bm10' | v1.24.0 | Wed, 24 Jan 2024 17:42:51 +03 | Wed, 24 Jan 2024 17:42:51 +03 |
| kubectl | -- get pods -n ingress-nginx                                 | minikube   | BM\bm10' | v1.24.0 | Wed, 24 Jan 2024 17:42:53 +03 | Wed, 24 Jan 2024 17:42:53 +03 |
| kubectl | -- get pods -n ingress-nginx                                 | minikube   | BM\bm10' | v1.24.0 | Wed, 24 Jan 2024 17:42:54 +03 | Wed, 24 Jan 2024 17:42:54 +03 |
| kubectl | -- get pods                                                  | minikube   | BM\bm10' | v1.24.0 | Wed, 24 Jan 2024 17:43:03 +03 | Wed, 24 Jan 2024 17:43:03 +03 |
| logs    | --file=fordelete.tx                                          | minikube   | BM\bm10' | v1.24.0 | Wed, 24 Jan 2024 17:50:26 +03 | Wed, 24 Jan 2024 17:50:28 +03 |
| stop    |                                                              | minikube   | BM\bm10' | v1.24.0 | Wed, 24 Jan 2024 17:53:59 +03 | Wed, 24 Jan 2024 17:54:15 +03 |
| start   |                                                              | minikube   | BM\bm10' | v1.24.0 | Wed, 24 Jan 2024 17:54:22 +03 | Wed, 24 Jan 2024 18:01:31 +03 |
| kubectl | -- get pods -A                                               | minikube   | BM\bm10' | v1.24.0 | Wed, 24 Jan 2024 18:01:39 +03 | Wed, 24 Jan 2024 18:01:39 +03 |
| kubectl | -- get pods -A                                               | minikube   | BM\bm10' | v1.24.0 | Wed, 24 Jan 2024 18:01:46 +03 | Wed, 24 Jan 2024 18:01:46 +03 |
| kubectl | -- get pods -A                                               | minikube   | BM\bm10' | v1.24.0 | Wed, 24 Jan 2024 18:06:07 +03 | Wed, 24 Jan 2024 18:06:08 +03 |
| kubectl | -- get pods -A                                               | minikube   | BM\bm10' | v1.24.0 | Wed, 24 Jan 2024 18:20:55 +03 | Wed, 24 Jan 2024 18:20:56 +03 |
| addons  | list                                                         | minikube   | BM\bm10' | v1.24.0 | Wed, 24 Jan 2024 22:14:44 +03 | Wed, 24 Jan 2024 22:14:44 +03 |
| start   |                                                              | minikube   | BM\bm10' | v1.24.0 | Thu, 25 Jan 2024 22:12:33 +03 | Thu, 25 Jan 2024 22:38:34 +03 |
| kubectl | -- get pods -A                                               | minikube   | BM\bm10' | v1.24.0 | Thu, 25 Jan 2024 22:39:32 +03 | Thu, 25 Jan 2024 22:39:34 +03 |
| kubectl | -- get pods -A                                               | minikube   | BM\bm10' | v1.24.0 | Thu, 25 Jan 2024 22:39:43 +03 | Thu, 25 Jan 2024 22:39:43 +03 |
| kubectl | -- get pods -A                                               | minikube   | BM\bm10' | v1.24.0 | Thu, 25 Jan 2024 22:40:10 +03 | Thu, 25 Jan 2024 22:40:10 +03 |
| kubectl | -- get nodes                                                 | minikube   | BM\bm10' | v1.24.0 | Thu, 25 Jan 2024 22:46:00 +03 | Thu, 25 Jan 2024 22:46:00 +03 |
|---------|--------------------------------------------------------------|------------|----------|---------|-------------------------------|-------------------------------|

* 
* ==> Last Start <==
* Log file created at: 2024/01/25 22:12:33
Running on machine: bm
Binary: Built with gc go1.17.2 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0125 22:12:33.505139   10760 out.go:297] Setting OutFile to fd 96 ...
I0125 22:12:33.506364   10760 out.go:344] TERM=,COLORTERM=, which probably does not support color
I0125 22:12:33.506364   10760 out.go:310] Setting ErrFile to fd 100...
I0125 22:12:33.506364   10760 out.go:344] TERM=,COLORTERM=, which probably does not support color
I0125 22:12:33.531470   10760 out.go:304] Setting JSON to false
I0125 22:12:33.547760   10760 start.go:112] hostinfo: {"hostname":"bm","uptime":36242,"bootTime":1706173711,"procs":278,"os":"windows","platform":"Microsoft Windows 11 Home","platformFamily":"Standalone Workstation","platformVersion":"10.0.22000 Build 22000","kernelVersion":"10.0.22000 Build 22000","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"01eff853-40d6-48cf-85cb-d1def38168d9"}
W0125 22:12:33.547760   10760 start.go:120] gopshost.Virtualization returned error: not implemented yet
I0125 22:12:33.550976   10760 out.go:176] * minikube v1.24.0 on Microsoft Windows 11 Home 10.0.22000 Build 22000
I0125 22:12:33.551348   10760 notify.go:174] Checking for updates...
I0125 22:12:33.585948   10760 config.go:176] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.22.3
I0125 22:12:33.585948   10760 driver.go:343] Setting default libvirt URI to qemu:///system
I0125 22:12:34.078920   10760 docker.go:132] docker version: linux-24.0.6
I0125 22:12:34.086744   10760 cli_runner.go:115] Run: docker system info --format "{{json .}}"
I0125 22:12:36.434387   10760 cli_runner.go:168] Completed: docker system info --format "{{json .}}": (2.347553s)
I0125 22:12:36.441693   10760 info.go:263] docker info: {ID:f1205269-385f-48cb-9ac9-b24faee81c86 Containers:96 ContainersRunning:0 ContainersPaused:0 ContainersStopped:96 Images:77 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:51 OomKillDisable:true NGoroutines:95 SystemTime:2024-01-25 19:12:36.276780534 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:13 KernelVersion:5.15.90.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:3761356800 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:bm Labels:[] ExperimentalBuild:false ServerVersion:24.0.6 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:8165feabfdfe38c65b599c4993d227328c231fca Expected:8165feabfdfe38c65b599c4993d227328c231fca} RuncCommit:{ID:v1.1.8-0-g82f18fe Expected:v1.1.8-0-g82f18fe} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.2-desktop.5] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.23.0-desktop.1] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.20] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.9] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:C:\Program Files\Docker\cli-plugins\docker-scan.exe SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.0.9]] Warnings:<nil>}}
I0125 22:12:36.448441   10760 out.go:176] * Using the docker driver based on existing profile
I0125 22:12:36.449844   10760 start.go:280] selected driver: docker
I0125 22:12:36.450432   10760 start.go:762] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.28@sha256:4780f1897569d2bf77aafb3d133a08d42b4fe61127f06fcfc90c2c5d902d893c Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.22.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: ExtraOptions:[{Component:kubelet Key:cni-conf-dir Value:/etc/cni/net.mk}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.22.3 ControlPlane:true Worker:true} {Name:m02 IP:192.168.49.3 Port:0 KubernetesVersion:v1.22.3 ControlPlane:false Worker:true} {Name:m03 IP:192.168.49.4 Port:0 KubernetesVersion:v1.22.3 ControlPlane:false Worker:true}] Addons:map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false helm-tiller:false ingress:true ingress-dns:true istio:false istio-provisioner:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[IngressController:ingress-nginx/controller:v1.0.4@sha256:545cff00370f28363dad31e3b59a94ba377854d3a11f18988f5f9e56841ef9ef IngressDNS:k8s-minikube/minikube-ingress-dns:0.0.2@sha256:4abe27f9fc03fedab1d655e2020e6b165faf3bf6de1088ce6cf215a75b78f05f KubeWebhookCertgenCreate:k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660 KubeWebhookCertgenPatch:k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\bm10':/minikube-host}
I0125 22:12:36.450950   10760 start.go:773] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc:}
I0125 22:12:36.470177   10760 cli_runner.go:115] Run: docker system info --format "{{json .}}"
I0125 22:12:36.926155   10760 info.go:263] docker info: {ID:f1205269-385f-48cb-9ac9-b24faee81c86 Containers:96 ContainersRunning:0 ContainersPaused:0 ContainersStopped:96 Images:77 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:51 OomKillDisable:true NGoroutines:95 SystemTime:2024-01-25 19:12:36.844274712 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:13 KernelVersion:5.15.90.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:3761356800 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:bm Labels:[] ExperimentalBuild:false ServerVersion:24.0.6 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:8165feabfdfe38c65b599c4993d227328c231fca Expected:8165feabfdfe38c65b599c4993d227328c231fca} RuncCommit:{ID:v1.1.8-0-g82f18fe Expected:v1.1.8-0-g82f18fe} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.2-desktop.5] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.23.0-desktop.1] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.20] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.9] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:C:\Program Files\Docker\cli-plugins\docker-scan.exe SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.0.9]] Warnings:<nil>}}
I0125 22:12:37.082354   10760 cni.go:93] Creating CNI manager for ""
I0125 22:12:37.082354   10760 cni.go:154] 3 nodes found, recommending kindnet
I0125 22:12:37.082941   10760 start_flags.go:282] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.28@sha256:4780f1897569d2bf77aafb3d133a08d42b4fe61127f06fcfc90c2c5d902d893c Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.22.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: ExtraOptions:[{Component:kubelet Key:cni-conf-dir Value:/etc/cni/net.mk}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.22.3 ControlPlane:true Worker:true} {Name:m02 IP:192.168.49.3 Port:0 KubernetesVersion:v1.22.3 ControlPlane:false Worker:true} {Name:m03 IP:192.168.49.4 Port:0 KubernetesVersion:v1.22.3 ControlPlane:false Worker:true}] Addons:map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false helm-tiller:false ingress:true ingress-dns:true istio:false istio-provisioner:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[IngressController:ingress-nginx/controller:v1.0.4@sha256:545cff00370f28363dad31e3b59a94ba377854d3a11f18988f5f9e56841ef9ef IngressDNS:k8s-minikube/minikube-ingress-dns:0.0.2@sha256:4abe27f9fc03fedab1d655e2020e6b165faf3bf6de1088ce6cf215a75b78f05f KubeWebhookCertgenCreate:k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660 KubeWebhookCertgenPatch:k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\bm10':/minikube-host}
I0125 22:12:37.086330   10760 out.go:176] * Starting control plane node minikube in cluster minikube
I0125 22:12:37.089818   10760 cache.go:118] Beginning downloading kic base image for docker with docker
I0125 22:12:37.093108   10760 out.go:176] * Pulling base image ...
I0125 22:12:37.094879   10760 preload.go:132] Checking if preload exists for k8s version v1.22.3 and runtime docker
I0125 22:12:37.095453   10760 image.go:75] Checking for gcr.io/k8s-minikube/kicbase:v0.0.28@sha256:4780f1897569d2bf77aafb3d133a08d42b4fe61127f06fcfc90c2c5d902d893c in local docker daemon
I0125 22:12:37.097718   10760 preload.go:148] Found local preload: C:\Users\bm10'\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v13-v1.22.3-docker-overlay2-amd64.tar.lz4
I0125 22:12:37.098169   10760 cache.go:57] Caching tarball of preloaded images
I0125 22:12:37.099934   10760 preload.go:174] Found C:\Users\bm10'\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v13-v1.22.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0125 22:12:37.099934   10760 cache.go:60] Finished verifying existence of preloaded tar for  v1.22.3 on docker
I0125 22:12:37.100483   10760 profile.go:147] Saving config to C:\Users\bm10'\.minikube\profiles\minikube\config.json ...
I0125 22:12:37.400682   10760 image.go:79] Found gcr.io/k8s-minikube/kicbase:v0.0.28@sha256:4780f1897569d2bf77aafb3d133a08d42b4fe61127f06fcfc90c2c5d902d893c in local docker daemon, skipping pull
I0125 22:12:37.401403   10760 cache.go:140] gcr.io/k8s-minikube/kicbase:v0.0.28@sha256:4780f1897569d2bf77aafb3d133a08d42b4fe61127f06fcfc90c2c5d902d893c exists in daemon, skipping load
I0125 22:12:37.401403   10760 cache.go:206] Successfully downloaded all kic artifacts
I0125 22:12:37.402991   10760 start.go:313] acquiring machines lock for minikube: {Name:mkabc75bdee574e627846402dc2701ca05094710 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0125 22:12:37.402991   10760 start.go:317] acquired machines lock for "minikube" in 0s
I0125 22:12:37.404332   10760 start.go:93] Skipping create...Using existing machine configuration
I0125 22:12:37.405231   10760 fix.go:55] fixHost starting: 
I0125 22:12:37.416588   10760 cli_runner.go:115] Run: docker container inspect minikube --format={{.State.Status}}
I0125 22:12:37.617270   10760 fix.go:108] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0125 22:12:37.617388   10760 fix.go:134] unexpected machine state, will restart: <nil>
I0125 22:12:37.618934   10760 out.go:176] * Restarting existing docker container for "minikube" ...
I0125 22:12:37.626654   10760 cli_runner.go:115] Run: docker start minikube
I0125 22:12:41.086058   10760 cli_runner.go:168] Completed: docker start minikube: (3.4594039s)
I0125 22:12:41.095979   10760 cli_runner.go:115] Run: docker container inspect minikube --format={{.State.Status}}
I0125 22:12:41.301592   10760 kic.go:420] container "minikube" state is running.
I0125 22:12:41.310883   10760 cli_runner.go:115] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0125 22:12:41.501953   10760 profile.go:147] Saving config to C:\Users\bm10'\.minikube\profiles\minikube\config.json ...
I0125 22:12:41.504472   10760 machine.go:88] provisioning docker machine ...
I0125 22:12:41.504472   10760 ubuntu.go:169] provisioning hostname "minikube"
I0125 22:12:41.510848   10760 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0125 22:12:41.705275   10760 main.go:130] libmachine: Using SSH client type: native
I0125 22:12:41.726547   10760 main.go:130] libmachine: &{{{<nil> 0 [] [] []} docker [0xa29f80] 0xa2ce40 <nil>  [] 0s} 127.0.0.1 52288 <nil> <nil>}
I0125 22:12:41.726547   10760 main.go:130] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0125 22:12:41.767707   10760 main.go:130] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I0125 22:12:45.258580   10760 main.go:130] libmachine: SSH cmd err, output: <nil>: minikube

I0125 22:12:45.276816   10760 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0125 22:12:45.489475   10760 main.go:130] libmachine: Using SSH client type: native
I0125 22:12:45.490044   10760 main.go:130] libmachine: &{{{<nil> 0 [] [] []} docker [0xa29f80] 0xa2ce40 <nil>  [] 0s} 127.0.0.1 52288 <nil> <nil>}
I0125 22:12:45.490044   10760 main.go:130] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0125 22:12:45.691551   10760 main.go:130] libmachine: SSH cmd err, output: <nil>: 
I0125 22:12:45.691613   10760 ubuntu.go:175] set auth options {CertDir:C:\Users\bm10'\.minikube CaCertPath:C:\Users\bm10'\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\bm10'\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\bm10'\.minikube\machines\server.pem ServerKeyPath:C:\Users\bm10'\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\bm10'\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\bm10'\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\bm10'\.minikube}
I0125 22:12:45.691613   10760 ubuntu.go:177] setting up certificates
I0125 22:12:45.692226   10760 provision.go:83] configureAuth start
I0125 22:12:45.698435   10760 cli_runner.go:115] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0125 22:12:45.901563   10760 provision.go:138] copyHostCerts
I0125 22:12:45.919170   10760 exec_runner.go:144] found C:\Users\bm10'\.minikube/ca.pem, removing ...
I0125 22:12:45.919170   10760 exec_runner.go:207] rm: C:\Users\bm10'\.minikube\ca.pem
I0125 22:12:45.919726   10760 exec_runner.go:151] cp: C:\Users\bm10'\.minikube\certs\ca.pem --> C:\Users\bm10'\.minikube/ca.pem (1074 bytes)
I0125 22:12:45.937020   10760 exec_runner.go:144] found C:\Users\bm10'\.minikube/cert.pem, removing ...
I0125 22:12:45.937020   10760 exec_runner.go:207] rm: C:\Users\bm10'\.minikube\cert.pem
I0125 22:12:45.937020   10760 exec_runner.go:151] cp: C:\Users\bm10'\.minikube\certs\cert.pem --> C:\Users\bm10'\.minikube/cert.pem (1119 bytes)
I0125 22:12:45.950578   10760 exec_runner.go:144] found C:\Users\bm10'\.minikube/key.pem, removing ...
I0125 22:12:45.951092   10760 exec_runner.go:207] rm: C:\Users\bm10'\.minikube\key.pem
I0125 22:12:45.951174   10760 exec_runner.go:151] cp: C:\Users\bm10'\.minikube\certs\key.pem --> C:\Users\bm10'\.minikube/key.pem (1675 bytes)
I0125 22:12:45.952254   10760 provision.go:112] generating server cert: C:\Users\bm10'\.minikube\machines\server.pem ca-key=C:\Users\bm10'\.minikube\certs\ca.pem private-key=C:\Users\bm10'\.minikube\certs\ca-key.pem org=bm10'.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I0125 22:12:46.057710   10760 provision.go:172] copyRemoteCerts
I0125 22:12:46.069633   10760 ssh_runner.go:152] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0125 22:12:46.077198   10760 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0125 22:12:46.305777   10760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52288 SSHKeyPath:C:\Users\bm10'\.minikube\machines\minikube\id_rsa Username:docker}
I0125 22:12:46.734118   10760 ssh_runner.go:319] scp C:\Users\bm10'\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1074 bytes)
I0125 22:12:46.866000   10760 ssh_runner.go:319] scp C:\Users\bm10'\.minikube\machines\server.pem --> /etc/docker/server.pem (1196 bytes)
I0125 22:12:46.951385   10760 ssh_runner.go:319] scp C:\Users\bm10'\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0125 22:12:47.032944   10760 provision.go:86] duration metric: configureAuth took 1.3406629s
I0125 22:12:47.032944   10760 ubuntu.go:193] setting minikube options for container-runtime
I0125 22:12:47.034121   10760 config.go:176] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.22.3
I0125 22:12:47.041698   10760 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0125 22:12:47.230980   10760 main.go:130] libmachine: Using SSH client type: native
I0125 22:12:47.237460   10760 main.go:130] libmachine: &{{{<nil> 0 [] [] []} docker [0xa29f80] 0xa2ce40 <nil>  [] 0s} 127.0.0.1 52288 <nil> <nil>}
I0125 22:12:47.237460   10760 main.go:130] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0125 22:12:47.444527   10760 main.go:130] libmachine: SSH cmd err, output: <nil>: overlay

I0125 22:12:47.445114   10760 ubuntu.go:71] root file system type: overlay
I0125 22:12:47.445634   10760 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0125 22:12:47.452437   10760 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0125 22:12:47.630037   10760 main.go:130] libmachine: Using SSH client type: native
I0125 22:12:47.630037   10760 main.go:130] libmachine: &{{{<nil> 0 [] [] []} docker [0xa29f80] 0xa2ce40 <nil>  [] 0s} 127.0.0.1 52288 <nil> <nil>}
I0125 22:12:47.630037   10760 main.go:130] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0125 22:12:47.847673   10760 main.go:130] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0125 22:12:47.858364   10760 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0125 22:12:48.046011   10760 main.go:130] libmachine: Using SSH client type: native
I0125 22:12:48.046011   10760 main.go:130] libmachine: &{{{<nil> 0 [] [] []} docker [0xa29f80] 0xa2ce40 <nil>  [] 0s} 127.0.0.1 52288 <nil> <nil>}
I0125 22:12:48.046570   10760 main.go:130] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0125 22:12:48.226773   10760 main.go:130] libmachine: SSH cmd err, output: <nil>: 
I0125 22:12:48.226773   10760 machine.go:91] provisioned docker machine in 6.7223015s
I0125 22:12:48.226833   10760 start.go:267] post-start starting for "minikube" (driver="docker")
I0125 22:12:48.226833   10760 start.go:277] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0125 22:12:48.237991   10760 ssh_runner.go:152] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0125 22:12:48.244280   10760 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0125 22:12:48.442300   10760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52288 SSHKeyPath:C:\Users\bm10'\.minikube\machines\minikube\id_rsa Username:docker}
I0125 22:12:48.578339   10760 ssh_runner.go:152] Run: cat /etc/os-release
I0125 22:12:48.593787   10760 main.go:130] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0125 22:12:48.593787   10760 main.go:130] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0125 22:12:48.593787   10760 main.go:130] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0125 22:12:48.594352   10760 info.go:137] Remote host: Ubuntu 20.04.2 LTS
I0125 22:12:48.594926   10760 filesync.go:126] Scanning C:\Users\bm10'\.minikube\addons for local assets ...
I0125 22:12:48.594926   10760 filesync.go:126] Scanning C:\Users\bm10'\.minikube\files for local assets ...
I0125 22:12:48.595480   10760 start.go:270] post-start completed in 368.6477ms
I0125 22:12:48.605489   10760 ssh_runner.go:152] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0125 22:12:48.610287   10760 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0125 22:12:48.795446   10760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52288 SSHKeyPath:C:\Users\bm10'\.minikube\machines\minikube\id_rsa Username:docker}
I0125 22:12:48.912881   10760 fix.go:57] fixHost completed within 11.50765s
I0125 22:12:48.912881   10760 start.go:80] releasing machines lock for "minikube", held for 11.5098906s
I0125 22:12:48.918949   10760 cli_runner.go:115] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0125 22:12:49.118696   10760 ssh_runner.go:152] Run: curl -sS -m 2 https://k8s.gcr.io/
I0125 22:12:49.126638   10760 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0125 22:12:49.128983   10760 ssh_runner.go:152] Run: systemctl --version
I0125 22:12:49.136566   10760 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0125 22:12:49.330407   10760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52288 SSHKeyPath:C:\Users\bm10'\.minikube\machines\minikube\id_rsa Username:docker}
I0125 22:12:49.345495   10760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52288 SSHKeyPath:C:\Users\bm10'\.minikube\machines\minikube\id_rsa Username:docker}
I0125 22:12:49.456450   10760 ssh_runner.go:152] Run: sudo systemctl is-active --quiet service containerd
I0125 22:12:50.954099   10760 ssh_runner.go:192] Completed: curl -sS -m 2 https://k8s.gcr.io/: (1.8354031s)
I0125 22:12:50.954099   10760 ssh_runner.go:192] Completed: sudo systemctl is-active --quiet service containerd: (1.497649s)
I0125 22:12:50.963029   10760 ssh_runner.go:152] Run: sudo systemctl cat docker.service
I0125 22:12:51.000544   10760 cruntime.go:255] skipping containerd shutdown because we are bound to it
I0125 22:12:51.012177   10760 ssh_runner.go:152] Run: sudo systemctl is-active --quiet service crio
I0125 22:12:51.051127   10760 ssh_runner.go:152] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/dockershim.sock
image-endpoint: unix:///var/run/dockershim.sock
" | sudo tee /etc/crictl.yaml"
I0125 22:12:51.134060   10760 ssh_runner.go:152] Run: sudo systemctl unmask docker.service
I0125 22:12:51.405066   10760 ssh_runner.go:152] Run: sudo systemctl enable docker.socket
I0125 22:12:51.604668   10760 ssh_runner.go:152] Run: sudo systemctl cat docker.service
I0125 22:12:51.643392   10760 ssh_runner.go:152] Run: sudo systemctl daemon-reload
I0125 22:12:51.811492   10760 ssh_runner.go:152] Run: sudo systemctl start docker
I0125 22:12:51.852831   10760 ssh_runner.go:152] Run: docker version --format {{.Server.Version}}
I0125 22:12:53.384697   10760 ssh_runner.go:192] Completed: docker version --format {{.Server.Version}}: (1.5318657s)
I0125 22:12:53.393064   10760 ssh_runner.go:152] Run: docker version --format {{.Server.Version}}
I0125 22:12:53.603874   10760 out.go:203] * Preparing Kubernetes v1.22.3 on Docker 20.10.8 ...
I0125 22:12:53.610292   10760 cli_runner.go:115] Run: docker exec -t minikube dig +short host.docker.internal
I0125 22:12:54.095006   10760 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0125 22:12:54.102881   10760 ssh_runner.go:152] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0125 22:12:54.112663   10760 ssh_runner.go:152] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0125 22:12:54.156029   10760 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0125 22:12:54.343969   10760 preload.go:132] Checking if preload exists for k8s version v1.22.3 and runtime docker
I0125 22:12:54.349157   10760 ssh_runner.go:152] Run: docker images --format {{.Repository}}:{{.Tag}}
I0125 22:12:54.645054   10760 docker.go:558] Got preloaded images: -- stdout --
mo1074/react:v2
mo1074/node:v1
postgres:latest
k8s.gcr.io/kube-apiserver:v1.22.3
k8s.gcr.io/kube-scheduler:v1.22.3
k8s.gcr.io/kube-controller-manager:v1.22.3
k8s.gcr.io/kube-proxy:v1.22.3
k8s.gcr.io/ingress-nginx/controller:<none>
kubernetesui/dashboard:v2.3.1
k8s.gcr.io/etcd:3.5.0-0
kubernetesui/metrics-scraper:v1.0.7
k8s.gcr.io/coredns/coredns:v1.8.4
gcr.io/k8s-minikube/storage-provisioner:v5
kindest/kindnetd:v20210326-1e038dc5
k8s.gcr.io/pause:3.5

-- /stdout --
I0125 22:12:54.645909   10760 docker.go:489] Images already preloaded, skipping extraction
I0125 22:12:54.652931   10760 ssh_runner.go:152] Run: docker images --format {{.Repository}}:{{.Tag}}
I0125 22:12:54.784407   10760 docker.go:558] Got preloaded images: -- stdout --
mo1074/react:v2
mo1074/node:v1
postgres:latest
k8s.gcr.io/kube-apiserver:v1.22.3
k8s.gcr.io/kube-scheduler:v1.22.3
k8s.gcr.io/kube-controller-manager:v1.22.3
k8s.gcr.io/kube-proxy:v1.22.3
k8s.gcr.io/ingress-nginx/controller:<none>
kubernetesui/dashboard:v2.3.1
k8s.gcr.io/etcd:3.5.0-0
kubernetesui/metrics-scraper:v1.0.7
k8s.gcr.io/coredns/coredns:v1.8.4
gcr.io/k8s-minikube/storage-provisioner:v5
kindest/kindnetd:v20210326-1e038dc5
k8s.gcr.io/pause:3.5

-- /stdout --
I0125 22:12:54.784965   10760 cache_images.go:79] Images are preloaded, skipping loading
I0125 22:12:54.791450   10760 ssh_runner.go:152] Run: docker info --format {{.CgroupDriver}}
I0125 22:12:57.014924   10760 ssh_runner.go:192] Completed: docker info --format {{.CgroupDriver}}: (2.2234734s)
I0125 22:12:57.015437   10760 cni.go:93] Creating CNI manager for ""
I0125 22:12:57.015437   10760 cni.go:154] 3 nodes found, recommending kindnet
I0125 22:12:57.016589   10760 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0125 22:12:57.016589   10760 kubeadm.go:153] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.22.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/dockershim.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NoTaintMaster:true NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[]}
I0125 22:12:57.017153   10760 kubeadm.go:157] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta2
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.22.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0125 22:12:57.017803   10760 kubeadm.go:909] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.22.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --cni-conf-dir=/etc/cni/net.mk --config=/var/lib/kubelet/config.yaml --container-runtime=docker --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.22.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: ExtraOptions:[{Component:kubelet Key:cni-conf-dir Value:/etc/cni/net.mk}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0125 22:12:57.026624   10760 ssh_runner.go:152] Run: sudo ls /var/lib/minikube/binaries/v1.22.3
I0125 22:12:57.066758   10760 binaries.go:44] Found k8s binaries, skipping transfer
I0125 22:12:57.076084   10760 ssh_runner.go:152] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0125 22:12:57.107118   10760 ssh_runner.go:319] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (365 bytes)
I0125 22:12:57.154766   10760 ssh_runner.go:319] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0125 22:12:57.213024   10760 ssh_runner.go:319] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2051 bytes)
I0125 22:12:57.295799   10760 ssh_runner.go:152] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0125 22:12:57.308002   10760 ssh_runner.go:152] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0125 22:12:57.354823   10760 certs.go:54] Setting up C:\Users\bm10'\.minikube\profiles\minikube for IP: 192.168.49.2
I0125 22:12:57.382099   10760 certs.go:182] skipping minikubeCA CA generation: C:\Users\bm10'\.minikube\ca.key
I0125 22:12:57.411160   10760 certs.go:182] skipping proxyClientCA CA generation: C:\Users\bm10'\.minikube\proxy-client-ca.key
I0125 22:12:57.412403   10760 certs.go:298] skipping minikube-user signed cert generation: C:\Users\bm10'\.minikube\profiles\minikube\client.key
I0125 22:12:57.441856   10760 certs.go:298] skipping minikube signed cert generation: C:\Users\bm10'\.minikube\profiles\minikube\apiserver.key.dd3b5fb2
I0125 22:12:57.472562   10760 certs.go:298] skipping aggregator signed cert generation: C:\Users\bm10'\.minikube\profiles\minikube\proxy-client.key
I0125 22:12:57.476951   10760 certs.go:388] found cert: C:\Users\bm10'\.minikube\certs\C:\Users\bm10'\.minikube\certs\ca-key.pem (1679 bytes)
I0125 22:12:57.476951   10760 certs.go:388] found cert: C:\Users\bm10'\.minikube\certs\C:\Users\bm10'\.minikube\certs\ca.pem (1074 bytes)
I0125 22:12:57.476951   10760 certs.go:388] found cert: C:\Users\bm10'\.minikube\certs\C:\Users\bm10'\.minikube\certs\cert.pem (1119 bytes)
I0125 22:12:57.477961   10760 certs.go:388] found cert: C:\Users\bm10'\.minikube\certs\C:\Users\bm10'\.minikube\certs\key.pem (1675 bytes)
I0125 22:12:57.496478   10760 ssh_runner.go:319] scp C:\Users\bm10'\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0125 22:12:57.570961   10760 ssh_runner.go:319] scp C:\Users\bm10'\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0125 22:12:57.640574   10760 ssh_runner.go:319] scp C:\Users\bm10'\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0125 22:12:57.708636   10760 ssh_runner.go:319] scp C:\Users\bm10'\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0125 22:12:57.773929   10760 ssh_runner.go:319] scp C:\Users\bm10'\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0125 22:12:57.843911   10760 ssh_runner.go:319] scp C:\Users\bm10'\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0125 22:12:57.911110   10760 ssh_runner.go:319] scp C:\Users\bm10'\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0125 22:12:57.978817   10760 ssh_runner.go:319] scp C:\Users\bm10'\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0125 22:12:58.049206   10760 ssh_runner.go:319] scp C:\Users\bm10'\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0125 22:12:58.112274   10760 ssh_runner.go:319] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0125 22:12:58.171455   10760 ssh_runner.go:152] Run: openssl version
I0125 22:12:58.201922   10760 ssh_runner.go:152] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0125 22:12:58.239105   10760 ssh_runner.go:152] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0125 22:12:58.248028   10760 certs.go:431] hashing: -rw-r--r-- 1 root root 1111 Dec 21 17:29 /usr/share/ca-certificates/minikubeCA.pem
I0125 22:12:58.258639   10760 ssh_runner.go:152] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0125 22:12:58.281042   10760 ssh_runner.go:152] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0125 22:12:58.306645   10760 kubeadm.go:390] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.28@sha256:4780f1897569d2bf77aafb3d133a08d42b4fe61127f06fcfc90c2c5d902d893c Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.22.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: ExtraOptions:[{Component:kubelet Key:cni-conf-dir Value:/etc/cni/net.mk}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.22.3 ControlPlane:true Worker:true} {Name:m02 IP:192.168.49.3 Port:0 KubernetesVersion:v1.22.3 ControlPlane:false Worker:true} {Name:m03 IP:192.168.49.4 Port:0 KubernetesVersion:v1.22.3 ControlPlane:false Worker:true}] Addons:map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false helm-tiller:false ingress:true ingress-dns:true istio:false istio-provisioner:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[IngressController:ingress-nginx/controller:v1.0.4@sha256:545cff00370f28363dad31e3b59a94ba377854d3a11f18988f5f9e56841ef9ef IngressDNS:k8s-minikube/minikube-ingress-dns:0.0.2@sha256:4abe27f9fc03fedab1d655e2020e6b165faf3bf6de1088ce6cf215a75b78f05f KubeWebhookCertgenCreate:k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660 KubeWebhookCertgenPatch:k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\bm10':/minikube-host}
I0125 22:12:58.312539   10760 ssh_runner.go:152] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0125 22:12:58.433050   10760 ssh_runner.go:152] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0125 22:12:58.459712   10760 kubeadm.go:401] found existing configuration files, will attempt cluster restart
I0125 22:12:58.460023   10760 kubeadm.go:600] restartCluster start
I0125 22:12:58.469661   10760 ssh_runner.go:152] Run: sudo test -d /data/minikube
I0125 22:12:58.494092   10760 kubeadm.go:126] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0125 22:12:58.500239   10760 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0125 22:12:58.707782   10760 kubeconfig.go:92] found "minikube" server: "https://127.0.0.1:53655"
I0125 22:12:58.707782   10760 kubeconfig.go:116] verify returned: got: 127.0.0.1:53655, want: 127.0.0.1:52292
I0125 22:12:58.709993   10760 lock.go:35] WriteFile acquiring C:\Users\bm10'\.kube\config: {Name:mk66573c3b36c6cf0dfba3353056706eaba08ff8 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0125 22:12:58.791803   10760 ssh_runner.go:152] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0125 22:12:58.820447   10760 api_server.go:165] Checking apiserver status ...
I0125 22:12:58.829391   10760 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0125 22:12:58.875096   10760 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0125 22:12:59.082539   10760 api_server.go:165] Checking apiserver status ...
I0125 22:12:59.091108   10760 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0125 22:12:59.127638   10760 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0125 22:12:59.283170   10760 api_server.go:165] Checking apiserver status ...
I0125 22:12:59.294070   10760 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0125 22:12:59.328502   10760 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0125 22:12:59.484868   10760 api_server.go:165] Checking apiserver status ...
I0125 22:12:59.492936   10760 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0125 22:12:59.528201   10760 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0125 22:12:59.685966   10760 api_server.go:165] Checking apiserver status ...
I0125 22:12:59.694203   10760 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0125 22:12:59.729406   10760 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0125 22:12:59.889108   10760 api_server.go:165] Checking apiserver status ...
I0125 22:12:59.898878   10760 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0125 22:12:59.952582   10760 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0125 22:13:00.089047   10760 api_server.go:165] Checking apiserver status ...
I0125 22:13:00.099870   10760 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0125 22:13:00.136071   10760 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0125 22:13:00.289801   10760 api_server.go:165] Checking apiserver status ...
I0125 22:13:00.304513   10760 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0125 22:13:00.348932   10760 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0125 22:13:00.477638   10760 api_server.go:165] Checking apiserver status ...
I0125 22:13:00.487269   10760 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0125 22:13:00.532864   10760 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0125 22:13:00.678689   10760 api_server.go:165] Checking apiserver status ...
I0125 22:13:00.687787   10760 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0125 22:13:00.731563   10760 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0125 22:13:00.882191   10760 api_server.go:165] Checking apiserver status ...
I0125 22:13:00.890199   10760 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0125 22:13:00.934460   10760 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0125 22:13:01.083507   10760 api_server.go:165] Checking apiserver status ...
I0125 22:13:01.093237   10760 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0125 22:13:01.141593   10760 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0125 22:13:01.282964   10760 api_server.go:165] Checking apiserver status ...
I0125 22:13:01.293057   10760 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0125 22:13:01.386244   10760 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0125 22:13:01.482658   10760 api_server.go:165] Checking apiserver status ...
I0125 22:13:01.491043   10760 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0125 22:13:01.557675   10760 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0125 22:13:01.681311   10760 api_server.go:165] Checking apiserver status ...
I0125 22:13:01.693797   10760 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0125 22:13:01.787369   10760 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0125 22:13:01.884589   10760 api_server.go:165] Checking apiserver status ...
I0125 22:13:01.940329   10760 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0125 22:13:02.105679   10760 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0125 22:13:02.105679   10760 api_server.go:165] Checking apiserver status ...
I0125 22:13:02.126857   10760 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0125 22:13:02.210388   10760 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0125 22:13:02.210388   10760 kubeadm.go:575] needs reconfigure: apiserver error: timed out waiting for the condition
I0125 22:13:02.211261   10760 kubeadm.go:1032] stopping kube-system containers ...
I0125 22:13:02.224573   10760 ssh_runner.go:152] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0125 22:13:02.409616   10760 docker.go:390] Stopping containers: [8ddf41ebeb62 12b48bf940c3 5598fdb8f397 2b7a56aea1bd 684ef364a5bf 5c52d4d7654c 62d76d0006e1 cc4357685ace aa35c4afd9ea 58d871a33e1c ba2861f29cef 97deec103a40 df5d1cd5aca3 bf17a92a29fc 8e351047d026 1a2fcb264b04 658fbc56d177 2a8e328f3e98 83faa11180c3 8b9e424f8703 5e9b39a5b5dd 232c41d2bcc5 f03376cad5be d5fd9c562868 8e20390e46a8 959be2232e41 360484f2a2d8 ef6c414d45c0 73b8dd317b31 86e566f4da2d adfc52fdbf9a]
I0125 22:13:02.415175   10760 ssh_runner.go:152] Run: docker stop 8ddf41ebeb62 12b48bf940c3 5598fdb8f397 2b7a56aea1bd 684ef364a5bf 5c52d4d7654c 62d76d0006e1 cc4357685ace aa35c4afd9ea 58d871a33e1c ba2861f29cef 97deec103a40 df5d1cd5aca3 bf17a92a29fc 8e351047d026 1a2fcb264b04 658fbc56d177 2a8e328f3e98 83faa11180c3 8b9e424f8703 5e9b39a5b5dd 232c41d2bcc5 f03376cad5be d5fd9c562868 8e20390e46a8 959be2232e41 360484f2a2d8 ef6c414d45c0 73b8dd317b31 86e566f4da2d adfc52fdbf9a
I0125 22:13:02.504197   10760 ssh_runner.go:152] Run: sudo systemctl stop kubelet
I0125 22:13:02.555850   10760 ssh_runner.go:152] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0125 22:13:02.585157   10760 kubeadm.go:154] found existing configuration files:
-rw------- 1 root root 5643 Jan 22 10:15 /etc/kubernetes/admin.conf
-rw------- 1 root root 5652 Jan 24 14:54 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 1971 Jan 22 10:15 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5600 Jan 24 14:54 /etc/kubernetes/scheduler.conf

I0125 22:13:02.596310   10760 ssh_runner.go:152] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0125 22:13:02.635099   10760 ssh_runner.go:152] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0125 22:13:02.677664   10760 ssh_runner.go:152] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0125 22:13:02.705617   10760 kubeadm.go:165] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I0125 22:13:02.714283   10760 ssh_runner.go:152] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0125 22:13:02.772539   10760 ssh_runner.go:152] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0125 22:13:02.809432   10760 kubeadm.go:165] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I0125 22:13:02.820249   10760 ssh_runner.go:152] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0125 22:13:02.867392   10760 ssh_runner.go:152] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0125 22:13:02.894171   10760 kubeadm.go:676] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I0125 22:13:02.894171   10760 ssh_runner.go:152] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.22.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I0125 22:13:03.566852   10760 ssh_runner.go:152] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.22.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I0125 22:13:04.651180   10760 ssh_runner.go:192] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.22.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml": (1.0843287s)
I0125 22:13:04.651180   10760 ssh_runner.go:152] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.22.3:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I0125 22:13:05.004333   10760 ssh_runner.go:152] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.22.3:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I0125 22:13:05.182798   10760 ssh_runner.go:152] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.22.3:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I0125 22:13:05.333240   10760 api_server.go:51] waiting for apiserver process to appear ...
I0125 22:13:05.343955   10760 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0125 22:13:05.908396   10760 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0125 22:13:06.409677   10760 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0125 22:13:06.906182   10760 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0125 22:13:07.396708   10760 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0125 22:13:07.922047   10760 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0125 22:13:08.403327   10760 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0125 22:13:08.901365   10760 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0125 22:13:09.409103   10760 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0125 22:13:09.904675   10760 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0125 22:13:10.400716   10760 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0125 22:13:10.897642   10760 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0125 22:13:11.052945   10760 api_server.go:71] duration metric: took 5.7197045s to wait for apiserver process to appear ...
I0125 22:13:11.053462   10760 api_server.go:87] waiting for apiserver healthz status ...
I0125 22:13:11.053514   10760 api_server.go:240] Checking apiserver healthz at https://127.0.0.1:52292/healthz ...
I0125 22:13:11.060196   10760 api_server.go:256] stopped: https://127.0.0.1:52292/healthz: Get "https://127.0.0.1:52292/healthz": EOF
I0125 22:13:11.570264   10760 api_server.go:240] Checking apiserver healthz at https://127.0.0.1:52292/healthz ...
I0125 22:13:11.573118   10760 api_server.go:256] stopped: https://127.0.0.1:52292/healthz: Get "https://127.0.0.1:52292/healthz": EOF
I0125 22:13:12.067085   10760 api_server.go:240] Checking apiserver healthz at https://127.0.0.1:52292/healthz ...
I0125 22:13:12.070671   10760 api_server.go:256] stopped: https://127.0.0.1:52292/healthz: Get "https://127.0.0.1:52292/healthz": EOF
I0125 22:13:12.563895   10760 api_server.go:240] Checking apiserver healthz at https://127.0.0.1:52292/healthz ...
I0125 22:13:12.567807   10760 api_server.go:256] stopped: https://127.0.0.1:52292/healthz: Get "https://127.0.0.1:52292/healthz": EOF
I0125 22:13:13.073098   10760 api_server.go:240] Checking apiserver healthz at https://127.0.0.1:52292/healthz ...
I0125 22:13:13.076308   10760 api_server.go:256] stopped: https://127.0.0.1:52292/healthz: Get "https://127.0.0.1:52292/healthz": EOF
I0125 22:13:13.569978   10760 api_server.go:240] Checking apiserver healthz at https://127.0.0.1:52292/healthz ...
I0125 22:13:13.573147   10760 api_server.go:256] stopped: https://127.0.0.1:52292/healthz: Get "https://127.0.0.1:52292/healthz": EOF
I0125 22:13:14.060954   10760 api_server.go:240] Checking apiserver healthz at https://127.0.0.1:52292/healthz ...
I0125 22:13:19.072785   10760 api_server.go:256] stopped: https://127.0.0.1:52292/healthz: Get "https://127.0.0.1:52292/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0125 22:13:19.571591   10760 api_server.go:240] Checking apiserver healthz at https://127.0.0.1:52292/healthz ...
I0125 22:13:22.246638   10760 api_server.go:266] https://127.0.0.1:52292/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0125 22:13:22.246638   10760 api_server.go:102] status: https://127.0.0.1:52292/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0125 22:13:22.561929   10760 api_server.go:240] Checking apiserver healthz at https://127.0.0.1:52292/healthz ...
I0125 22:13:22.727494   10760 api_server.go:266] https://127.0.0.1:52292/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
healthz check failed
W0125 22:13:22.727494   10760 api_server.go:102] status: https://127.0.0.1:52292/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
healthz check failed
I0125 22:13:23.070225   10760 api_server.go:240] Checking apiserver healthz at https://127.0.0.1:52292/healthz ...
I0125 22:13:23.140191   10760 api_server.go:266] https://127.0.0.1:52292/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
healthz check failed
W0125 22:13:23.140191   10760 api_server.go:102] status: https://127.0.0.1:52292/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
healthz check failed
I0125 22:13:23.562556   10760 api_server.go:240] Checking apiserver healthz at https://127.0.0.1:52292/healthz ...
I0125 22:13:23.581674   10760 api_server.go:266] https://127.0.0.1:52292/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
healthz check failed
W0125 22:13:23.581674   10760 api_server.go:102] status: https://127.0.0.1:52292/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
healthz check failed
I0125 22:13:24.067584   10760 api_server.go:240] Checking apiserver healthz at https://127.0.0.1:52292/healthz ...
I0125 22:13:24.079407   10760 api_server.go:266] https://127.0.0.1:52292/healthz returned 200:
ok
I0125 22:13:24.101288   10760 api_server.go:140] control plane version: v1.22.3
I0125 22:13:24.101288   10760 api_server.go:130] duration metric: took 13.0478258s to wait for apiserver health ...
I0125 22:13:24.101288   10760 cni.go:93] Creating CNI manager for ""
I0125 22:13:24.101288   10760 cni.go:154] 3 nodes found, recommending kindnet
I0125 22:13:24.102374   10760 out.go:176] * Configuring CNI (Container Networking Interface) ...
I0125 22:13:24.110512   10760 ssh_runner.go:152] Run: stat /opt/cni/bin/portmap
I0125 22:13:24.119304   10760 cni.go:187] applying CNI manifest using /var/lib/minikube/binaries/v1.22.3/kubectl ...
I0125 22:13:24.119304   10760 ssh_runner.go:319] scp memory --> /var/tmp/minikube/cni.yaml (2429 bytes)
I0125 22:13:24.177220   10760 ssh_runner.go:152] Run: sudo /var/lib/minikube/binaries/v1.22.3/kubectl apply --kubeconfig=/var/lib/minikube/kubeconfig -f /var/tmp/minikube/cni.yaml
I0125 22:13:30.232095   10760 ssh_runner.go:192] Completed: sudo /var/lib/minikube/binaries/v1.22.3/kubectl apply --kubeconfig=/var/lib/minikube/kubeconfig -f /var/tmp/minikube/cni.yaml: (6.0548753s)
I0125 22:13:30.237596   10760 system_pods.go:43] waiting for kube-system pods to appear ...
I0125 22:13:30.373112   10760 system_pods.go:59] 12 kube-system pods found
I0125 22:13:30.373201   10760 system_pods.go:61] "coredns-78fcd69978-fxjn2" [fede93ee-615e-4f21-b3a5-52db8b25c054] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0125 22:13:30.373201   10760 system_pods.go:61] "etcd-minikube" [db2529a3-8203-4b7c-a264-12c240220f26] Running
I0125 22:13:30.373201   10760 system_pods.go:61] "kindnet-5tdfc" [4279a9d7-4483-459d-a486-d59b47896987] Running / Ready:ContainersNotReady (containers with unready status: [kindnet-cni]) / ContainersReady:ContainersNotReady (containers with unready status: [kindnet-cni])
I0125 22:13:30.373201   10760 system_pods.go:61] "kindnet-6gvnv" [ddbfc58b-5a54-4a1e-8dab-2b4863d3b375] Running
I0125 22:13:30.373201   10760 system_pods.go:61] "kindnet-llqkw" [0606b8a8-a7c6-40e0-bd1e-dd81325f667c] Running
I0125 22:13:30.373201   10760 system_pods.go:61] "kube-apiserver-minikube" [9ecd86ab-c406-4433-b5b1-246e01bcc357] Running
I0125 22:13:30.373255   10760 system_pods.go:61] "kube-controller-manager-minikube" [5cc05ae0-6790-4229-b66f-70f8c25ae6b3] Running
I0125 22:13:30.373255   10760 system_pods.go:61] "kube-proxy-ct2gp" [7f7047be-d512-4e31-8ab4-24159174265b] Running
I0125 22:13:30.373255   10760 system_pods.go:61] "kube-proxy-kxmdl" [c3227681-8947-46eb-ad52-486edf23c38d] Running
I0125 22:13:30.373255   10760 system_pods.go:61] "kube-proxy-nc2gp" [40c6dd1f-c5d7-43bb-8cb7-3d7e33cbbae7] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0125 22:13:30.373255   10760 system_pods.go:61] "kube-scheduler-minikube" [e5262bce-b22c-4865-a830-ed00cd09b9f0] Running
I0125 22:13:30.373255   10760 system_pods.go:61] "storage-provisioner" [6b688e41-602a-4aef-bc66-f3d564a9db43] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0125 22:13:30.373255   10760 system_pods.go:74] duration metric: took 135.6588ms to wait for pod list to return data ...
I0125 22:13:30.373255   10760 node_conditions.go:102] verifying NodePressure condition ...
I0125 22:13:30.463121   10760 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0125 22:13:30.463121   10760 node_conditions.go:123] node cpu capacity is 12
I0125 22:13:30.463121   10760 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0125 22:13:30.463121   10760 node_conditions.go:123] node cpu capacity is 12
I0125 22:13:30.463121   10760 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0125 22:13:30.463121   10760 node_conditions.go:123] node cpu capacity is 12
I0125 22:13:30.463121   10760 node_conditions.go:105] duration metric: took 89.8665ms to run NodePressure ...
I0125 22:13:30.463121   10760 ssh_runner.go:152] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.22.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I0125 22:13:32.461332   10760 ssh_runner.go:192] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.22.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml": (1.9982111s)
I0125 22:13:32.461332   10760 ssh_runner.go:152] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0125 22:13:32.767250   10760 ops.go:34] apiserver oom_adj: -16
I0125 22:13:32.767250   10760 kubeadm.go:604] restartCluster took 34.3072267s
I0125 22:13:32.767250   10760 kubeadm.go:392] StartCluster complete in 34.4606045s
I0125 22:13:32.768419   10760 settings.go:142] acquiring lock: {Name:mk6107c823d005dbfc4ce6453c199616a35f3e7a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0125 22:13:32.768475   10760 settings.go:150] Updating kubeconfig:  C:\Users\bm10'\.kube\config
I0125 22:13:32.771650   10760 lock.go:35] WriteFile acquiring C:\Users\bm10'\.kube\config: {Name:mk66573c3b36c6cf0dfba3353056706eaba08ff8 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0125 22:13:32.839264   10760 kapi.go:244] deployment "coredns" in namespace "kube-system" and context "minikube" rescaled to 1
I0125 22:13:32.839878   10760 start.go:229] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.22.3 ControlPlane:true Worker:true}
I0125 22:13:32.841288   10760 config.go:176] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.22.3
I0125 22:13:32.841883   10760 out.go:176] * Verifying Kubernetes components...
I0125 22:13:32.843074   10760 ssh_runner.go:152] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.22.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0125 22:13:32.843074   10760 addons.go:415] enableAddons start: toEnable=map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false helm-tiller:false ingress:true ingress-dns:true istio:false istio-provisioner:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false], additional=[]
I0125 22:13:32.846525   10760 addons.go:65] Setting default-storageclass=true in profile "minikube"
I0125 22:13:32.846525   10760 addons.go:65] Setting storage-provisioner=true in profile "minikube"
I0125 22:13:32.846525   10760 addons.go:65] Setting ingress-dns=true in profile "minikube"
I0125 22:13:32.846525   10760 addons.go:65] Setting ingress=true in profile "minikube"
I0125 22:13:32.847384   10760 addons.go:153] Setting addon ingress-dns=true in "minikube"
I0125 22:13:32.847384   10760 addons.go:153] Setting addon storage-provisioner=true in "minikube"
W0125 22:13:32.847384   10760 addons.go:165] addon storage-provisioner should already be in state true
W0125 22:13:32.847384   10760 addons.go:165] addon ingress-dns should already be in state true
I0125 22:13:32.847384   10760 addons.go:153] Setting addon ingress=true in "minikube"
W0125 22:13:32.847384   10760 addons.go:165] addon ingress should already be in state true
I0125 22:13:32.847384   10760 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0125 22:13:32.849764   10760 host.go:66] Checking if "minikube" exists ...
I0125 22:13:32.849764   10760 host.go:66] Checking if "minikube" exists ...
I0125 22:13:32.849764   10760 host.go:66] Checking if "minikube" exists ...
I0125 22:13:32.865155   10760 ssh_runner.go:152] Run: sudo systemctl is-active --quiet service kubelet
I0125 22:13:32.880648   10760 cli_runner.go:115] Run: docker container inspect minikube --format={{.State.Status}}
I0125 22:13:32.881741   10760 cli_runner.go:115] Run: docker container inspect minikube --format={{.State.Status}}
I0125 22:13:32.881741   10760 cli_runner.go:115] Run: docker container inspect minikube --format={{.State.Status}}
I0125 22:13:32.882373   10760 cli_runner.go:115] Run: docker container inspect minikube --format={{.State.Status}}
I0125 22:13:33.216469   10760 out.go:176] * After the addon is enabled, please run "minikube tunnel" and your ingress resources would be available at "127.0.0.1"
I0125 22:13:33.221721   10760 out.go:176]   - Using image gcr.io/k8s-minikube/minikube-ingress-dns:0.0.2
I0125 22:13:33.221772   10760 addons.go:348] installing /etc/kubernetes/addons/ingress-dns-pod.yaml
I0125 22:13:33.221772   10760 ssh_runner.go:319] scp memory --> /etc/kubernetes/addons/ingress-dns-pod.yaml (2442 bytes)
I0125 22:13:33.238627   10760 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0125 22:13:33.249215   10760 out.go:176] * After the addon is enabled, please run "minikube tunnel" and your ingress resources would be available at "127.0.0.1"
I0125 22:13:33.253076   10760 out.go:176]   - Using image k8s.gcr.io/ingress-nginx/controller:v1.0.4
I0125 22:13:33.258227   10760 out.go:176]   - Using image k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1
I0125 22:13:33.260812   10760 out.go:176]   - Using image k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1
I0125 22:13:33.261385   10760 addons.go:348] installing /etc/kubernetes/addons/ingress-deploy.yaml
I0125 22:13:33.261385   10760 ssh_runner.go:319] scp memory --> /etc/kubernetes/addons/ingress-deploy.yaml (17469 bytes)
I0125 22:13:33.269658   10760 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0125 22:13:33.277175   10760 out.go:176]   - Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0125 22:13:33.277840   10760 addons.go:348] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0125 22:13:33.277970   10760 ssh_runner.go:319] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0125 22:13:33.286787   10760 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0125 22:13:33.507491   10760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52288 SSHKeyPath:C:\Users\bm10'\.minikube\machines\minikube\id_rsa Username:docker}
I0125 22:13:33.523322   10760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52288 SSHKeyPath:C:\Users\bm10'\.minikube\machines\minikube\id_rsa Username:docker}
I0125 22:13:33.533916   10760 addons.go:153] Setting addon default-storageclass=true in "minikube"
W0125 22:13:33.534154   10760 addons.go:165] addon default-storageclass should already be in state true
I0125 22:13:33.535315   10760 host.go:66] Checking if "minikube" exists ...
I0125 22:13:33.542822   10760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52288 SSHKeyPath:C:\Users\bm10'\.minikube\machines\minikube\id_rsa Username:docker}
I0125 22:13:33.555272   10760 cli_runner.go:115] Run: docker container inspect minikube --format={{.State.Status}}
I0125 22:13:33.790272   10760 addons.go:348] installing /etc/kubernetes/addons/storageclass.yaml
I0125 22:13:33.790272   10760 ssh_runner.go:319] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0125 22:13:33.798741   10760 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0125 22:13:34.024438   10760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52288 SSHKeyPath:C:\Users\bm10'\.minikube\machines\minikube\id_rsa Username:docker}
I0125 22:13:34.537594   10760 ssh_runner.go:192] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.22.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml": (1.6945205s)
I0125 22:13:34.537955   10760 ssh_runner.go:192] Completed: sudo systemctl is-active --quiet service kubelet: (1.6728003s)
I0125 22:13:34.537955   10760 start.go:719] CoreDNS already contains "host.minikube.internal" host record, skipping...
I0125 22:13:34.552855   10760 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0125 22:13:34.663822   10760 ssh_runner.go:152] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/ingress-dns-pod.yaml
I0125 22:13:34.666454   10760 ssh_runner.go:152] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml
I0125 22:13:34.669198   10760 ssh_runner.go:152] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0125 22:13:34.797956   10760 api_server.go:51] waiting for apiserver process to appear ...
I0125 22:13:34.806481   10760 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0125 22:13:34.958345   10760 ssh_runner.go:152] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0125 22:13:42.563294   10760 ssh_runner.go:192] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml: (7.8963125s)
I0125 22:13:42.563294   10760 ssh_runner.go:192] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/ingress-dns-pod.yaml: (7.8994727s)
I0125 22:13:42.563294   10760 ssh_runner.go:192] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (7.894096s)
I0125 22:13:42.563294   10760 ssh_runner.go:192] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (7.7568134s)
I0125 22:13:42.563294   10760 api_server.go:71] duration metric: took 9.7227454s to wait for apiserver process to appear ...
I0125 22:13:42.563294   10760 api_server.go:87] waiting for apiserver healthz status ...
I0125 22:13:42.563294   10760 api_server.go:240] Checking apiserver healthz at https://127.0.0.1:52292/healthz ...
I0125 22:13:42.563294   10760 ssh_runner.go:192] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (7.6049489s)
W0125 22:13:42.563903   10760 addons.go:369] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:
namespace/ingress-nginx unchanged
serviceaccount/ingress-nginx configured
configmap/ingress-nginx-controller configured
configmap/tcp-services created
configmap/udp-services created
clusterrole.rbac.authorization.k8s.io/ingress-nginx configured
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx configured
role.rbac.authorization.k8s.io/ingress-nginx configured
rolebinding.rbac.authorization.k8s.io/ingress-nginx configured
service/ingress-nginx-controller-admission configured
service/ingress-nginx-controller configured
deployment.apps/ingress-nginx-controller configured
ingressclass.networking.k8s.io/nginx configured
validatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission configured
serviceaccount/ingress-nginx-admission configured
clusterrole.rbac.authorization.k8s.io/ingress-nginx-admission configured
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission configured
role.rbac.authorization.k8s.io/ingress-nginx-admission configured
rolebinding.rbac.authorization.k8s.io/ingress-nginx-admission configured

stderr:
Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-create\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-create\"},\"spec\":{\"containers\":[{\"args\":[\"create\",\"--host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc\",\"--namespace=$(POD_NAMESPACE)\",\"--secret-name=ingress-nginx-admission\"],\"env\":[{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"create\"}],\"restartPolicy\":\"OnFailure\",\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":2000},\"serviceAccountName\":\"ingress-nginx-admission\"}}}}\n"},"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"template":{"metadata":{"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"$setElementOrder/containers":[{"name":"create"}],"containers":[{"image":"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660","name":"create","securityContext":null}],"nodeSelector":null,"securityContext":{"runAsNonRoot":true,"runAsUser":2000}}}}}
to:
Resource: "batch/v1, Resource=jobs", GroupVersionKind: "batch/v1, Kind=Job"
Name: "ingress-nginx-admission-create", Namespace: "ingress-nginx"
for: "/etc/kubernetes/addons/ingress-deploy.yaml": Job.batch "ingress-nginx-admission-create" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-create", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app.kubernetes.io/component":"admission-webhook", "app.kubernetes.io/instance":"ingress-nginx", "app.kubernetes.io/name":"ingress-nginx", "controller-uid":"8d577ee3-188a-46d7-a283-795857a59e12", "job-name":"ingress-nginx-admission-create"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:"create", Image:"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660", Command:[]string(nil), Args:[]string{"create", "--host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc", "--namespace=$(POD_NAMESPACE)", "--secret-name=ingress-nginx-admission"}, WorkingDir:"", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar{core.EnvVar{Name:"POD_NAMESPACE", Value:"", ValueFrom:(*core.EnvVarSource)(0xc00564fd80)}}, Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*core.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:"OnFailure", TerminationGracePeriodSeconds:(*int64)(0xc006733810), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"ingress-nginx-admission", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", SecurityContext:(*core.PodSecurityContext)(0xc005bcf080), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:"", Subdomain:"", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable
Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-patch\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-patch\"},\"spec\":{\"containers\":[{\"args\":[\"patch\",\"--webhook-name=ingress-nginx-admission\",\"--namespace=$(POD_NAMESPACE)\",\"--patch-mutating=false\",\"--secret-name=ingress-nginx-admission\",\"--patch-failure-policy=Fail\"],\"env\":[{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"patch\"}],\"restartPolicy\":\"OnFailure\",\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":2000},\"serviceAccountName\":\"ingress-nginx-admission\"}}}}\n"},"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"template":{"metadata":{"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"$setElementOrder/containers":[{"name":"patch"}],"containers":[{"image":"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660","name":"patch","securityContext":null}],"nodeSelector":null,"securityContext":{"runAsNonRoot":true,"runAsUser":2000}}}}}
to:
Resource: "batch/v1, Resource=jobs", GroupVersionKind: "batch/v1, Kind=Job"
Name: "ingress-nginx-admission-patch", Namespace: "ingress-nginx"
for: "/etc/kubernetes/addons/ingress-deploy.yaml": Job.batch "ingress-nginx-admission-patch" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-patch", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app.kubernetes.io/component":"admission-webhook", "app.kubernetes.io/instance":"ingress-nginx", "app.kubernetes.io/name":"ingress-nginx", "controller-uid":"9961db6d-58a8-4d49-a390-8d0d0f4d7d10", "job-name":"ingress-nginx-admission-patch"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:"patch", Image:"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660", Command:[]string(nil), Args:[]string{"patch", "--webhook-name=ingress-nginx-admission", "--namespace=$(POD_NAMESPACE)", "--patch-mutating=false", "--secret-name=ingress-nginx-admission", "--patch-failure-policy=Fail"}, WorkingDir:"", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar{core.EnvVar{Name:"POD_NAMESPACE", Value:"", ValueFrom:(*core.EnvVarSource)(0xc00546d4e0)}}, Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*core.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:"OnFailure", TerminationGracePeriodSeconds:(*int64)(0xc005c6b4f0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"ingress-nginx-admission", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", SecurityContext:(*core.PodSecurityContext)(0xc005bb3e00), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:"", Subdomain:"", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable
I0125 22:13:42.563903   10760 retry.go:31] will retry after 276.165072ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:
namespace/ingress-nginx unchanged
serviceaccount/ingress-nginx configured
configmap/ingress-nginx-controller configured
configmap/tcp-services created
configmap/udp-services created
clusterrole.rbac.authorization.k8s.io/ingress-nginx configured
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx configured
role.rbac.authorization.k8s.io/ingress-nginx configured
rolebinding.rbac.authorization.k8s.io/ingress-nginx configured
service/ingress-nginx-controller-admission configured
service/ingress-nginx-controller configured
deployment.apps/ingress-nginx-controller configured
ingressclass.networking.k8s.io/nginx configured
validatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission configured
serviceaccount/ingress-nginx-admission configured
clusterrole.rbac.authorization.k8s.io/ingress-nginx-admission configured
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission configured
role.rbac.authorization.k8s.io/ingress-nginx-admission configured
rolebinding.rbac.authorization.k8s.io/ingress-nginx-admission configured

stderr:
Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-create\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-create\"},\"spec\":{\"containers\":[{\"args\":[\"create\",\"--host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc\",\"--namespace=$(POD_NAMESPACE)\",\"--secret-name=ingress-nginx-admission\"],\"env\":[{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"create\"}],\"restartPolicy\":\"OnFailure\",\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":2000},\"serviceAccountName\":\"ingress-nginx-admission\"}}}}\n"},"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"template":{"metadata":{"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"$setElementOrder/containers":[{"name":"create"}],"containers":[{"image":"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660","name":"create","securityContext":null}],"nodeSelector":null,"securityContext":{"runAsNonRoot":true,"runAsUser":2000}}}}}
to:
Resource: "batch/v1, Resource=jobs", GroupVersionKind: "batch/v1, Kind=Job"
Name: "ingress-nginx-admission-create", Namespace: "ingress-nginx"
for: "/etc/kubernetes/addons/ingress-deploy.yaml": Job.batch "ingress-nginx-admission-create" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-create", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app.kubernetes.io/component":"admission-webhook", "app.kubernetes.io/instance":"ingress-nginx", "app.kubernetes.io/name":"ingress-nginx", "controller-uid":"8d577ee3-188a-46d7-a283-795857a59e12", "job-name":"ingress-nginx-admission-create"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:"create", Image:"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660", Command:[]string(nil), Args:[]string{"create", "--host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc", "--namespace=$(POD_NAMESPACE)", "--secret-name=ingress-nginx-admission"}, WorkingDir:"", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar{core.EnvVar{Name:"POD_NAMESPACE", Value:"", ValueFrom:(*core.EnvVarSource)(0xc00564fd80)}}, Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*core.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:"OnFailure", TerminationGracePeriodSeconds:(*int64)(0xc006733810), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"ingress-nginx-admission", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", SecurityContext:(*core.PodSecurityContext)(0xc005bcf080), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:"", Subdomain:"", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable
Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-patch\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-patch\"},\"spec\":{\"containers\":[{\"args\":[\"patch\",\"--webhook-name=ingress-nginx-admission\",\"--namespace=$(POD_NAMESPACE)\",\"--patch-mutating=false\",\"--secret-name=ingress-nginx-admission\",\"--patch-failure-policy=Fail\"],\"env\":[{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"patch\"}],\"restartPolicy\":\"OnFailure\",\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":2000},\"serviceAccountName\":\"ingress-nginx-admission\"}}}}\n"},"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"template":{"metadata":{"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"$setElementOrder/containers":[{"name":"patch"}],"containers":[{"image":"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660","name":"patch","securityContext":null}],"nodeSelector":null,"securityContext":{"runAsNonRoot":true,"runAsUser":2000}}}}}
to:
Resource: "batch/v1, Resource=jobs", GroupVersionKind: "batch/v1, Kind=Job"
Name: "ingress-nginx-admission-patch", Namespace: "ingress-nginx"
for: "/etc/kubernetes/addons/ingress-deploy.yaml": Job.batch "ingress-nginx-admission-patch" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-patch", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app.kubernetes.io/component":"admission-webhook", "app.kubernetes.io/instance":"ingress-nginx", "app.kubernetes.io/name":"ingress-nginx", "controller-uid":"9961db6d-58a8-4d49-a390-8d0d0f4d7d10", "job-name":"ingress-nginx-admission-patch"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:"patch", Image:"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660", Command:[]string(nil), Args:[]string{"patch", "--webhook-name=ingress-nginx-admission", "--namespace=$(POD_NAMESPACE)", "--patch-mutating=false", "--secret-name=ingress-nginx-admission", "--patch-failure-policy=Fail"}, WorkingDir:"", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar{core.EnvVar{Name:"POD_NAMESPACE", Value:"", ValueFrom:(*core.EnvVarSource)(0xc00546d4e0)}}, Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*core.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:"OnFailure", TerminationGracePeriodSeconds:(*int64)(0xc005c6b4f0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"ingress-nginx-admission", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", SecurityContext:(*core.PodSecurityContext)(0xc005bb3e00), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:"", Subdomain:"", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable
I0125 22:13:42.580714   10760 api_server.go:266] https://127.0.0.1:52292/healthz returned 200:
ok
I0125 22:13:42.586125   10760 api_server.go:140] control plane version: v1.22.3
I0125 22:13:42.586125   10760 api_server.go:130] duration metric: took 22.8308ms to wait for apiserver health ...
I0125 22:13:42.586125   10760 system_pods.go:43] waiting for kube-system pods to appear ...
I0125 22:13:42.646716   10760 system_pods.go:59] 13 kube-system pods found
I0125 22:13:42.646716   10760 system_pods.go:61] "coredns-78fcd69978-fxjn2" [fede93ee-615e-4f21-b3a5-52db8b25c054] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0125 22:13:42.646716   10760 system_pods.go:61] "etcd-minikube" [db2529a3-8203-4b7c-a264-12c240220f26] Running
I0125 22:13:42.646716   10760 system_pods.go:61] "kindnet-5tdfc" [4279a9d7-4483-459d-a486-d59b47896987] Running
I0125 22:13:42.646716   10760 system_pods.go:61] "kindnet-6gvnv" [ddbfc58b-5a54-4a1e-8dab-2b4863d3b375] Running
I0125 22:13:42.646716   10760 system_pods.go:61] "kindnet-llqkw" [0606b8a8-a7c6-40e0-bd1e-dd81325f667c] Running
I0125 22:13:42.646716   10760 system_pods.go:61] "kube-apiserver-minikube" [9ecd86ab-c406-4433-b5b1-246e01bcc357] Running
I0125 22:13:42.646716   10760 system_pods.go:61] "kube-controller-manager-minikube" [5cc05ae0-6790-4229-b66f-70f8c25ae6b3] Running
I0125 22:13:42.646716   10760 system_pods.go:61] "kube-ingress-dns-minikube" [2ca99673-6fa4-4f17-a0ba-77102b97cbb6] Pending
I0125 22:13:42.646716   10760 system_pods.go:61] "kube-proxy-ct2gp" [7f7047be-d512-4e31-8ab4-24159174265b] Running
I0125 22:13:42.646716   10760 system_pods.go:61] "kube-proxy-kxmdl" [c3227681-8947-46eb-ad52-486edf23c38d] Running
I0125 22:13:42.646716   10760 system_pods.go:61] "kube-proxy-nc2gp" [40c6dd1f-c5d7-43bb-8cb7-3d7e33cbbae7] Running
I0125 22:13:42.646716   10760 system_pods.go:61] "kube-scheduler-minikube" [e5262bce-b22c-4865-a830-ed00cd09b9f0] Running
I0125 22:13:42.646716   10760 system_pods.go:61] "storage-provisioner" [6b688e41-602a-4aef-bc66-f3d564a9db43] Running
I0125 22:13:42.646716   10760 system_pods.go:74] duration metric: took 60.5913ms to wait for pod list to return data ...
I0125 22:13:42.646716   10760 kubeadm.go:547] duration metric: took 9.8061675s to wait for : map[apiserver:true system_pods:true] ...
I0125 22:13:42.647321   10760 node_conditions.go:102] verifying NodePressure condition ...
I0125 22:13:42.658515   10760 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0125 22:13:42.658515   10760 node_conditions.go:123] node cpu capacity is 12
I0125 22:13:42.658515   10760 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0125 22:13:42.658515   10760 node_conditions.go:123] node cpu capacity is 12
I0125 22:13:42.658515   10760 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0125 22:13:42.658515   10760 node_conditions.go:123] node cpu capacity is 12
I0125 22:13:42.658515   10760 node_conditions.go:105] duration metric: took 11.1326ms to run NodePressure ...
I0125 22:13:42.658515   10760 start.go:234] waiting for startup goroutines ...
I0125 22:13:42.857650   10760 ssh_runner.go:152] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml
I0125 22:13:44.067392   10760 ssh_runner.go:192] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml: (1.209742s)
W0125 22:13:44.067392   10760 addons.go:369] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:
namespace/ingress-nginx unchanged
serviceaccount/ingress-nginx unchanged
configmap/ingress-nginx-controller unchanged
configmap/tcp-services unchanged
configmap/udp-services unchanged
clusterrole.rbac.authorization.k8s.io/ingress-nginx unchanged
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged
role.rbac.authorization.k8s.io/ingress-nginx unchanged
rolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged
service/ingress-nginx-controller-admission unchanged
service/ingress-nginx-controller unchanged
deployment.apps/ingress-nginx-controller configured
ingressclass.networking.k8s.io/nginx unchanged
validatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission configured
serviceaccount/ingress-nginx-admission unchanged
clusterrole.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
role.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
rolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged

stderr:
Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-create\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-create\"},\"spec\":{\"containers\":[{\"args\":[\"create\",\"--host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc\",\"--namespace=$(POD_NAMESPACE)\",\"--secret-name=ingress-nginx-admission\"],\"env\":[{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"create\"}],\"restartPolicy\":\"OnFailure\",\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":2000},\"serviceAccountName\":\"ingress-nginx-admission\"}}}}\n"},"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"template":{"metadata":{"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"$setElementOrder/containers":[{"name":"create"}],"containers":[{"image":"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660","name":"create","securityContext":null}],"nodeSelector":null,"securityContext":{"runAsNonRoot":true,"runAsUser":2000}}}}}
to:
Resource: "batch/v1, Resource=jobs", GroupVersionKind: "batch/v1, Kind=Job"
Name: "ingress-nginx-admission-create", Namespace: "ingress-nginx"
for: "/etc/kubernetes/addons/ingress-deploy.yaml": Job.batch "ingress-nginx-admission-create" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-create", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app.kubernetes.io/component":"admission-webhook", "app.kubernetes.io/instance":"ingress-nginx", "app.kubernetes.io/name":"ingress-nginx", "controller-uid":"8d577ee3-188a-46d7-a283-795857a59e12", "job-name":"ingress-nginx-admission-create"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:"create", Image:"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660", Command:[]string(nil), Args:[]string{"create", "--host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc", "--namespace=$(POD_NAMESPACE)", "--secret-name=ingress-nginx-admission"}, WorkingDir:"", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar{core.EnvVar{Name:"POD_NAMESPACE", Value:"", ValueFrom:(*core.EnvVarSource)(0xc005791000)}}, Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*core.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:"OnFailure", TerminationGracePeriodSeconds:(*int64)(0xc007124590), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"ingress-nginx-admission", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", SecurityContext:(*core.PodSecurityContext)(0xc005e35200), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:"", Subdomain:"", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable
Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-patch\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-patch\"},\"spec\":{\"containers\":[{\"args\":[\"patch\",\"--webhook-name=ingress-nginx-admission\",\"--namespace=$(POD_NAMESPACE)\",\"--patch-mutating=false\",\"--secret-name=ingress-nginx-admission\",\"--patch-failure-policy=Fail\"],\"env\":[{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"patch\"}],\"restartPolicy\":\"OnFailure\",\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":2000},\"serviceAccountName\":\"ingress-nginx-admission\"}}}}\n"},"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"template":{"metadata":{"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"$setElementOrder/containers":[{"name":"patch"}],"containers":[{"image":"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660","name":"patch","securityContext":null}],"nodeSelector":null,"securityContext":{"runAsNonRoot":true,"runAsUser":2000}}}}}
to:
Resource: "batch/v1, Resource=jobs", GroupVersionKind: "batch/v1, Kind=Job"
Name: "ingress-nginx-admission-patch", Namespace: "ingress-nginx"
for: "/etc/kubernetes/addons/ingress-deploy.yaml": Job.batch "ingress-nginx-admission-patch" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-patch", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app.kubernetes.io/component":"admission-webhook", "app.kubernetes.io/instance":"ingress-nginx", "app.kubernetes.io/name":"ingress-nginx", "controller-uid":"9961db6d-58a8-4d49-a390-8d0d0f4d7d10", "job-name":"ingress-nginx-admission-patch"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:"patch", Image:"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660", Command:[]string(nil), Args:[]string{"patch", "--webhook-name=ingress-nginx-admission", "--namespace=$(POD_NAMESPACE)", "--patch-mutating=false", "--secret-name=ingress-nginx-admission", "--patch-failure-policy=Fail"}, WorkingDir:"", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar{core.EnvVar{Name:"POD_NAMESPACE", Value:"", ValueFrom:(*core.EnvVarSource)(0xc005c40460)}}, Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*core.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:"OnFailure", TerminationGracePeriodSeconds:(*int64)(0xc006dbd6d0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"ingress-nginx-admission", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", SecurityContext:(*core.PodSecurityContext)(0xc0066b4680), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:"", Subdomain:"", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable
I0125 22:13:44.067392   10760 retry.go:31] will retry after 540.190908ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:
namespace/ingress-nginx unchanged
serviceaccount/ingress-nginx unchanged
configmap/ingress-nginx-controller unchanged
configmap/tcp-services unchanged
configmap/udp-services unchanged
clusterrole.rbac.authorization.k8s.io/ingress-nginx unchanged
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged
role.rbac.authorization.k8s.io/ingress-nginx unchanged
rolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged
service/ingress-nginx-controller-admission unchanged
service/ingress-nginx-controller unchanged
deployment.apps/ingress-nginx-controller configured
ingressclass.networking.k8s.io/nginx unchanged
validatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission configured
serviceaccount/ingress-nginx-admission unchanged
clusterrole.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
role.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
rolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged

stderr:
Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-create\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-create\"},\"spec\":{\"containers\":[{\"args\":[\"create\",\"--host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc\",\"--namespace=$(POD_NAMESPACE)\",\"--secret-name=ingress-nginx-admission\"],\"env\":[{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"create\"}],\"restartPolicy\":\"OnFailure\",\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":2000},\"serviceAccountName\":\"ingress-nginx-admission\"}}}}\n"},"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"template":{"metadata":{"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"$setElementOrder/containers":[{"name":"create"}],"containers":[{"image":"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660","name":"create","securityContext":null}],"nodeSelector":null,"securityContext":{"runAsNonRoot":true,"runAsUser":2000}}}}}
to:
Resource: "batch/v1, Resource=jobs", GroupVersionKind: "batch/v1, Kind=Job"
Name: "ingress-nginx-admission-create", Namespace: "ingress-nginx"
for: "/etc/kubernetes/addons/ingress-deploy.yaml": Job.batch "ingress-nginx-admission-create" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-create", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app.kubernetes.io/component":"admission-webhook", "app.kubernetes.io/instance":"ingress-nginx", "app.kubernetes.io/name":"ingress-nginx", "controller-uid":"8d577ee3-188a-46d7-a283-795857a59e12", "job-name":"ingress-nginx-admission-create"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:"create", Image:"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660", Command:[]string(nil), Args:[]string{"create", "--host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc", "--namespace=$(POD_NAMESPACE)", "--secret-name=ingress-nginx-admission"}, WorkingDir:"", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar{core.EnvVar{Name:"POD_NAMESPACE", Value:"", ValueFrom:(*core.EnvVarSource)(0xc005791000)}}, Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*core.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:"OnFailure", TerminationGracePeriodSeconds:(*int64)(0xc007124590), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"ingress-nginx-admission", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", SecurityContext:(*core.PodSecurityContext)(0xc005e35200), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:"", Subdomain:"", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable
Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-patch\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-patch\"},\"spec\":{\"containers\":[{\"args\":[\"patch\",\"--webhook-name=ingress-nginx-admission\",\"--namespace=$(POD_NAMESPACE)\",\"--patch-mutating=false\",\"--secret-name=ingress-nginx-admission\",\"--patch-failure-policy=Fail\"],\"env\":[{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"patch\"}],\"restartPolicy\":\"OnFailure\",\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":2000},\"serviceAccountName\":\"ingress-nginx-admission\"}}}}\n"},"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"template":{"metadata":{"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"$setElementOrder/containers":[{"name":"patch"}],"containers":[{"image":"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660","name":"patch","securityContext":null}],"nodeSelector":null,"securityContext":{"runAsNonRoot":true,"runAsUser":2000}}}}}
to:
Resource: "batch/v1, Resource=jobs", GroupVersionKind: "batch/v1, Kind=Job"
Name: "ingress-nginx-admission-patch", Namespace: "ingress-nginx"
for: "/etc/kubernetes/addons/ingress-deploy.yaml": Job.batch "ingress-nginx-admission-patch" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-patch", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app.kubernetes.io/component":"admission-webhook", "app.kubernetes.io/instance":"ingress-nginx", "app.kubernetes.io/name":"ingress-nginx", "controller-uid":"9961db6d-58a8-4d49-a390-8d0d0f4d7d10", "job-name":"ingress-nginx-admission-patch"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:"patch", Image:"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660", Command:[]string(nil), Args:[]string{"patch", "--webhook-name=ingress-nginx-admission", "--namespace=$(POD_NAMESPACE)", "--patch-mutating=false", "--secret-name=ingress-nginx-admission", "--patch-failure-policy=Fail"}, WorkingDir:"", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar{core.EnvVar{Name:"POD_NAMESPACE", Value:"", ValueFrom:(*core.EnvVarSource)(0xc005c40460)}}, Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*core.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:"OnFailure", TerminationGracePeriodSeconds:(*int64)(0xc006dbd6d0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"ingress-nginx-admission", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", SecurityContext:(*core.PodSecurityContext)(0xc0066b4680), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:"", Subdomain:"", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable
I0125 22:13:44.617959   10760 ssh_runner.go:152] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml
I0125 22:13:45.891784   10760 ssh_runner.go:192] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml: (1.2738253s)
W0125 22:13:45.892358   10760 addons.go:369] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:
namespace/ingress-nginx unchanged
serviceaccount/ingress-nginx unchanged
configmap/ingress-nginx-controller unchanged
configmap/tcp-services unchanged
configmap/udp-services unchanged
clusterrole.rbac.authorization.k8s.io/ingress-nginx unchanged
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged
role.rbac.authorization.k8s.io/ingress-nginx unchanged
rolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged
service/ingress-nginx-controller-admission unchanged
service/ingress-nginx-controller unchanged
deployment.apps/ingress-nginx-controller configured
ingressclass.networking.k8s.io/nginx unchanged
validatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission configured
serviceaccount/ingress-nginx-admission unchanged
clusterrole.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
role.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
rolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged

stderr:
Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-create\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-create\"},\"spec\":{\"containers\":[{\"args\":[\"create\",\"--host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc\",\"--namespace=$(POD_NAMESPACE)\",\"--secret-name=ingress-nginx-admission\"],\"env\":[{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"create\"}],\"restartPolicy\":\"OnFailure\",\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":2000},\"serviceAccountName\":\"ingress-nginx-admission\"}}}}\n"},"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"template":{"metadata":{"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"$setElementOrder/containers":[{"name":"create"}],"containers":[{"image":"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660","name":"create","securityContext":null}],"nodeSelector":null,"securityContext":{"runAsNonRoot":true,"runAsUser":2000}}}}}
to:
Resource: "batch/v1, Resource=jobs", GroupVersionKind: "batch/v1, Kind=Job"
Name: "ingress-nginx-admission-create", Namespace: "ingress-nginx"
for: "/etc/kubernetes/addons/ingress-deploy.yaml": Job.batch "ingress-nginx-admission-create" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-create", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app.kubernetes.io/component":"admission-webhook", "app.kubernetes.io/instance":"ingress-nginx", "app.kubernetes.io/name":"ingress-nginx", "controller-uid":"8d577ee3-188a-46d7-a283-795857a59e12", "job-name":"ingress-nginx-admission-create"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:"create", Image:"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660", Command:[]string(nil), Args:[]string{"create", "--host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc", "--namespace=$(POD_NAMESPACE)", "--secret-name=ingress-nginx-admission"}, WorkingDir:"", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar{core.EnvVar{Name:"POD_NAMESPACE", Value:"", ValueFrom:(*core.EnvVarSource)(0xc005df29e0)}}, Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*core.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:"OnFailure", TerminationGracePeriodSeconds:(*int64)(0xc008526cd0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"ingress-nginx-admission", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", SecurityContext:(*core.PodSecurityContext)(0xc006913280), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:"", Subdomain:"", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable
Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-patch\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-patch\"},\"spec\":{\"containers\":[{\"args\":[\"patch\",\"--webhook-name=ingress-nginx-admission\",\"--namespace=$(POD_NAMESPACE)\",\"--patch-mutating=false\",\"--secret-name=ingress-nginx-admission\",\"--patch-failure-policy=Fail\"],\"env\":[{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"patch\"}],\"restartPolicy\":\"OnFailure\",\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":2000},\"serviceAccountName\":\"ingress-nginx-admission\"}}}}\n"},"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"template":{"metadata":{"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"$setElementOrder/containers":[{"name":"patch"}],"containers":[{"image":"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660","name":"patch","securityContext":null}],"nodeSelector":null,"securityContext":{"runAsNonRoot":true,"runAsUser":2000}}}}}
to:
Resource: "batch/v1, Resource=jobs", GroupVersionKind: "batch/v1, Kind=Job"
Name: "ingress-nginx-admission-patch", Namespace: "ingress-nginx"
for: "/etc/kubernetes/addons/ingress-deploy.yaml": Job.batch "ingress-nginx-admission-patch" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-patch", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app.kubernetes.io/component":"admission-webhook", "app.kubernetes.io/instance":"ingress-nginx", "app.kubernetes.io/name":"ingress-nginx", "controller-uid":"9961db6d-58a8-4d49-a390-8d0d0f4d7d10", "job-name":"ingress-nginx-admission-patch"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:"patch", Image:"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660", Command:[]string(nil), Args:[]string{"patch", "--webhook-name=ingress-nginx-admission", "--namespace=$(POD_NAMESPACE)", "--patch-mutating=false", "--secret-name=ingress-nginx-admission", "--patch-failure-policy=Fail"}, WorkingDir:"", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar{core.EnvVar{Name:"POD_NAMESPACE", Value:"", ValueFrom:(*core.EnvVarSource)(0xc005ce7640)}}, Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*core.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:"OnFailure", TerminationGracePeriodSeconds:(*int64)(0xc006f74ce0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"ingress-nginx-admission", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", SecurityContext:(*core.PodSecurityContext)(0xc00685d980), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:"", Subdomain:"", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable
I0125 22:13:45.892358   10760 retry.go:31] will retry after 655.06503ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:
namespace/ingress-nginx unchanged
serviceaccount/ingress-nginx unchanged
configmap/ingress-nginx-controller unchanged
configmap/tcp-services unchanged
configmap/udp-services unchanged
clusterrole.rbac.authorization.k8s.io/ingress-nginx unchanged
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged
role.rbac.authorization.k8s.io/ingress-nginx unchanged
rolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged
service/ingress-nginx-controller-admission unchanged
service/ingress-nginx-controller unchanged
deployment.apps/ingress-nginx-controller configured
ingressclass.networking.k8s.io/nginx unchanged
validatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission configured
serviceaccount/ingress-nginx-admission unchanged
clusterrole.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
role.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
rolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged

stderr:
Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-create\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-create\"},\"spec\":{\"containers\":[{\"args\":[\"create\",\"--host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc\",\"--namespace=$(POD_NAMESPACE)\",\"--secret-name=ingress-nginx-admission\"],\"env\":[{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"create\"}],\"restartPolicy\":\"OnFailure\",\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":2000},\"serviceAccountName\":\"ingress-nginx-admission\"}}}}\n"},"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"template":{"metadata":{"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"$setElementOrder/containers":[{"name":"create"}],"containers":[{"image":"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660","name":"create","securityContext":null}],"nodeSelector":null,"securityContext":{"runAsNonRoot":true,"runAsUser":2000}}}}}
to:
Resource: "batch/v1, Resource=jobs", GroupVersionKind: "batch/v1, Kind=Job"
Name: "ingress-nginx-admission-create", Namespace: "ingress-nginx"
for: "/etc/kubernetes/addons/ingress-deploy.yaml": Job.batch "ingress-nginx-admission-create" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-create", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app.kubernetes.io/component":"admission-webhook", "app.kubernetes.io/instance":"ingress-nginx", "app.kubernetes.io/name":"ingress-nginx", "controller-uid":"8d577ee3-188a-46d7-a283-795857a59e12", "job-name":"ingress-nginx-admission-create"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:"create", Image:"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660", Command:[]string(nil), Args:[]string{"create", "--host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc", "--namespace=$(POD_NAMESPACE)", "--secret-name=ingress-nginx-admission"}, WorkingDir:"", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar{core.EnvVar{Name:"POD_NAMESPACE", Value:"", ValueFrom:(*core.EnvVarSource)(0xc005df29e0)}}, Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*core.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:"OnFailure", TerminationGracePeriodSeconds:(*int64)(0xc008526cd0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"ingress-nginx-admission", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", SecurityContext:(*core.PodSecurityContext)(0xc006913280), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:"", Subdomain:"", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable
Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-patch\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-patch\"},\"spec\":{\"containers\":[{\"args\":[\"patch\",\"--webhook-name=ingress-nginx-admission\",\"--namespace=$(POD_NAMESPACE)\",\"--patch-mutating=false\",\"--secret-name=ingress-nginx-admission\",\"--patch-failure-policy=Fail\"],\"env\":[{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"patch\"}],\"restartPolicy\":\"OnFailure\",\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":2000},\"serviceAccountName\":\"ingress-nginx-admission\"}}}}\n"},"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"template":{"metadata":{"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"$setElementOrder/containers":[{"name":"patch"}],"containers":[{"image":"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660","name":"patch","securityContext":null}],"nodeSelector":null,"securityContext":{"runAsNonRoot":true,"runAsUser":2000}}}}}
to:
Resource: "batch/v1, Resource=jobs", GroupVersionKind: "batch/v1, Kind=Job"
Name: "ingress-nginx-admission-patch", Namespace: "ingress-nginx"
for: "/etc/kubernetes/addons/ingress-deploy.yaml": Job.batch "ingress-nginx-admission-patch" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-patch", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app.kubernetes.io/component":"admission-webhook", "app.kubernetes.io/instance":"ingress-nginx", "app.kubernetes.io/name":"ingress-nginx", "controller-uid":"9961db6d-58a8-4d49-a390-8d0d0f4d7d10", "job-name":"ingress-nginx-admission-patch"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:"patch", Image:"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660", Command:[]string(nil), Args:[]string{"patch", "--webhook-name=ingress-nginx-admission", "--namespace=$(POD_NAMESPACE)", "--patch-mutating=false", "--secret-name=ingress-nginx-admission", "--patch-failure-policy=Fail"}, WorkingDir:"", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar{core.EnvVar{Name:"POD_NAMESPACE", Value:"", ValueFrom:(*core.EnvVarSource)(0xc005ce7640)}}, Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*core.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:"OnFailure", TerminationGracePeriodSeconds:(*int64)(0xc006f74ce0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"ingress-nginx-admission", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", SecurityContext:(*core.PodSecurityContext)(0xc00685d980), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:"", Subdomain:"", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable
I0125 22:13:46.576612   10760 ssh_runner.go:152] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml
W0125 22:13:47.443170   10760 addons.go:369] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:
namespace/ingress-nginx unchanged
serviceaccount/ingress-nginx unchanged
configmap/ingress-nginx-controller unchanged
configmap/tcp-services unchanged
configmap/udp-services unchanged
clusterrole.rbac.authorization.k8s.io/ingress-nginx unchanged
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged
role.rbac.authorization.k8s.io/ingress-nginx unchanged
rolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged
service/ingress-nginx-controller-admission unchanged
service/ingress-nginx-controller unchanged
deployment.apps/ingress-nginx-controller configured
ingressclass.networking.k8s.io/nginx unchanged
validatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission configured
serviceaccount/ingress-nginx-admission unchanged
clusterrole.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
role.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
rolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged

stderr:
Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-create\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-create\"},\"spec\":{\"containers\":[{\"args\":[\"create\",\"--host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc\",\"--namespace=$(POD_NAMESPACE)\",\"--secret-name=ingress-nginx-admission\"],\"env\":[{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"create\"}],\"restartPolicy\":\"OnFailure\",\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":2000},\"serviceAccountName\":\"ingress-nginx-admission\"}}}}\n"},"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"template":{"metadata":{"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"$setElementOrder/containers":[{"name":"create"}],"containers":[{"image":"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660","name":"create","securityContext":null}],"nodeSelector":null,"securityContext":{"runAsNonRoot":true,"runAsUser":2000}}}}}
to:
Resource: "batch/v1, Resource=jobs", GroupVersionKind: "batch/v1, Kind=Job"
Name: "ingress-nginx-admission-create", Namespace: "ingress-nginx"
for: "/etc/kubernetes/addons/ingress-deploy.yaml": Job.batch "ingress-nginx-admission-create" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-create", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app.kubernetes.io/component":"admission-webhook", "app.kubernetes.io/instance":"ingress-nginx", "app.kubernetes.io/name":"ingress-nginx", "controller-uid":"8d577ee3-188a-46d7-a283-795857a59e12", "job-name":"ingress-nginx-admission-create"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:"create", Image:"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660", Command:[]string(nil), Args:[]string{"create", "--host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc", "--namespace=$(POD_NAMESPACE)", "--secret-name=ingress-nginx-admission"}, WorkingDir:"", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar{core.EnvVar{Name:"POD_NAMESPACE", Value:"", ValueFrom:(*core.EnvVarSource)(0xc00568aa00)}}, Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*core.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:"OnFailure", TerminationGracePeriodSeconds:(*int64)(0xc005f9ead0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"ingress-nginx-admission", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", SecurityContext:(*core.PodSecurityContext)(0xc006c74700), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:"", Subdomain:"", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable
Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-patch\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-patch\"},\"spec\":{\"containers\":[{\"args\":[\"patch\",\"--webhook-name=ingress-nginx-admission\",\"--namespace=$(POD_NAMESPACE)\",\"--patch-mutating=false\",\"--secret-name=ingress-nginx-admission\",\"--patch-failure-policy=Fail\"],\"env\":[{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"patch\"}],\"restartPolicy\":\"OnFailure\",\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":2000},\"serviceAccountName\":\"ingress-nginx-admission\"}}}}\n"},"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"template":{"metadata":{"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"$setElementOrder/containers":[{"name":"patch"}],"containers":[{"image":"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660","name":"patch","securityContext":null}],"nodeSelector":null,"securityContext":{"runAsNonRoot":true,"runAsUser":2000}}}}}
to:
Resource: "batch/v1, Resource=jobs", GroupVersionKind: "batch/v1, Kind=Job"
Name: "ingress-nginx-admission-patch", Namespace: "ingress-nginx"
for: "/etc/kubernetes/addons/ingress-deploy.yaml": Job.batch "ingress-nginx-admission-patch" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-patch", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app.kubernetes.io/component":"admission-webhook", "app.kubernetes.io/instance":"ingress-nginx", "app.kubernetes.io/name":"ingress-nginx", "controller-uid":"9961db6d-58a8-4d49-a390-8d0d0f4d7d10", "job-name":"ingress-nginx-admission-patch"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:"patch", Image:"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660", Command:[]string(nil), Args:[]string{"patch", "--webhook-name=ingress-nginx-admission", "--namespace=$(POD_NAMESPACE)", "--patch-mutating=false", "--secret-name=ingress-nginx-admission", "--patch-failure-policy=Fail"}, WorkingDir:"", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar{core.EnvVar{Name:"POD_NAMESPACE", Value:"", ValueFrom:(*core.EnvVarSource)(0xc006024280)}}, Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*core.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:"OnFailure", TerminationGracePeriodSeconds:(*int64)(0xc006009cc0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"ingress-nginx-admission", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", SecurityContext:(*core.PodSecurityContext)(0xc006ce9f80), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:"", Subdomain:"", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable
I0125 22:13:47.443170   10760 retry.go:31] will retry after 791.196345ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:
namespace/ingress-nginx unchanged
serviceaccount/ingress-nginx unchanged
configmap/ingress-nginx-controller unchanged
configmap/tcp-services unchanged
configmap/udp-services unchanged
clusterrole.rbac.authorization.k8s.io/ingress-nginx unchanged
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged
role.rbac.authorization.k8s.io/ingress-nginx unchanged
rolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged
service/ingress-nginx-controller-admission unchanged
service/ingress-nginx-controller unchanged
deployment.apps/ingress-nginx-controller configured
ingressclass.networking.k8s.io/nginx unchanged
validatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission configured
serviceaccount/ingress-nginx-admission unchanged
clusterrole.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
role.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
rolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged

stderr:
Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-create\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-create\"},\"spec\":{\"containers\":[{\"args\":[\"create\",\"--host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc\",\"--namespace=$(POD_NAMESPACE)\",\"--secret-name=ingress-nginx-admission\"],\"env\":[{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"create\"}],\"restartPolicy\":\"OnFailure\",\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":2000},\"serviceAccountName\":\"ingress-nginx-admission\"}}}}\n"},"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"template":{"metadata":{"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"$setElementOrder/containers":[{"name":"create"}],"containers":[{"image":"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660","name":"create","securityContext":null}],"nodeSelector":null,"securityContext":{"runAsNonRoot":true,"runAsUser":2000}}}}}
to:
Resource: "batch/v1, Resource=jobs", GroupVersionKind: "batch/v1, Kind=Job"
Name: "ingress-nginx-admission-create", Namespace: "ingress-nginx"
for: "/etc/kubernetes/addons/ingress-deploy.yaml": Job.batch "ingress-nginx-admission-create" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-create", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app.kubernetes.io/component":"admission-webhook", "app.kubernetes.io/instance":"ingress-nginx", "app.kubernetes.io/name":"ingress-nginx", "controller-uid":"8d577ee3-188a-46d7-a283-795857a59e12", "job-name":"ingress-nginx-admission-create"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:"create", Image:"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660", Command:[]string(nil), Args:[]string{"create", "--host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc", "--namespace=$(POD_NAMESPACE)", "--secret-name=ingress-nginx-admission"}, WorkingDir:"", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar{core.EnvVar{Name:"POD_NAMESPACE", Value:"", ValueFrom:(*core.EnvVarSource)(0xc00568aa00)}}, Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*core.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:"OnFailure", TerminationGracePeriodSeconds:(*int64)(0xc005f9ead0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"ingress-nginx-admission", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", SecurityContext:(*core.PodSecurityContext)(0xc006c74700), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:"", Subdomain:"", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable
Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-patch\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-patch\"},\"spec\":{\"containers\":[{\"args\":[\"patch\",\"--webhook-name=ingress-nginx-admission\",\"--namespace=$(POD_NAMESPACE)\",\"--patch-mutating=false\",\"--secret-name=ingress-nginx-admission\",\"--patch-failure-policy=Fail\"],\"env\":[{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"patch\"}],\"restartPolicy\":\"OnFailure\",\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":2000},\"serviceAccountName\":\"ingress-nginx-admission\"}}}}\n"},"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"template":{"metadata":{"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"$setElementOrder/containers":[{"name":"patch"}],"containers":[{"image":"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660","name":"patch","securityContext":null}],"nodeSelector":null,"securityContext":{"runAsNonRoot":true,"runAsUser":2000}}}}}
to:
Resource: "batch/v1, Resource=jobs", GroupVersionKind: "batch/v1, Kind=Job"
Name: "ingress-nginx-admission-patch", Namespace: "ingress-nginx"
for: "/etc/kubernetes/addons/ingress-deploy.yaml": Job.batch "ingress-nginx-admission-patch" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-patch", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app.kubernetes.io/component":"admission-webhook", "app.kubernetes.io/instance":"ingress-nginx", "app.kubernetes.io/name":"ingress-nginx", "controller-uid":"9961db6d-58a8-4d49-a390-8d0d0f4d7d10", "job-name":"ingress-nginx-admission-patch"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:"patch", Image:"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660", Command:[]string(nil), Args:[]string{"patch", "--webhook-name=ingress-nginx-admission", "--namespace=$(POD_NAMESPACE)", "--patch-mutating=false", "--secret-name=ingress-nginx-admission", "--patch-failure-policy=Fail"}, WorkingDir:"", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar{core.EnvVar{Name:"POD_NAMESPACE", Value:"", ValueFrom:(*core.EnvVarSource)(0xc006024280)}}, Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*core.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:"OnFailure", TerminationGracePeriodSeconds:(*int64)(0xc006009cc0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"ingress-nginx-admission", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", SecurityContext:(*core.PodSecurityContext)(0xc006ce9f80), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:"", Subdomain:"", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable
I0125 22:13:48.250326   10760 ssh_runner.go:152] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml
W0125 22:13:48.788835   10760 addons.go:369] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:
namespace/ingress-nginx unchanged
serviceaccount/ingress-nginx unchanged
configmap/ingress-nginx-controller unchanged
configmap/tcp-services unchanged
configmap/udp-services unchanged
clusterrole.rbac.authorization.k8s.io/ingress-nginx unchanged
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged
role.rbac.authorization.k8s.io/ingress-nginx unchanged
rolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged
service/ingress-nginx-controller-admission unchanged
service/ingress-nginx-controller unchanged
deployment.apps/ingress-nginx-controller configured
ingressclass.networking.k8s.io/nginx unchanged
validatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission configured
serviceaccount/ingress-nginx-admission unchanged
clusterrole.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
role.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
rolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged

stderr:
Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-create\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-create\"},\"spec\":{\"containers\":[{\"args\":[\"create\",\"--host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc\",\"--namespace=$(POD_NAMESPACE)\",\"--secret-name=ingress-nginx-admission\"],\"env\":[{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"create\"}],\"restartPolicy\":\"OnFailure\",\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":2000},\"serviceAccountName\":\"ingress-nginx-admission\"}}}}\n"},"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"template":{"metadata":{"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"$setElementOrder/containers":[{"name":"create"}],"containers":[{"image":"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660","name":"create","securityContext":null}],"nodeSelector":null,"securityContext":{"runAsNonRoot":true,"runAsUser":2000}}}}}
to:
Resource: "batch/v1, Resource=jobs", GroupVersionKind: "batch/v1, Kind=Job"
Name: "ingress-nginx-admission-create", Namespace: "ingress-nginx"
for: "/etc/kubernetes/addons/ingress-deploy.yaml": Job.batch "ingress-nginx-admission-create" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-create", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app.kubernetes.io/component":"admission-webhook", "app.kubernetes.io/instance":"ingress-nginx", "app.kubernetes.io/name":"ingress-nginx", "controller-uid":"8d577ee3-188a-46d7-a283-795857a59e12", "job-name":"ingress-nginx-admission-create"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:"create", Image:"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660", Command:[]string(nil), Args:[]string{"create", "--host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc", "--namespace=$(POD_NAMESPACE)", "--secret-name=ingress-nginx-admission"}, WorkingDir:"", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar{core.EnvVar{Name:"POD_NAMESPACE", Value:"", ValueFrom:(*core.EnvVarSource)(0xc005eafb80)}}, Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*core.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:"OnFailure", TerminationGracePeriodSeconds:(*int64)(0xc007bea9a0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"ingress-nginx-admission", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", SecurityContext:(*core.PodSecurityContext)(0xc006fc1400), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:"", Subdomain:"", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable
Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-patch\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-patch\"},\"spec\":{\"containers\":[{\"args\":[\"patch\",\"--webhook-name=ingress-nginx-admission\",\"--namespace=$(POD_NAMESPACE)\",\"--patch-mutating=false\",\"--secret-name=ingress-nginx-admission\",\"--patch-failure-policy=Fail\"],\"env\":[{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"patch\"}],\"restartPolicy\":\"OnFailure\",\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":2000},\"serviceAccountName\":\"ingress-nginx-admission\"}}}}\n"},"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"template":{"metadata":{"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"$setElementOrder/containers":[{"name":"patch"}],"containers":[{"image":"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660","name":"patch","securityContext":null}],"nodeSelector":null,"securityContext":{"runAsNonRoot":true,"runAsUser":2000}}}}}
to:
Resource: "batch/v1, Resource=jobs", GroupVersionKind: "batch/v1, Kind=Job"
Name: "ingress-nginx-admission-patch", Namespace: "ingress-nginx"
for: "/etc/kubernetes/addons/ingress-deploy.yaml": Job.batch "ingress-nginx-admission-patch" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-patch", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app.kubernetes.io/component":"admission-webhook", "app.kubernetes.io/instance":"ingress-nginx", "app.kubernetes.io/name":"ingress-nginx", "controller-uid":"9961db6d-58a8-4d49-a390-8d0d0f4d7d10", "job-name":"ingress-nginx-admission-patch"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:"patch", Image:"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660", Command:[]string(nil), Args:[]string{"patch", "--webhook-name=ingress-nginx-admission", "--namespace=$(POD_NAMESPACE)", "--patch-mutating=false", "--secret-name=ingress-nginx-admission", "--patch-failure-policy=Fail"}, WorkingDir:"", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar{core.EnvVar{Name:"POD_NAMESPACE", Value:"", ValueFrom:(*core.EnvVarSource)(0xc0064dfd20)}}, Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*core.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:"OnFailure", TerminationGracePeriodSeconds:(*int64)(0xc0062411d0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"ingress-nginx-admission", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", SecurityContext:(*core.PodSecurityContext)(0xc005df7e80), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:"", Subdomain:"", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable
I0125 22:13:48.788835   10760 retry.go:31] will retry after 1.170244332s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:
namespace/ingress-nginx unchanged
serviceaccount/ingress-nginx unchanged
configmap/ingress-nginx-controller unchanged
configmap/tcp-services unchanged
configmap/udp-services unchanged
clusterrole.rbac.authorization.k8s.io/ingress-nginx unchanged
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged
role.rbac.authorization.k8s.io/ingress-nginx unchanged
rolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged
service/ingress-nginx-controller-admission unchanged
service/ingress-nginx-controller unchanged
deployment.apps/ingress-nginx-controller configured
ingressclass.networking.k8s.io/nginx unchanged
validatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission configured
serviceaccount/ingress-nginx-admission unchanged
clusterrole.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
role.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
rolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged

stderr:
Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-create\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-create\"},\"spec\":{\"containers\":[{\"args\":[\"create\",\"--host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc\",\"--namespace=$(POD_NAMESPACE)\",\"--secret-name=ingress-nginx-admission\"],\"env\":[{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"create\"}],\"restartPolicy\":\"OnFailure\",\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":2000},\"serviceAccountName\":\"ingress-nginx-admission\"}}}}\n"},"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"template":{"metadata":{"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"$setElementOrder/containers":[{"name":"create"}],"containers":[{"image":"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660","name":"create","securityContext":null}],"nodeSelector":null,"securityContext":{"runAsNonRoot":true,"runAsUser":2000}}}}}
to:
Resource: "batch/v1, Resource=jobs", GroupVersionKind: "batch/v1, Kind=Job"
Name: "ingress-nginx-admission-create", Namespace: "ingress-nginx"
for: "/etc/kubernetes/addons/ingress-deploy.yaml": Job.batch "ingress-nginx-admission-create" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-create", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app.kubernetes.io/component":"admission-webhook", "app.kubernetes.io/instance":"ingress-nginx", "app.kubernetes.io/name":"ingress-nginx", "controller-uid":"8d577ee3-188a-46d7-a283-795857a59e12", "job-name":"ingress-nginx-admission-create"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:"create", Image:"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660", Command:[]string(nil), Args:[]string{"create", "--host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc", "--namespace=$(POD_NAMESPACE)", "--secret-name=ingress-nginx-admission"}, WorkingDir:"", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar{core.EnvVar{Name:"POD_NAMESPACE", Value:"", ValueFrom:(*core.EnvVarSource)(0xc005eafb80)}}, Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*core.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:"OnFailure", TerminationGracePeriodSeconds:(*int64)(0xc007bea9a0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"ingress-nginx-admission", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", SecurityContext:(*core.PodSecurityContext)(0xc006fc1400), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:"", Subdomain:"", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable
Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-patch\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-patch\"},\"spec\":{\"containers\":[{\"args\":[\"patch\",\"--webhook-name=ingress-nginx-admission\",\"--namespace=$(POD_NAMESPACE)\",\"--patch-mutating=false\",\"--secret-name=ingress-nginx-admission\",\"--patch-failure-policy=Fail\"],\"env\":[{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"patch\"}],\"restartPolicy\":\"OnFailure\",\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":2000},\"serviceAccountName\":\"ingress-nginx-admission\"}}}}\n"},"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"template":{"metadata":{"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"$setElementOrder/containers":[{"name":"patch"}],"containers":[{"image":"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660","name":"patch","securityContext":null}],"nodeSelector":null,"securityContext":{"runAsNonRoot":true,"runAsUser":2000}}}}}
to:
Resource: "batch/v1, Resource=jobs", GroupVersionKind: "batch/v1, Kind=Job"
Name: "ingress-nginx-admission-patch", Namespace: "ingress-nginx"
for: "/etc/kubernetes/addons/ingress-deploy.yaml": Job.batch "ingress-nginx-admission-patch" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-patch", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app.kubernetes.io/component":"admission-webhook", "app.kubernetes.io/instance":"ingress-nginx", "app.kubernetes.io/name":"ingress-nginx", "controller-uid":"9961db6d-58a8-4d49-a390-8d0d0f4d7d10", "job-name":"ingress-nginx-admission-patch"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:"patch", Image:"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660", Command:[]string(nil), Args:[]string{"patch", "--webhook-name=ingress-nginx-admission", "--namespace=$(POD_NAMESPACE)", "--patch-mutating=false", "--secret-name=ingress-nginx-admission", "--patch-failure-policy=Fail"}, WorkingDir:"", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar{core.EnvVar{Name:"POD_NAMESPACE", Value:"", ValueFrom:(*core.EnvVarSource)(0xc0064dfd20)}}, Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*core.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:"OnFailure", TerminationGracePeriodSeconds:(*int64)(0xc0062411d0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"ingress-nginx-admission", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", SecurityContext:(*core.PodSecurityContext)(0xc005df7e80), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:"", Subdomain:"", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable
I0125 22:13:49.975157   10760 ssh_runner.go:152] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml
W0125 22:13:50.582586   10760 addons.go:369] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:
namespace/ingress-nginx unchanged
serviceaccount/ingress-nginx unchanged
configmap/ingress-nginx-controller unchanged
configmap/tcp-services unchanged
configmap/udp-services unchanged
clusterrole.rbac.authorization.k8s.io/ingress-nginx unchanged
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged
role.rbac.authorization.k8s.io/ingress-nginx unchanged
rolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged
service/ingress-nginx-controller-admission unchanged
service/ingress-nginx-controller unchanged
deployment.apps/ingress-nginx-controller configured
ingressclass.networking.k8s.io/nginx unchanged
validatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission configured
serviceaccount/ingress-nginx-admission unchanged
clusterrole.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
role.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
rolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged

stderr:
Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-create\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-create\"},\"spec\":{\"containers\":[{\"args\":[\"create\",\"--host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc\",\"--namespace=$(POD_NAMESPACE)\",\"--secret-name=ingress-nginx-admission\"],\"env\":[{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"create\"}],\"restartPolicy\":\"OnFailure\",\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":2000},\"serviceAccountName\":\"ingress-nginx-admission\"}}}}\n"},"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"template":{"metadata":{"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"$setElementOrder/containers":[{"name":"create"}],"containers":[{"image":"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660","name":"create","securityContext":null}],"nodeSelector":null,"securityContext":{"runAsNonRoot":true,"runAsUser":2000}}}}}
to:
Resource: "batch/v1, Resource=jobs", GroupVersionKind: "batch/v1, Kind=Job"
Name: "ingress-nginx-admission-create", Namespace: "ingress-nginx"
for: "/etc/kubernetes/addons/ingress-deploy.yaml": Job.batch "ingress-nginx-admission-create" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-create", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app.kubernetes.io/component":"admission-webhook", "app.kubernetes.io/instance":"ingress-nginx", "app.kubernetes.io/name":"ingress-nginx", "controller-uid":"8d577ee3-188a-46d7-a283-795857a59e12", "job-name":"ingress-nginx-admission-create"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:"create", Image:"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660", Command:[]string(nil), Args:[]string{"create", "--host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc", "--namespace=$(POD_NAMESPACE)", "--secret-name=ingress-nginx-admission"}, WorkingDir:"", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar{core.EnvVar{Name:"POD_NAMESPACE", Value:"", ValueFrom:(*core.EnvVarSource)(0xc006725640)}}, Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*core.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:"OnFailure", TerminationGracePeriodSeconds:(*int64)(0xc007f33730), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"ingress-nginx-admission", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", SecurityContext:(*core.PodSecurityContext)(0xc00a580700), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:"", Subdomain:"", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable
Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-patch\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-patch\"},\"spec\":{\"containers\":[{\"args\":[\"patch\",\"--webhook-name=ingress-nginx-admission\",\"--namespace=$(POD_NAMESPACE)\",\"--patch-mutating=false\",\"--secret-name=ingress-nginx-admission\",\"--patch-failure-policy=Fail\"],\"env\":[{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"patch\"}],\"restartPolicy\":\"OnFailure\",\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":2000},\"serviceAccountName\":\"ingress-nginx-admission\"}}}}\n"},"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"template":{"metadata":{"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"$setElementOrder/containers":[{"name":"patch"}],"containers":[{"image":"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660","name":"patch","securityContext":null}],"nodeSelector":null,"securityContext":{"runAsNonRoot":true,"runAsUser":2000}}}}}
to:
Resource: "batch/v1, Resource=jobs", GroupVersionKind: "batch/v1, Kind=Job"
Name: "ingress-nginx-admission-patch", Namespace: "ingress-nginx"
for: "/etc/kubernetes/addons/ingress-deploy.yaml": Job.batch "ingress-nginx-admission-patch" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-patch", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app.kubernetes.io/component":"admission-webhook", "app.kubernetes.io/instance":"ingress-nginx", "app.kubernetes.io/name":"ingress-nginx", "controller-uid":"9961db6d-58a8-4d49-a390-8d0d0f4d7d10", "job-name":"ingress-nginx-admission-patch"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:"patch", Image:"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660", Command:[]string(nil), Args:[]string{"patch", "--webhook-name=ingress-nginx-admission", "--namespace=$(POD_NAMESPACE)", "--patch-mutating=false", "--secret-name=ingress-nginx-admission", "--patch-failure-policy=Fail"}, WorkingDir:"", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar{core.EnvVar{Name:"POD_NAMESPACE", Value:"", ValueFrom:(*core.EnvVarSource)(0xc006688600)}}, Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*core.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:"OnFailure", TerminationGracePeriodSeconds:(*int64)(0xc000b49c40), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"ingress-nginx-admission", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", SecurityContext:(*core.PodSecurityContext)(0xc00a5dea00), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:"", Subdomain:"", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable
I0125 22:13:50.582586   10760 retry.go:31] will retry after 2.253109428s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:
namespace/ingress-nginx unchanged
serviceaccount/ingress-nginx unchanged
configmap/ingress-nginx-controller unchanged
configmap/tcp-services unchanged
configmap/udp-services unchanged
clusterrole.rbac.authorization.k8s.io/ingress-nginx unchanged
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged
role.rbac.authorization.k8s.io/ingress-nginx unchanged
rolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged
service/ingress-nginx-controller-admission unchanged
service/ingress-nginx-controller unchanged
deployment.apps/ingress-nginx-controller configured
ingressclass.networking.k8s.io/nginx unchanged
validatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission configured
serviceaccount/ingress-nginx-admission unchanged
clusterrole.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
role.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
rolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged

stderr:
Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-create\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-create\"},\"spec\":{\"containers\":[{\"args\":[\"create\",\"--host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc\",\"--namespace=$(POD_NAMESPACE)\",\"--secret-name=ingress-nginx-admission\"],\"env\":[{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"create\"}],\"restartPolicy\":\"OnFailure\",\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":2000},\"serviceAccountName\":\"ingress-nginx-admission\"}}}}\n"},"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"template":{"metadata":{"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"$setElementOrder/containers":[{"name":"create"}],"containers":[{"image":"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660","name":"create","securityContext":null}],"nodeSelector":null,"securityContext":{"runAsNonRoot":true,"runAsUser":2000}}}}}
to:
Resource: "batch/v1, Resource=jobs", GroupVersionKind: "batch/v1, Kind=Job"
Name: "ingress-nginx-admission-create", Namespace: "ingress-nginx"
for: "/etc/kubernetes/addons/ingress-deploy.yaml": Job.batch "ingress-nginx-admission-create" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-create", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app.kubernetes.io/component":"admission-webhook", "app.kubernetes.io/instance":"ingress-nginx", "app.kubernetes.io/name":"ingress-nginx", "controller-uid":"8d577ee3-188a-46d7-a283-795857a59e12", "job-name":"ingress-nginx-admission-create"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:"create", Image:"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660", Command:[]string(nil), Args:[]string{"create", "--host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc", "--namespace=$(POD_NAMESPACE)", "--secret-name=ingress-nginx-admission"}, WorkingDir:"", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar{core.EnvVar{Name:"POD_NAMESPACE", Value:"", ValueFrom:(*core.EnvVarSource)(0xc006725640)}}, Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*core.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:"OnFailure", TerminationGracePeriodSeconds:(*int64)(0xc007f33730), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"ingress-nginx-admission", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", SecurityContext:(*core.PodSecurityContext)(0xc00a580700), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:"", Subdomain:"", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable
Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-patch\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-patch\"},\"spec\":{\"containers\":[{\"args\":[\"patch\",\"--webhook-name=ingress-nginx-admission\",\"--namespace=$(POD_NAMESPACE)\",\"--patch-mutating=false\",\"--secret-name=ingress-nginx-admission\",\"--patch-failure-policy=Fail\"],\"env\":[{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"patch\"}],\"restartPolicy\":\"OnFailure\",\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":2000},\"serviceAccountName\":\"ingress-nginx-admission\"}}}}\n"},"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"template":{"metadata":{"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"$setElementOrder/containers":[{"name":"patch"}],"containers":[{"image":"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660","name":"patch","securityContext":null}],"nodeSelector":null,"securityContext":{"runAsNonRoot":true,"runAsUser":2000}}}}}
to:
Resource: "batch/v1, Resource=jobs", GroupVersionKind: "batch/v1, Kind=Job"
Name: "ingress-nginx-admission-patch", Namespace: "ingress-nginx"
for: "/etc/kubernetes/addons/ingress-deploy.yaml": Job.batch "ingress-nginx-admission-patch" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-patch", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app.kubernetes.io/component":"admission-webhook", "app.kubernetes.io/instance":"ingress-nginx", "app.kubernetes.io/name":"ingress-nginx", "controller-uid":"9961db6d-58a8-4d49-a390-8d0d0f4d7d10", "job-name":"ingress-nginx-admission-patch"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:"patch", Image:"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660", Command:[]string(nil), Args:[]string{"patch", "--webhook-name=ingress-nginx-admission", "--namespace=$(POD_NAMESPACE)", "--patch-mutating=false", "--secret-name=ingress-nginx-admission", "--patch-failure-policy=Fail"}, WorkingDir:"", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar{core.EnvVar{Name:"POD_NAMESPACE", Value:"", ValueFrom:(*core.EnvVarSource)(0xc006688600)}}, Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*core.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:"OnFailure", TerminationGracePeriodSeconds:(*int64)(0xc000b49c40), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"ingress-nginx-admission", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", SecurityContext:(*core.PodSecurityContext)(0xc00a5dea00), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:"", Subdomain:"", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable
I0125 22:13:52.845535   10760 ssh_runner.go:152] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml
W0125 22:13:53.693210   10760 addons.go:369] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:
namespace/ingress-nginx unchanged
serviceaccount/ingress-nginx unchanged
configmap/ingress-nginx-controller unchanged
configmap/tcp-services unchanged
configmap/udp-services unchanged
clusterrole.rbac.authorization.k8s.io/ingress-nginx unchanged
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged
role.rbac.authorization.k8s.io/ingress-nginx unchanged
rolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged
service/ingress-nginx-controller-admission unchanged
service/ingress-nginx-controller unchanged
deployment.apps/ingress-nginx-controller configured
ingressclass.networking.k8s.io/nginx unchanged
validatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission configured
serviceaccount/ingress-nginx-admission unchanged
clusterrole.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
role.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
rolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged

stderr:
Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-create\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-create\"},\"spec\":{\"containers\":[{\"args\":[\"create\",\"--host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc\",\"--namespace=$(POD_NAMESPACE)\",\"--secret-name=ingress-nginx-admission\"],\"env\":[{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"create\"}],\"restartPolicy\":\"OnFailure\",\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":2000},\"serviceAccountName\":\"ingress-nginx-admission\"}}}}\n"},"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"template":{"metadata":{"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"$setElementOrder/containers":[{"name":"create"}],"containers":[{"image":"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660","name":"create","securityContext":null}],"nodeSelector":null,"securityContext":{"runAsNonRoot":true,"runAsUser":2000}}}}}
to:
Resource: "batch/v1, Resource=jobs", GroupVersionKind: "batch/v1, Kind=Job"
Name: "ingress-nginx-admission-create", Namespace: "ingress-nginx"
for: "/etc/kubernetes/addons/ingress-deploy.yaml": Job.batch "ingress-nginx-admission-create" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-create", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app.kubernetes.io/component":"admission-webhook", "app.kubernetes.io/instance":"ingress-nginx", "app.kubernetes.io/name":"ingress-nginx", "controller-uid":"8d577ee3-188a-46d7-a283-795857a59e12", "job-name":"ingress-nginx-admission-create"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:"create", Image:"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660", Command:[]string(nil), Args:[]string{"create", "--host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc", "--namespace=$(POD_NAMESPACE)", "--secret-name=ingress-nginx-admission"}, WorkingDir:"", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar{core.EnvVar{Name:"POD_NAMESPACE", Value:"", ValueFrom:(*core.EnvVarSource)(0xc006bca580)}}, Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*core.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:"OnFailure", TerminationGracePeriodSeconds:(*int64)(0xc00d161080), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"ingress-nginx-admission", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", SecurityContext:(*core.PodSecurityContext)(0xc00a64cc80), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:"", Subdomain:"", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable
Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-patch\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-patch\"},\"spec\":{\"containers\":[{\"args\":[\"patch\",\"--webhook-name=ingress-nginx-admission\",\"--namespace=$(POD_NAMESPACE)\",\"--patch-mutating=false\",\"--secret-name=ingress-nginx-admission\",\"--patch-failure-policy=Fail\"],\"env\":[{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"patch\"}],\"restartPolicy\":\"OnFailure\",\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":2000},\"serviceAccountName\":\"ingress-nginx-admission\"}}}}\n"},"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"template":{"metadata":{"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"$setElementOrder/containers":[{"name":"patch"}],"containers":[{"image":"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660","name":"patch","securityContext":null}],"nodeSelector":null,"securityContext":{"runAsNonRoot":true,"runAsUser":2000}}}}}
to:
Resource: "batch/v1, Resource=jobs", GroupVersionKind: "batch/v1, Kind=Job"
Name: "ingress-nginx-admission-patch", Namespace: "ingress-nginx"
for: "/etc/kubernetes/addons/ingress-deploy.yaml": Job.batch "ingress-nginx-admission-patch" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-patch", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app.kubernetes.io/component":"admission-webhook", "app.kubernetes.io/instance":"ingress-nginx", "app.kubernetes.io/name":"ingress-nginx", "controller-uid":"9961db6d-58a8-4d49-a390-8d0d0f4d7d10", "job-name":"ingress-nginx-admission-patch"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:"patch", Image:"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660", Command:[]string(nil), Args:[]string{"patch", "--webhook-name=ingress-nginx-admission", "--namespace=$(POD_NAMESPACE)", "--patch-mutating=false", "--secret-name=ingress-nginx-admission", "--patch-failure-policy=Fail"}, WorkingDir:"", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar{core.EnvVar{Name:"POD_NAMESPACE", Value:"", ValueFrom:(*core.EnvVarSource)(0xc006bf0720)}}, Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*core.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:"OnFailure", TerminationGracePeriodSeconds:(*int64)(0xc00a4d4620), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"ingress-nginx-admission", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", SecurityContext:(*core.PodSecurityContext)(0xc00a8ca400), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:"", Subdomain:"", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable
I0125 22:13:53.693210   10760 retry.go:31] will retry after 1.610739793s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:
namespace/ingress-nginx unchanged
serviceaccount/ingress-nginx unchanged
configmap/ingress-nginx-controller unchanged
configmap/tcp-services unchanged
configmap/udp-services unchanged
clusterrole.rbac.authorization.k8s.io/ingress-nginx unchanged
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged
role.rbac.authorization.k8s.io/ingress-nginx unchanged
rolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged
service/ingress-nginx-controller-admission unchanged
service/ingress-nginx-controller unchanged
deployment.apps/ingress-nginx-controller configured
ingressclass.networking.k8s.io/nginx unchanged
validatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission configured
serviceaccount/ingress-nginx-admission unchanged
clusterrole.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
role.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
rolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged

stderr:
Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-create\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-create\"},\"spec\":{\"containers\":[{\"args\":[\"create\",\"--host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc\",\"--namespace=$(POD_NAMESPACE)\",\"--secret-name=ingress-nginx-admission\"],\"env\":[{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"create\"}],\"restartPolicy\":\"OnFailure\",\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":2000},\"serviceAccountName\":\"ingress-nginx-admission\"}}}}\n"},"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"template":{"metadata":{"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"$setElementOrder/containers":[{"name":"create"}],"containers":[{"image":"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660","name":"create","securityContext":null}],"nodeSelector":null,"securityContext":{"runAsNonRoot":true,"runAsUser":2000}}}}}
to:
Resource: "batch/v1, Resource=jobs", GroupVersionKind: "batch/v1, Kind=Job"
Name: "ingress-nginx-admission-create", Namespace: "ingress-nginx"
for: "/etc/kubernetes/addons/ingress-deploy.yaml": Job.batch "ingress-nginx-admission-create" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-create", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app.kubernetes.io/component":"admission-webhook", "app.kubernetes.io/instance":"ingress-nginx", "app.kubernetes.io/name":"ingress-nginx", "controller-uid":"8d577ee3-188a-46d7-a283-795857a59e12", "job-name":"ingress-nginx-admission-create"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:"create", Image:"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660", Command:[]string(nil), Args:[]string{"create", "--host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc", "--namespace=$(POD_NAMESPACE)", "--secret-name=ingress-nginx-admission"}, WorkingDir:"", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar{core.EnvVar{Name:"POD_NAMESPACE", Value:"", ValueFrom:(*core.EnvVarSource)(0xc006bca580)}}, Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*core.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:"OnFailure", TerminationGracePeriodSeconds:(*int64)(0xc00d161080), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"ingress-nginx-admission", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", SecurityContext:(*core.PodSecurityContext)(0xc00a64cc80), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:"", Subdomain:"", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable
Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-patch\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-patch\"},\"spec\":{\"containers\":[{\"args\":[\"patch\",\"--webhook-name=ingress-nginx-admission\",\"--namespace=$(POD_NAMESPACE)\",\"--patch-mutating=false\",\"--secret-name=ingress-nginx-admission\",\"--patch-failure-policy=Fail\"],\"env\":[{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"patch\"}],\"restartPolicy\":\"OnFailure\",\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":2000},\"serviceAccountName\":\"ingress-nginx-admission\"}}}}\n"},"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"template":{"metadata":{"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"$setElementOrder/containers":[{"name":"patch"}],"containers":[{"image":"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660","name":"patch","securityContext":null}],"nodeSelector":null,"securityContext":{"runAsNonRoot":true,"runAsUser":2000}}}}}
to:
Resource: "batch/v1, Resource=jobs", GroupVersionKind: "batch/v1, Kind=Job"
Name: "ingress-nginx-admission-patch", Namespace: "ingress-nginx"
for: "/etc/kubernetes/addons/ingress-deploy.yaml": Job.batch "ingress-nginx-admission-patch" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-patch", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app.kubernetes.io/component":"admission-webhook", "app.kubernetes.io/instance":"ingress-nginx", "app.kubernetes.io/name":"ingress-nginx", "controller-uid":"9961db6d-58a8-4d49-a390-8d0d0f4d7d10", "job-name":"ingress-nginx-admission-patch"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:"patch", Image:"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660", Command:[]string(nil), Args:[]string{"patch", "--webhook-name=ingress-nginx-admission", "--namespace=$(POD_NAMESPACE)", "--patch-mutating=false", "--secret-name=ingress-nginx-admission", "--patch-failure-policy=Fail"}, WorkingDir:"", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar{core.EnvVar{Name:"POD_NAMESPACE", Value:"", ValueFrom:(*core.EnvVarSource)(0xc006bf0720)}}, Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*core.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:"OnFailure", TerminationGracePeriodSeconds:(*int64)(0xc00a4d4620), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"ingress-nginx-admission", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", SecurityContext:(*core.PodSecurityContext)(0xc00a8ca400), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:"", Subdomain:"", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable
I0125 22:13:55.319899   10760 ssh_runner.go:152] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml
W0125 22:13:56.150373   10760 addons.go:369] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:
namespace/ingress-nginx unchanged
serviceaccount/ingress-nginx unchanged
configmap/ingress-nginx-controller unchanged
configmap/tcp-services unchanged
configmap/udp-services unchanged
clusterrole.rbac.authorization.k8s.io/ingress-nginx unchanged
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged
role.rbac.authorization.k8s.io/ingress-nginx unchanged
rolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged
service/ingress-nginx-controller-admission unchanged
service/ingress-nginx-controller unchanged
deployment.apps/ingress-nginx-controller configured
ingressclass.networking.k8s.io/nginx unchanged
validatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission configured
serviceaccount/ingress-nginx-admission unchanged
clusterrole.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
role.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
rolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged

stderr:
Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-create\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-create\"},\"spec\":{\"containers\":[{\"args\":[\"create\",\"--host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc\",\"--namespace=$(POD_NAMESPACE)\",\"--secret-name=ingress-nginx-admission\"],\"env\":[{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"create\"}],\"restartPolicy\":\"OnFailure\",\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":2000},\"serviceAccountName\":\"ingress-nginx-admission\"}}}}\n"},"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"template":{"metadata":{"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"$setElementOrder/containers":[{"name":"create"}],"containers":[{"image":"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660","name":"create","securityContext":null}],"nodeSelector":null,"securityContext":{"runAsNonRoot":true,"runAsUser":2000}}}}}
to:
Resource: "batch/v1, Resource=jobs", GroupVersionKind: "batch/v1, Kind=Job"
Name: "ingress-nginx-admission-create", Namespace: "ingress-nginx"
for: "/etc/kubernetes/addons/ingress-deploy.yaml": Job.batch "ingress-nginx-admission-create" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-create", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app.kubernetes.io/component":"admission-webhook", "app.kubernetes.io/instance":"ingress-nginx", "app.kubernetes.io/name":"ingress-nginx", "controller-uid":"8d577ee3-188a-46d7-a283-795857a59e12", "job-name":"ingress-nginx-admission-create"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:"create", Image:"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660", Command:[]string(nil), Args:[]string{"create", "--host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc", "--namespace=$(POD_NAMESPACE)", "--secret-name=ingress-nginx-admission"}, WorkingDir:"", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar{core.EnvVar{Name:"POD_NAMESPACE", Value:"", ValueFrom:(*core.EnvVarSource)(0xc006785de0)}}, Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*core.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:"OnFailure", TerminationGracePeriodSeconds:(*int64)(0xc00d19b780), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"ingress-nginx-admission", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", SecurityContext:(*core.PodSecurityContext)(0xc00a820b80), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:"", Subdomain:"", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable
Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-patch\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-patch\"},\"spec\":{\"containers\":[{\"args\":[\"patch\",\"--webhook-name=ingress-nginx-admission\",\"--namespace=$(POD_NAMESPACE)\",\"--patch-mutating=false\",\"--secret-name=ingress-nginx-admission\",\"--patch-failure-policy=Fail\"],\"env\":[{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"patch\"}],\"restartPolicy\":\"OnFailure\",\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":2000},\"serviceAccountName\":\"ingress-nginx-admission\"}}}}\n"},"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"template":{"metadata":{"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"$setElementOrder/containers":[{"name":"patch"}],"containers":[{"image":"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660","name":"patch","securityContext":null}],"nodeSelector":null,"securityContext":{"runAsNonRoot":true,"runAsUser":2000}}}}}
to:
Resource: "batch/v1, Resource=jobs", GroupVersionKind: "batch/v1, Kind=Job"
Name: "ingress-nginx-admission-patch", Namespace: "ingress-nginx"
for: "/etc/kubernetes/addons/ingress-deploy.yaml": Job.batch "ingress-nginx-admission-patch" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-patch", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app.kubernetes.io/component":"admission-webhook", "app.kubernetes.io/instance":"ingress-nginx", "app.kubernetes.io/name":"ingress-nginx", "controller-uid":"9961db6d-58a8-4d49-a390-8d0d0f4d7d10", "job-name":"ingress-nginx-admission-patch"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:"patch", Image:"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660", Command:[]string(nil), Args:[]string{"patch", "--webhook-name=ingress-nginx-admission", "--namespace=$(POD_NAMESPACE)", "--patch-mutating=false", "--secret-name=ingress-nginx-admission", "--patch-failure-policy=Fail"}, WorkingDir:"", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar{core.EnvVar{Name:"POD_NAMESPACE", Value:"", ValueFrom:(*core.EnvVarSource)(0xc0069088e0)}}, Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*core.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:"OnFailure", TerminationGracePeriodSeconds:(*int64)(0xc00d291ef0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"ingress-nginx-admission", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", SecurityContext:(*core.PodSecurityContext)(0xc00ac3a600), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:"", Subdomain:"", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable
I0125 22:13:56.150373   10760 retry.go:31] will retry after 2.804311738s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:
namespace/ingress-nginx unchanged
serviceaccount/ingress-nginx unchanged
configmap/ingress-nginx-controller unchanged
configmap/tcp-services unchanged
configmap/udp-services unchanged
clusterrole.rbac.authorization.k8s.io/ingress-nginx unchanged
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged
role.rbac.authorization.k8s.io/ingress-nginx unchanged
rolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged
service/ingress-nginx-controller-admission unchanged
service/ingress-nginx-controller unchanged
deployment.apps/ingress-nginx-controller configured
ingressclass.networking.k8s.io/nginx unchanged
validatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission configured
serviceaccount/ingress-nginx-admission unchanged
clusterrole.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
role.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
rolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged

stderr:
Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-create\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-create\"},\"spec\":{\"containers\":[{\"args\":[\"create\",\"--host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc\",\"--namespace=$(POD_NAMESPACE)\",\"--secret-name=ingress-nginx-admission\"],\"env\":[{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"create\"}],\"restartPolicy\":\"OnFailure\",\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":2000},\"serviceAccountName\":\"ingress-nginx-admission\"}}}}\n"},"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"template":{"metadata":{"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"$setElementOrder/containers":[{"name":"create"}],"containers":[{"image":"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660","name":"create","securityContext":null}],"nodeSelector":null,"securityContext":{"runAsNonRoot":true,"runAsUser":2000}}}}}
to:
Resource: "batch/v1, Resource=jobs", GroupVersionKind: "batch/v1, Kind=Job"
Name: "ingress-nginx-admission-create", Namespace: "ingress-nginx"
for: "/etc/kubernetes/addons/ingress-deploy.yaml": Job.batch "ingress-nginx-admission-create" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-create", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app.kubernetes.io/component":"admission-webhook", "app.kubernetes.io/instance":"ingress-nginx", "app.kubernetes.io/name":"ingress-nginx", "controller-uid":"8d577ee3-188a-46d7-a283-795857a59e12", "job-name":"ingress-nginx-admission-create"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:"create", Image:"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660", Command:[]string(nil), Args:[]string{"create", "--host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc", "--namespace=$(POD_NAMESPACE)", "--secret-name=ingress-nginx-admission"}, WorkingDir:"", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar{core.EnvVar{Name:"POD_NAMESPACE", Value:"", ValueFrom:(*core.EnvVarSource)(0xc006785de0)}}, Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*core.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:"OnFailure", TerminationGracePeriodSeconds:(*int64)(0xc00d19b780), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"ingress-nginx-admission", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", SecurityContext:(*core.PodSecurityContext)(0xc00a820b80), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:"", Subdomain:"", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable
Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-patch\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-patch\"},\"spec\":{\"containers\":[{\"args\":[\"patch\",\"--webhook-name=ingress-nginx-admission\",\"--namespace=$(POD_NAMESPACE)\",\"--patch-mutating=false\",\"--secret-name=ingress-nginx-admission\",\"--patch-failure-policy=Fail\"],\"env\":[{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"patch\"}],\"restartPolicy\":\"OnFailure\",\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":2000},\"serviceAccountName\":\"ingress-nginx-admission\"}}}}\n"},"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"template":{"metadata":{"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"$setElementOrder/containers":[{"name":"patch"}],"containers":[{"image":"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660","name":"patch","securityContext":null}],"nodeSelector":null,"securityContext":{"runAsNonRoot":true,"runAsUser":2000}}}}}
to:
Resource: "batch/v1, Resource=jobs", GroupVersionKind: "batch/v1, Kind=Job"
Name: "ingress-nginx-admission-patch", Namespace: "ingress-nginx"
for: "/etc/kubernetes/addons/ingress-deploy.yaml": Job.batch "ingress-nginx-admission-patch" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-patch", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app.kubernetes.io/component":"admission-webhook", "app.kubernetes.io/instance":"ingress-nginx", "app.kubernetes.io/name":"ingress-nginx", "controller-uid":"9961db6d-58a8-4d49-a390-8d0d0f4d7d10", "job-name":"ingress-nginx-admission-patch"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:"patch", Image:"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660", Command:[]string(nil), Args:[]string{"patch", "--webhook-name=ingress-nginx-admission", "--namespace=$(POD_NAMESPACE)", "--patch-mutating=false", "--secret-name=ingress-nginx-admission", "--patch-failure-policy=Fail"}, WorkingDir:"", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar{core.EnvVar{Name:"POD_NAMESPACE", Value:"", ValueFrom:(*core.EnvVarSource)(0xc0069088e0)}}, Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*core.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:"OnFailure", TerminationGracePeriodSeconds:(*int64)(0xc00d291ef0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"ingress-nginx-admission", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", SecurityContext:(*core.PodSecurityContext)(0xc00ac3a600), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:"", Subdomain:"", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable
I0125 22:13:58.975976   10760 ssh_runner.go:152] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml
I0125 22:14:00.065186   10760 ssh_runner.go:192] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml: (1.0892104s)
W0125 22:14:00.065186   10760 addons.go:369] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:
namespace/ingress-nginx unchanged
serviceaccount/ingress-nginx unchanged
configmap/ingress-nginx-controller unchanged
configmap/tcp-services unchanged
configmap/udp-services unchanged
clusterrole.rbac.authorization.k8s.io/ingress-nginx unchanged
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged
role.rbac.authorization.k8s.io/ingress-nginx unchanged
rolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged
service/ingress-nginx-controller-admission unchanged
service/ingress-nginx-controller unchanged
deployment.apps/ingress-nginx-controller configured
ingressclass.networking.k8s.io/nginx unchanged
validatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission configured
serviceaccount/ingress-nginx-admission unchanged
clusterrole.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
role.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
rolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged

stderr:
Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-create\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-create\"},\"spec\":{\"containers\":[{\"args\":[\"create\",\"--host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc\",\"--namespace=$(POD_NAMESPACE)\",\"--secret-name=ingress-nginx-admission\"],\"env\":[{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"create\"}],\"restartPolicy\":\"OnFailure\",\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":2000},\"serviceAccountName\":\"ingress-nginx-admission\"}}}}\n"},"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"template":{"metadata":{"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"$setElementOrder/containers":[{"name":"create"}],"containers":[{"image":"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660","name":"create","securityContext":null}],"nodeSelector":null,"securityContext":{"runAsNonRoot":true,"runAsUser":2000}}}}}
to:
Resource: "batch/v1, Resource=jobs", GroupVersionKind: "batch/v1, Kind=Job"
Name: "ingress-nginx-admission-create", Namespace: "ingress-nginx"
for: "/etc/kubernetes/addons/ingress-deploy.yaml": Job.batch "ingress-nginx-admission-create" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-create", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app.kubernetes.io/component":"admission-webhook", "app.kubernetes.io/instance":"ingress-nginx", "app.kubernetes.io/name":"ingress-nginx", "controller-uid":"8d577ee3-188a-46d7-a283-795857a59e12", "job-name":"ingress-nginx-admission-create"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:"create", Image:"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660", Command:[]string(nil), Args:[]string{"create", "--host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc", "--namespace=$(POD_NAMESPACE)", "--secret-name=ingress-nginx-admission"}, WorkingDir:"", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar{core.EnvVar{Name:"POD_NAMESPACE", Value:"", ValueFrom:(*core.EnvVarSource)(0xc0066d54e0)}}, Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*core.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:"OnFailure", TerminationGracePeriodSeconds:(*int64)(0xc00d2185f0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"ingress-nginx-admission", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", SecurityContext:(*core.PodSecurityContext)(0xc00b408500), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:"", Subdomain:"", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable
Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-patch\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-patch\"},\"spec\":{\"containers\":[{\"args\":[\"patch\",\"--webhook-name=ingress-nginx-admission\",\"--namespace=$(POD_NAMESPACE)\",\"--patch-mutating=false\",\"--secret-name=ingress-nginx-admission\",\"--patch-failure-policy=Fail\"],\"env\":[{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"patch\"}],\"restartPolicy\":\"OnFailure\",\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":2000},\"serviceAccountName\":\"ingress-nginx-admission\"}}}}\n"},"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"template":{"metadata":{"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"$setElementOrder/containers":[{"name":"patch"}],"containers":[{"image":"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660","name":"patch","securityContext":null}],"nodeSelector":null,"securityContext":{"runAsNonRoot":true,"runAsUser":2000}}}}}
to:
Resource: "batch/v1, Resource=jobs", GroupVersionKind: "batch/v1, Kind=Job"
Name: "ingress-nginx-admission-patch", Namespace: "ingress-nginx"
for: "/etc/kubernetes/addons/ingress-deploy.yaml": Job.batch "ingress-nginx-admission-patch" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-patch", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app.kubernetes.io/component":"admission-webhook", "app.kubernetes.io/instance":"ingress-nginx", "app.kubernetes.io/name":"ingress-nginx", "controller-uid":"9961db6d-58a8-4d49-a390-8d0d0f4d7d10", "job-name":"ingress-nginx-admission-patch"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:"patch", Image:"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660", Command:[]string(nil), Args:[]string{"patch", "--webhook-name=ingress-nginx-admission", "--namespace=$(POD_NAMESPACE)", "--patch-mutating=false", "--secret-name=ingress-nginx-admission", "--patch-failure-policy=Fail"}, WorkingDir:"", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar{core.EnvVar{Name:"POD_NAMESPACE", Value:"", ValueFrom:(*core.EnvVarSource)(0xc006de6ec0)}}, Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*core.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:"OnFailure", TerminationGracePeriodSeconds:(*int64)(0xc00d3a66d0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"ingress-nginx-admission", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", SecurityContext:(*core.PodSecurityContext)(0xc00b448300), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:"", Subdomain:"", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable
I0125 22:14:00.065186   10760 retry.go:31] will retry after 3.824918958s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:
namespace/ingress-nginx unchanged
serviceaccount/ingress-nginx unchanged
configmap/ingress-nginx-controller unchanged
configmap/tcp-services unchanged
configmap/udp-services unchanged
clusterrole.rbac.authorization.k8s.io/ingress-nginx unchanged
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged
role.rbac.authorization.k8s.io/ingress-nginx unchanged
rolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged
service/ingress-nginx-controller-admission unchanged
service/ingress-nginx-controller unchanged
deployment.apps/ingress-nginx-controller configured
ingressclass.networking.k8s.io/nginx unchanged
validatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission configured
serviceaccount/ingress-nginx-admission unchanged
clusterrole.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
role.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
rolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged

stderr:
Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-create\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-create\"},\"spec\":{\"containers\":[{\"args\":[\"create\",\"--host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc\",\"--namespace=$(POD_NAMESPACE)\",\"--secret-name=ingress-nginx-admission\"],\"env\":[{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"create\"}],\"restartPolicy\":\"OnFailure\",\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":2000},\"serviceAccountName\":\"ingress-nginx-admission\"}}}}\n"},"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"template":{"metadata":{"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"$setElementOrder/containers":[{"name":"create"}],"containers":[{"image":"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660","name":"create","securityContext":null}],"nodeSelector":null,"securityContext":{"runAsNonRoot":true,"runAsUser":2000}}}}}
to:
Resource: "batch/v1, Resource=jobs", GroupVersionKind: "batch/v1, Kind=Job"
Name: "ingress-nginx-admission-create", Namespace: "ingress-nginx"
for: "/etc/kubernetes/addons/ingress-deploy.yaml": Job.batch "ingress-nginx-admission-create" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-create", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app.kubernetes.io/component":"admission-webhook", "app.kubernetes.io/instance":"ingress-nginx", "app.kubernetes.io/name":"ingress-nginx", "controller-uid":"8d577ee3-188a-46d7-a283-795857a59e12", "job-name":"ingress-nginx-admission-create"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:"create", Image:"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660", Command:[]string(nil), Args:[]string{"create", "--host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc", "--namespace=$(POD_NAMESPACE)", "--secret-name=ingress-nginx-admission"}, WorkingDir:"", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar{core.EnvVar{Name:"POD_NAMESPACE", Value:"", ValueFrom:(*core.EnvVarSource)(0xc0066d54e0)}}, Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*core.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:"OnFailure", TerminationGracePeriodSeconds:(*int64)(0xc00d2185f0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"ingress-nginx-admission", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", SecurityContext:(*core.PodSecurityContext)(0xc00b408500), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:"", Subdomain:"", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable
Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-patch\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-patch\"},\"spec\":{\"containers\":[{\"args\":[\"patch\",\"--webhook-name=ingress-nginx-admission\",\"--namespace=$(POD_NAMESPACE)\",\"--patch-mutating=false\",\"--secret-name=ingress-nginx-admission\",\"--patch-failure-policy=Fail\"],\"env\":[{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"patch\"}],\"restartPolicy\":\"OnFailure\",\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":2000},\"serviceAccountName\":\"ingress-nginx-admission\"}}}}\n"},"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"template":{"metadata":{"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"$setElementOrder/containers":[{"name":"patch"}],"containers":[{"image":"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660","name":"patch","securityContext":null}],"nodeSelector":null,"securityContext":{"runAsNonRoot":true,"runAsUser":2000}}}}}
to:
Resource: "batch/v1, Resource=jobs", GroupVersionKind: "batch/v1, Kind=Job"
Name: "ingress-nginx-admission-patch", Namespace: "ingress-nginx"
for: "/etc/kubernetes/addons/ingress-deploy.yaml": Job.batch "ingress-nginx-admission-patch" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-patch", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app.kubernetes.io/component":"admission-webhook", "app.kubernetes.io/instance":"ingress-nginx", "app.kubernetes.io/name":"ingress-nginx", "controller-uid":"9961db6d-58a8-4d49-a390-8d0d0f4d7d10", "job-name":"ingress-nginx-admission-patch"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:"patch", Image:"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660", Command:[]string(nil), Args:[]string{"patch", "--webhook-name=ingress-nginx-admission", "--namespace=$(POD_NAMESPACE)", "--patch-mutating=false", "--secret-name=ingress-nginx-admission", "--patch-failure-policy=Fail"}, WorkingDir:"", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar{core.EnvVar{Name:"POD_NAMESPACE", Value:"", ValueFrom:(*core.EnvVarSource)(0xc006de6ec0)}}, Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*core.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:"OnFailure", TerminationGracePeriodSeconds:(*int64)(0xc00d3a66d0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"ingress-nginx-admission", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", SecurityContext:(*core.PodSecurityContext)(0xc00b448300), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:"", Subdomain:"", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable
I0125 22:14:03.905858   10760 ssh_runner.go:152] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml
I0125 22:14:05.336016   10760 ssh_runner.go:192] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml: (1.4301579s)
W0125 22:14:05.336532   10760 addons.go:369] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:
namespace/ingress-nginx unchanged
serviceaccount/ingress-nginx unchanged
configmap/ingress-nginx-controller unchanged
configmap/tcp-services unchanged
configmap/udp-services unchanged
clusterrole.rbac.authorization.k8s.io/ingress-nginx unchanged
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged
role.rbac.authorization.k8s.io/ingress-nginx unchanged
rolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged
service/ingress-nginx-controller-admission unchanged
service/ingress-nginx-controller unchanged
deployment.apps/ingress-nginx-controller configured
ingressclass.networking.k8s.io/nginx unchanged
validatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission configured
serviceaccount/ingress-nginx-admission unchanged
clusterrole.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
role.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
rolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged

stderr:
Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-create\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-create\"},\"spec\":{\"containers\":[{\"args\":[\"create\",\"--host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc\",\"--namespace=$(POD_NAMESPACE)\",\"--secret-name=ingress-nginx-admission\"],\"env\":[{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"create\"}],\"restartPolicy\":\"OnFailure\",\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":2000},\"serviceAccountName\":\"ingress-nginx-admission\"}}}}\n"},"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"template":{"metadata":{"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"$setElementOrder/containers":[{"name":"create"}],"containers":[{"image":"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660","name":"create","securityContext":null}],"nodeSelector":null,"securityContext":{"runAsNonRoot":true,"runAsUser":2000}}}}}
to:
Resource: "batch/v1, Resource=jobs", GroupVersionKind: "batch/v1, Kind=Job"
Name: "ingress-nginx-admission-create", Namespace: "ingress-nginx"
for: "/etc/kubernetes/addons/ingress-deploy.yaml": Job.batch "ingress-nginx-admission-create" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-create", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app.kubernetes.io/component":"admission-webhook", "app.kubernetes.io/instance":"ingress-nginx", "app.kubernetes.io/name":"ingress-nginx", "controller-uid":"8d577ee3-188a-46d7-a283-795857a59e12", "job-name":"ingress-nginx-admission-create"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:"create", Image:"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660", Command:[]string(nil), Args:[]string{"create", "--host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc", "--namespace=$(POD_NAMESPACE)", "--secret-name=ingress-nginx-admission"}, WorkingDir:"", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar{core.EnvVar{Name:"POD_NAMESPACE", Value:"", ValueFrom:(*core.EnvVarSource)(0xc008802da0)}}, Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*core.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:"OnFailure", TerminationGracePeriodSeconds:(*int64)(0xc00d50c3e0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"ingress-nginx-admission", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", SecurityContext:(*core.PodSecurityContext)(0xc00be69b00), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:"", Subdomain:"", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable
Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-patch\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-patch\"},\"spec\":{\"containers\":[{\"args\":[\"patch\",\"--webhook-name=ingress-nginx-admission\",\"--namespace=$(POD_NAMESPACE)\",\"--patch-mutating=false\",\"--secret-name=ingress-nginx-admission\",\"--patch-failure-policy=Fail\"],\"env\":[{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"patch\"}],\"restartPolicy\":\"OnFailure\",\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":2000},\"serviceAccountName\":\"ingress-nginx-admission\"}}}}\n"},"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"template":{"metadata":{"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"$setElementOrder/containers":[{"name":"patch"}],"containers":[{"image":"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660","name":"patch","securityContext":null}],"nodeSelector":null,"securityContext":{"runAsNonRoot":true,"runAsUser":2000}}}}}
to:
Resource: "batch/v1, Resource=jobs", GroupVersionKind: "batch/v1, Kind=Job"
Name: "ingress-nginx-admission-patch", Namespace: "ingress-nginx"
for: "/etc/kubernetes/addons/ingress-deploy.yaml": Job.batch "ingress-nginx-admission-patch" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-patch", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app.kubernetes.io/component":"admission-webhook", "app.kubernetes.io/instance":"ingress-nginx", "app.kubernetes.io/name":"ingress-nginx", "controller-uid":"9961db6d-58a8-4d49-a390-8d0d0f4d7d10", "job-name":"ingress-nginx-admission-patch"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:"patch", Image:"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660", Command:[]string(nil), Args:[]string{"patch", "--webhook-name=ingress-nginx-admission", "--namespace=$(POD_NAMESPACE)", "--patch-mutating=false", "--secret-name=ingress-nginx-admission", "--patch-failure-policy=Fail"}, WorkingDir:"", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar{core.EnvVar{Name:"POD_NAMESPACE", Value:"", ValueFrom:(*core.EnvVarSource)(0xc006ad36a0)}}, Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*core.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:"OnFailure", TerminationGracePeriodSeconds:(*int64)(0xc00d415640), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"ingress-nginx-admission", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", SecurityContext:(*core.PodSecurityContext)(0xc00bf0e400), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:"", Subdomain:"", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable
I0125 22:14:05.336532   10760 retry.go:31] will retry after 7.69743562s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:
namespace/ingress-nginx unchanged
serviceaccount/ingress-nginx unchanged
configmap/ingress-nginx-controller unchanged
configmap/tcp-services unchanged
configmap/udp-services unchanged
clusterrole.rbac.authorization.k8s.io/ingress-nginx unchanged
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged
role.rbac.authorization.k8s.io/ingress-nginx unchanged
rolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged
service/ingress-nginx-controller-admission unchanged
service/ingress-nginx-controller unchanged
deployment.apps/ingress-nginx-controller configured
ingressclass.networking.k8s.io/nginx unchanged
validatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission configured
serviceaccount/ingress-nginx-admission unchanged
clusterrole.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
role.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
rolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged

stderr:
Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-create\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-create\"},\"spec\":{\"containers\":[{\"args\":[\"create\",\"--host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc\",\"--namespace=$(POD_NAMESPACE)\",\"--secret-name=ingress-nginx-admission\"],\"env\":[{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"create\"}],\"restartPolicy\":\"OnFailure\",\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":2000},\"serviceAccountName\":\"ingress-nginx-admission\"}}}}\n"},"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"template":{"metadata":{"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"$setElementOrder/containers":[{"name":"create"}],"containers":[{"image":"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660","name":"create","securityContext":null}],"nodeSelector":null,"securityContext":{"runAsNonRoot":true,"runAsUser":2000}}}}}
to:
Resource: "batch/v1, Resource=jobs", GroupVersionKind: "batch/v1, Kind=Job"
Name: "ingress-nginx-admission-create", Namespace: "ingress-nginx"
for: "/etc/kubernetes/addons/ingress-deploy.yaml": Job.batch "ingress-nginx-admission-create" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-create", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app.kubernetes.io/component":"admission-webhook", "app.kubernetes.io/instance":"ingress-nginx", "app.kubernetes.io/name":"ingress-nginx", "controller-uid":"8d577ee3-188a-46d7-a283-795857a59e12", "job-name":"ingress-nginx-admission-create"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:"create", Image:"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660", Command:[]string(nil), Args:[]string{"create", "--host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc", "--namespace=$(POD_NAMESPACE)", "--secret-name=ingress-nginx-admission"}, WorkingDir:"", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar{core.EnvVar{Name:"POD_NAMESPACE", Value:"", ValueFrom:(*core.EnvVarSource)(0xc008802da0)}}, Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*core.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:"OnFailure", TerminationGracePeriodSeconds:(*int64)(0xc00d50c3e0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"ingress-nginx-admission", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", SecurityContext:(*core.PodSecurityContext)(0xc00be69b00), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:"", Subdomain:"", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable
Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-patch\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-patch\"},\"spec\":{\"containers\":[{\"args\":[\"patch\",\"--webhook-name=ingress-nginx-admission\",\"--namespace=$(POD_NAMESPACE)\",\"--patch-mutating=false\",\"--secret-name=ingress-nginx-admission\",\"--patch-failure-policy=Fail\"],\"env\":[{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"patch\"}],\"restartPolicy\":\"OnFailure\",\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":2000},\"serviceAccountName\":\"ingress-nginx-admission\"}}}}\n"},"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"template":{"metadata":{"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"$setElementOrder/containers":[{"name":"patch"}],"containers":[{"image":"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660","name":"patch","securityContext":null}],"nodeSelector":null,"securityContext":{"runAsNonRoot":true,"runAsUser":2000}}}}}
to:
Resource: "batch/v1, Resource=jobs", GroupVersionKind: "batch/v1, Kind=Job"
Name: "ingress-nginx-admission-patch", Namespace: "ingress-nginx"
for: "/etc/kubernetes/addons/ingress-deploy.yaml": Job.batch "ingress-nginx-admission-patch" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-patch", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app.kubernetes.io/component":"admission-webhook", "app.kubernetes.io/instance":"ingress-nginx", "app.kubernetes.io/name":"ingress-nginx", "controller-uid":"9961db6d-58a8-4d49-a390-8d0d0f4d7d10", "job-name":"ingress-nginx-admission-patch"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:"patch", Image:"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660", Command:[]string(nil), Args:[]string{"patch", "--webhook-name=ingress-nginx-admission", "--namespace=$(POD_NAMESPACE)", "--patch-mutating=false", "--secret-name=ingress-nginx-admission", "--patch-failure-policy=Fail"}, WorkingDir:"", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar{core.EnvVar{Name:"POD_NAMESPACE", Value:"", ValueFrom:(*core.EnvVarSource)(0xc006ad36a0)}}, Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*core.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:"OnFailure", TerminationGracePeriodSeconds:(*int64)(0xc00d415640), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"ingress-nginx-admission", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", SecurityContext:(*core.PodSecurityContext)(0xc00bf0e400), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:"", Subdomain:"", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable
I0125 22:14:13.046060   10760 ssh_runner.go:152] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml
I0125 22:14:14.672684   10760 ssh_runner.go:192] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml: (1.6266237s)
W0125 22:14:14.672684   10760 addons.go:369] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:
namespace/ingress-nginx unchanged
serviceaccount/ingress-nginx unchanged
configmap/ingress-nginx-controller unchanged
configmap/tcp-services unchanged
configmap/udp-services unchanged
clusterrole.rbac.authorization.k8s.io/ingress-nginx unchanged
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged
role.rbac.authorization.k8s.io/ingress-nginx unchanged
rolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged
service/ingress-nginx-controller-admission unchanged
service/ingress-nginx-controller unchanged
deployment.apps/ingress-nginx-controller configured
ingressclass.networking.k8s.io/nginx unchanged
validatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission configured
serviceaccount/ingress-nginx-admission unchanged
clusterrole.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
role.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
rolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged

stderr:
Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-create\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-create\"},\"spec\":{\"containers\":[{\"args\":[\"create\",\"--host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc\",\"--namespace=$(POD_NAMESPACE)\",\"--secret-name=ingress-nginx-admission\"],\"env\":[{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"create\"}],\"restartPolicy\":\"OnFailure\",\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":2000},\"serviceAccountName\":\"ingress-nginx-admission\"}}}}\n"},"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"template":{"metadata":{"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"$setElementOrder/containers":[{"name":"create"}],"containers":[{"image":"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660","name":"create","securityContext":null}],"nodeSelector":null,"securityContext":{"runAsNonRoot":true,"runAsUser":2000}}}}}
to:
Resource: "batch/v1, Resource=jobs", GroupVersionKind: "batch/v1, Kind=Job"
Name: "ingress-nginx-admission-create", Namespace: "ingress-nginx"
for: "/etc/kubernetes/addons/ingress-deploy.yaml": Job.batch "ingress-nginx-admission-create" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-create", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app.kubernetes.io/component":"admission-webhook", "app.kubernetes.io/instance":"ingress-nginx", "app.kubernetes.io/name":"ingress-nginx", "controller-uid":"8d577ee3-188a-46d7-a283-795857a59e12", "job-name":"ingress-nginx-admission-create"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:"create", Image:"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660", Command:[]string(nil), Args:[]string{"create", "--host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc", "--namespace=$(POD_NAMESPACE)", "--secret-name=ingress-nginx-admission"}, WorkingDir:"", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar{core.EnvVar{Name:"POD_NAMESPACE", Value:"", ValueFrom:(*core.EnvVarSource)(0xc00e800a00)}}, Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*core.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:"OnFailure", TerminationGracePeriodSeconds:(*int64)(0xc00d6e4b90), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"ingress-nginx-admission", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", SecurityContext:(*core.PodSecurityContext)(0xc00e765680), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:"", Subdomain:"", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable
Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-patch\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-patch\"},\"spec\":{\"containers\":[{\"args\":[\"patch\",\"--webhook-name=ingress-nginx-admission\",\"--namespace=$(POD_NAMESPACE)\",\"--patch-mutating=false\",\"--secret-name=ingress-nginx-admission\",\"--patch-failure-policy=Fail\"],\"env\":[{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"patch\"}],\"restartPolicy\":\"OnFailure\",\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":2000},\"serviceAccountName\":\"ingress-nginx-admission\"}}}}\n"},"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"template":{"metadata":{"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"$setElementOrder/containers":[{"name":"patch"}],"containers":[{"image":"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660","name":"patch","securityContext":null}],"nodeSelector":null,"securityContext":{"runAsNonRoot":true,"runAsUser":2000}}}}}
to:
Resource: "batch/v1, Resource=jobs", GroupVersionKind: "batch/v1, Kind=Job"
Name: "ingress-nginx-admission-patch", Namespace: "ingress-nginx"
for: "/etc/kubernetes/addons/ingress-deploy.yaml": Job.batch "ingress-nginx-admission-patch" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-patch", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app.kubernetes.io/component":"admission-webhook", "app.kubernetes.io/instance":"ingress-nginx", "app.kubernetes.io/name":"ingress-nginx", "controller-uid":"9961db6d-58a8-4d49-a390-8d0d0f4d7d10", "job-name":"ingress-nginx-admission-patch"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:"patch", Image:"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660", Command:[]string(nil), Args:[]string{"patch", "--webhook-name=ingress-nginx-admission", "--namespace=$(POD_NAMESPACE)", "--patch-mutating=false", "--secret-name=ingress-nginx-admission", "--patch-failure-policy=Fail"}, WorkingDir:"", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar{core.EnvVar{Name:"POD_NAMESPACE", Value:"", ValueFrom:(*core.EnvVarSource)(0xc00e84b0a0)}}, Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*core.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:"OnFailure", TerminationGracePeriodSeconds:(*int64)(0xc00d672670), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"ingress-nginx-admission", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", SecurityContext:(*core.PodSecurityContext)(0xc00e773480), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:"", Subdomain:"", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable
I0125 22:14:14.672684   10760 retry.go:31] will retry after 14.635568968s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:
namespace/ingress-nginx unchanged
serviceaccount/ingress-nginx unchanged
configmap/ingress-nginx-controller unchanged
configmap/tcp-services unchanged
configmap/udp-services unchanged
clusterrole.rbac.authorization.k8s.io/ingress-nginx unchanged
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged
role.rbac.authorization.k8s.io/ingress-nginx unchanged
rolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged
service/ingress-nginx-controller-admission unchanged
service/ingress-nginx-controller unchanged
deployment.apps/ingress-nginx-controller configured
ingressclass.networking.k8s.io/nginx unchanged
validatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission configured
serviceaccount/ingress-nginx-admission unchanged
clusterrole.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
role.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
rolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged

stderr:
Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-create\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-create\"},\"spec\":{\"containers\":[{\"args\":[\"create\",\"--host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc\",\"--namespace=$(POD_NAMESPACE)\",\"--secret-name=ingress-nginx-admission\"],\"env\":[{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"create\"}],\"restartPolicy\":\"OnFailure\",\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":2000},\"serviceAccountName\":\"ingress-nginx-admission\"}}}}\n"},"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"template":{"metadata":{"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"$setElementOrder/containers":[{"name":"create"}],"containers":[{"image":"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660","name":"create","securityContext":null}],"nodeSelector":null,"securityContext":{"runAsNonRoot":true,"runAsUser":2000}}}}}
to:
Resource: "batch/v1, Resource=jobs", GroupVersionKind: "batch/v1, Kind=Job"
Name: "ingress-nginx-admission-create", Namespace: "ingress-nginx"
for: "/etc/kubernetes/addons/ingress-deploy.yaml": Job.batch "ingress-nginx-admission-create" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-create", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app.kubernetes.io/component":"admission-webhook", "app.kubernetes.io/instance":"ingress-nginx", "app.kubernetes.io/name":"ingress-nginx", "controller-uid":"8d577ee3-188a-46d7-a283-795857a59e12", "job-name":"ingress-nginx-admission-create"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:"create", Image:"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660", Command:[]string(nil), Args:[]string{"create", "--host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc", "--namespace=$(POD_NAMESPACE)", "--secret-name=ingress-nginx-admission"}, WorkingDir:"", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar{core.EnvVar{Name:"POD_NAMESPACE", Value:"", ValueFrom:(*core.EnvVarSource)(0xc00e800a00)}}, Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*core.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:"OnFailure", TerminationGracePeriodSeconds:(*int64)(0xc00d6e4b90), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"ingress-nginx-admission", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", SecurityContext:(*core.PodSecurityContext)(0xc00e765680), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:"", Subdomain:"", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable
Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-patch\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-patch\"},\"spec\":{\"containers\":[{\"args\":[\"patch\",\"--webhook-name=ingress-nginx-admission\",\"--namespace=$(POD_NAMESPACE)\",\"--patch-mutating=false\",\"--secret-name=ingress-nginx-admission\",\"--patch-failure-policy=Fail\"],\"env\":[{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"patch\"}],\"restartPolicy\":\"OnFailure\",\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":2000},\"serviceAccountName\":\"ingress-nginx-admission\"}}}}\n"},"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"template":{"metadata":{"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"$setElementOrder/containers":[{"name":"patch"}],"containers":[{"image":"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660","name":"patch","securityContext":null}],"nodeSelector":null,"securityContext":{"runAsNonRoot":true,"runAsUser":2000}}}}}
to:
Resource: "batch/v1, Resource=jobs", GroupVersionKind: "batch/v1, Kind=Job"
Name: "ingress-nginx-admission-patch", Namespace: "ingress-nginx"
for: "/etc/kubernetes/addons/ingress-deploy.yaml": Job.batch "ingress-nginx-admission-patch" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-patch", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app.kubernetes.io/component":"admission-webhook", "app.kubernetes.io/instance":"ingress-nginx", "app.kubernetes.io/name":"ingress-nginx", "controller-uid":"9961db6d-58a8-4d49-a390-8d0d0f4d7d10", "job-name":"ingress-nginx-admission-patch"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:"patch", Image:"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660", Command:[]string(nil), Args:[]string{"patch", "--webhook-name=ingress-nginx-admission", "--namespace=$(POD_NAMESPACE)", "--patch-mutating=false", "--secret-name=ingress-nginx-admission", "--patch-failure-policy=Fail"}, WorkingDir:"", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar{core.EnvVar{Name:"POD_NAMESPACE", Value:"", ValueFrom:(*core.EnvVarSource)(0xc00e84b0a0)}}, Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*core.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:"OnFailure", TerminationGracePeriodSeconds:(*int64)(0xc00d672670), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"ingress-nginx-admission", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", SecurityContext:(*core.PodSecurityContext)(0xc00e773480), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:"", Subdomain:"", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable
I0125 22:14:29.316297   10760 ssh_runner.go:152] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml
W0125 22:14:29.884754   10760 addons.go:369] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:
namespace/ingress-nginx unchanged
serviceaccount/ingress-nginx unchanged
configmap/ingress-nginx-controller unchanged
configmap/tcp-services unchanged
configmap/udp-services unchanged
clusterrole.rbac.authorization.k8s.io/ingress-nginx unchanged
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged
role.rbac.authorization.k8s.io/ingress-nginx unchanged
rolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged
service/ingress-nginx-controller-admission unchanged
service/ingress-nginx-controller unchanged
deployment.apps/ingress-nginx-controller configured
ingressclass.networking.k8s.io/nginx unchanged
validatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission configured
serviceaccount/ingress-nginx-admission unchanged
clusterrole.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
role.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
rolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged

stderr:
Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-create\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-create\"},\"spec\":{\"containers\":[{\"args\":[\"create\",\"--host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc\",\"--namespace=$(POD_NAMESPACE)\",\"--secret-name=ingress-nginx-admission\"],\"env\":[{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"create\"}],\"restartPolicy\":\"OnFailure\",\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":2000},\"serviceAccountName\":\"ingress-nginx-admission\"}}}}\n"},"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"template":{"metadata":{"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"$setElementOrder/containers":[{"name":"create"}],"containers":[{"image":"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660","name":"create","securityContext":null}],"nodeSelector":null,"securityContext":{"runAsNonRoot":true,"runAsUser":2000}}}}}
to:
Resource: "batch/v1, Resource=jobs", GroupVersionKind: "batch/v1, Kind=Job"
Name: "ingress-nginx-admission-create", Namespace: "ingress-nginx"
for: "/etc/kubernetes/addons/ingress-deploy.yaml": Job.batch "ingress-nginx-admission-create" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-create", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app.kubernetes.io/component":"admission-webhook", "app.kubernetes.io/instance":"ingress-nginx", "app.kubernetes.io/name":"ingress-nginx", "controller-uid":"8d577ee3-188a-46d7-a283-795857a59e12", "job-name":"ingress-nginx-admission-create"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:"create", Image:"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660", Command:[]string(nil), Args:[]string{"create", "--host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc", "--namespace=$(POD_NAMESPACE)", "--secret-name=ingress-nginx-admission"}, WorkingDir:"", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar{core.EnvVar{Name:"POD_NAMESPACE", Value:"", ValueFrom:(*core.EnvVarSource)(0xc0104c2560)}}, Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*core.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:"OnFailure", TerminationGracePeriodSeconds:(*int64)(0xc00dd63a60), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"ingress-nginx-admission", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", SecurityContext:(*core.PodSecurityContext)(0xc00f23bf80), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:"", Subdomain:"", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable
Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-patch\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-patch\"},\"spec\":{\"containers\":[{\"args\":[\"patch\",\"--webhook-name=ingress-nginx-admission\",\"--namespace=$(POD_NAMESPACE)\",\"--patch-mutating=false\",\"--secret-name=ingress-nginx-admission\",\"--patch-failure-policy=Fail\"],\"env\":[{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"patch\"}],\"restartPolicy\":\"OnFailure\",\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":2000},\"serviceAccountName\":\"ingress-nginx-admission\"}}}}\n"},"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"template":{"metadata":{"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"$setElementOrder/containers":[{"name":"patch"}],"containers":[{"image":"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660","name":"patch","securityContext":null}],"nodeSelector":null,"securityContext":{"runAsNonRoot":true,"runAsUser":2000}}}}}
to:
Resource: "batch/v1, Resource=jobs", GroupVersionKind: "batch/v1, Kind=Job"
Name: "ingress-nginx-admission-patch", Namespace: "ingress-nginx"
for: "/etc/kubernetes/addons/ingress-deploy.yaml": Job.batch "ingress-nginx-admission-patch" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-patch", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app.kubernetes.io/component":"admission-webhook", "app.kubernetes.io/instance":"ingress-nginx", "app.kubernetes.io/name":"ingress-nginx", "controller-uid":"9961db6d-58a8-4d49-a390-8d0d0f4d7d10", "job-name":"ingress-nginx-admission-patch"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:"patch", Image:"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660", Command:[]string(nil), Args:[]string{"patch", "--webhook-name=ingress-nginx-admission", "--namespace=$(POD_NAMESPACE)", "--patch-mutating=false", "--secret-name=ingress-nginx-admission", "--patch-failure-policy=Fail"}, WorkingDir:"", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar{core.EnvVar{Name:"POD_NAMESPACE", Value:"", ValueFrom:(*core.EnvVarSource)(0xc010a50c40)}}, Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*core.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:"OnFailure", TerminationGracePeriodSeconds:(*int64)(0xc00defa940), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"ingress-nginx-admission", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", SecurityContext:(*core.PodSecurityContext)(0xc00f71de00), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:"", Subdomain:"", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable
I0125 22:14:29.884754   10760 retry.go:31] will retry after 28.406662371s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:
namespace/ingress-nginx unchanged
serviceaccount/ingress-nginx unchanged
configmap/ingress-nginx-controller unchanged
configmap/tcp-services unchanged
configmap/udp-services unchanged
clusterrole.rbac.authorization.k8s.io/ingress-nginx unchanged
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged
role.rbac.authorization.k8s.io/ingress-nginx unchanged
rolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged
service/ingress-nginx-controller-admission unchanged
service/ingress-nginx-controller unchanged
deployment.apps/ingress-nginx-controller configured
ingressclass.networking.k8s.io/nginx unchanged
validatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission configured
serviceaccount/ingress-nginx-admission unchanged
clusterrole.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
role.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
rolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged

stderr:
Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-create\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-create\"},\"spec\":{\"containers\":[{\"args\":[\"create\",\"--host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc\",\"--namespace=$(POD_NAMESPACE)\",\"--secret-name=ingress-nginx-admission\"],\"env\":[{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"create\"}],\"restartPolicy\":\"OnFailure\",\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":2000},\"serviceAccountName\":\"ingress-nginx-admission\"}}}}\n"},"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"template":{"metadata":{"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"$setElementOrder/containers":[{"name":"create"}],"containers":[{"image":"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660","name":"create","securityContext":null}],"nodeSelector":null,"securityContext":{"runAsNonRoot":true,"runAsUser":2000}}}}}
to:
Resource: "batch/v1, Resource=jobs", GroupVersionKind: "batch/v1, Kind=Job"
Name: "ingress-nginx-admission-create", Namespace: "ingress-nginx"
for: "/etc/kubernetes/addons/ingress-deploy.yaml": Job.batch "ingress-nginx-admission-create" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-create", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app.kubernetes.io/component":"admission-webhook", "app.kubernetes.io/instance":"ingress-nginx", "app.kubernetes.io/name":"ingress-nginx", "controller-uid":"8d577ee3-188a-46d7-a283-795857a59e12", "job-name":"ingress-nginx-admission-create"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:"create", Image:"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660", Command:[]string(nil), Args:[]string{"create", "--host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc", "--namespace=$(POD_NAMESPACE)", "--secret-name=ingress-nginx-admission"}, WorkingDir:"", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar{core.EnvVar{Name:"POD_NAMESPACE", Value:"", ValueFrom:(*core.EnvVarSource)(0xc0104c2560)}}, Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*core.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:"OnFailure", TerminationGracePeriodSeconds:(*int64)(0xc00dd63a60), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"ingress-nginx-admission", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", SecurityContext:(*core.PodSecurityContext)(0xc00f23bf80), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:"", Subdomain:"", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable
Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-patch\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-patch\"},\"spec\":{\"containers\":[{\"args\":[\"patch\",\"--webhook-name=ingress-nginx-admission\",\"--namespace=$(POD_NAMESPACE)\",\"--patch-mutating=false\",\"--secret-name=ingress-nginx-admission\",\"--patch-failure-policy=Fail\"],\"env\":[{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"patch\"}],\"restartPolicy\":\"OnFailure\",\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":2000},\"serviceAccountName\":\"ingress-nginx-admission\"}}}}\n"},"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"template":{"metadata":{"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"$setElementOrder/containers":[{"name":"patch"}],"containers":[{"image":"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660","name":"patch","securityContext":null}],"nodeSelector":null,"securityContext":{"runAsNonRoot":true,"runAsUser":2000}}}}}
to:
Resource: "batch/v1, Resource=jobs", GroupVersionKind: "batch/v1, Kind=Job"
Name: "ingress-nginx-admission-patch", Namespace: "ingress-nginx"
for: "/etc/kubernetes/addons/ingress-deploy.yaml": Job.batch "ingress-nginx-admission-patch" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-patch", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app.kubernetes.io/component":"admission-webhook", "app.kubernetes.io/instance":"ingress-nginx", "app.kubernetes.io/name":"ingress-nginx", "controller-uid":"9961db6d-58a8-4d49-a390-8d0d0f4d7d10", "job-name":"ingress-nginx-admission-patch"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:"patch", Image:"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660", Command:[]string(nil), Args:[]string{"patch", "--webhook-name=ingress-nginx-admission", "--namespace=$(POD_NAMESPACE)", "--patch-mutating=false", "--secret-name=ingress-nginx-admission", "--patch-failure-policy=Fail"}, WorkingDir:"", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar{core.EnvVar{Name:"POD_NAMESPACE", Value:"", ValueFrom:(*core.EnvVarSource)(0xc010a50c40)}}, Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*core.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:"OnFailure", TerminationGracePeriodSeconds:(*int64)(0xc00defa940), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"ingress-nginx-admission", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", SecurityContext:(*core.PodSecurityContext)(0xc00f71de00), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:"", Subdomain:"", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable
I0125 22:14:58.316998   10760 ssh_runner.go:152] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml
W0125 22:14:58.860008   10760 addons.go:369] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:
namespace/ingress-nginx unchanged
serviceaccount/ingress-nginx unchanged
configmap/ingress-nginx-controller unchanged
configmap/tcp-services unchanged
configmap/udp-services unchanged
clusterrole.rbac.authorization.k8s.io/ingress-nginx unchanged
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged
role.rbac.authorization.k8s.io/ingress-nginx unchanged
rolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged
service/ingress-nginx-controller-admission unchanged
service/ingress-nginx-controller unchanged
deployment.apps/ingress-nginx-controller configured
ingressclass.networking.k8s.io/nginx unchanged
validatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission configured
serviceaccount/ingress-nginx-admission unchanged
clusterrole.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
role.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
rolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged

stderr:
Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-create\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-create\"},\"spec\":{\"containers\":[{\"args\":[\"create\",\"--host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc\",\"--namespace=$(POD_NAMESPACE)\",\"--secret-name=ingress-nginx-admission\"],\"env\":[{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"create\"}],\"restartPolicy\":\"OnFailure\",\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":2000},\"serviceAccountName\":\"ingress-nginx-admission\"}}}}\n"},"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"template":{"metadata":{"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"$setElementOrder/containers":[{"name":"create"}],"containers":[{"image":"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660","name":"create","securityContext":null}],"nodeSelector":null,"securityContext":{"runAsNonRoot":true,"runAsUser":2000}}}}}
to:
Resource: "batch/v1, Resource=jobs", GroupVersionKind: "batch/v1, Kind=Job"
Name: "ingress-nginx-admission-create", Namespace: "ingress-nginx"
for: "/etc/kubernetes/addons/ingress-deploy.yaml": Job.batch "ingress-nginx-admission-create" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-create", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app.kubernetes.io/component":"admission-webhook", "app.kubernetes.io/instance":"ingress-nginx", "app.kubernetes.io/name":"ingress-nginx", "controller-uid":"8d577ee3-188a-46d7-a283-795857a59e12", "job-name":"ingress-nginx-admission-create"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:"create", Image:"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660", Command:[]string(nil), Args:[]string{"create", "--host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc", "--namespace=$(POD_NAMESPACE)", "--secret-name=ingress-nginx-admission"}, WorkingDir:"", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar{core.EnvVar{Name:"POD_NAMESPACE", Value:"", ValueFrom:(*core.EnvVarSource)(0xc00e638980)}}, Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*core.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:"OnFailure", TerminationGracePeriodSeconds:(*int64)(0xc00e1b1a40), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"ingress-nginx-admission", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", SecurityContext:(*core.PodSecurityContext)(0xc00efd6000), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:"", Subdomain:"", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable
Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-patch\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-patch\"},\"spec\":{\"containers\":[{\"args\":[\"patch\",\"--webhook-name=ingress-nginx-admission\",\"--namespace=$(POD_NAMESPACE)\",\"--patch-mutating=false\",\"--secret-name=ingress-nginx-admission\",\"--patch-failure-policy=Fail\"],\"env\":[{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"patch\"}],\"restartPolicy\":\"OnFailure\",\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":2000},\"serviceAccountName\":\"ingress-nginx-admission\"}}}}\n"},"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"template":{"metadata":{"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"$setElementOrder/containers":[{"name":"patch"}],"containers":[{"image":"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660","name":"patch","securityContext":null}],"nodeSelector":null,"securityContext":{"runAsNonRoot":true,"runAsUser":2000}}}}}
to:
Resource: "batch/v1, Resource=jobs", GroupVersionKind: "batch/v1, Kind=Job"
Name: "ingress-nginx-admission-patch", Namespace: "ingress-nginx"
for: "/etc/kubernetes/addons/ingress-deploy.yaml": Job.batch "ingress-nginx-admission-patch" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-patch", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app.kubernetes.io/component":"admission-webhook", "app.kubernetes.io/instance":"ingress-nginx", "app.kubernetes.io/name":"ingress-nginx", "controller-uid":"9961db6d-58a8-4d49-a390-8d0d0f4d7d10", "job-name":"ingress-nginx-admission-patch"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:"patch", Image:"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660", Command:[]string(nil), Args:[]string{"patch", "--webhook-name=ingress-nginx-admission", "--namespace=$(POD_NAMESPACE)", "--patch-mutating=false", "--secret-name=ingress-nginx-admission", "--patch-failure-policy=Fail"}, WorkingDir:"", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar{core.EnvVar{Name:"POD_NAMESPACE", Value:"", ValueFrom:(*core.EnvVarSource)(0xc00f04a740)}}, Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*core.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:"OnFailure", TerminationGracePeriodSeconds:(*int64)(0xc00e165090), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"ingress-nginx-admission", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", SecurityContext:(*core.PodSecurityContext)(0xc00efd7b00), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:"", Subdomain:"", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable
I0125 22:14:58.860008   10760 retry.go:31] will retry after 23.168280436s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:
namespace/ingress-nginx unchanged
serviceaccount/ingress-nginx unchanged
configmap/ingress-nginx-controller unchanged
configmap/tcp-services unchanged
configmap/udp-services unchanged
clusterrole.rbac.authorization.k8s.io/ingress-nginx unchanged
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged
role.rbac.authorization.k8s.io/ingress-nginx unchanged
rolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged
service/ingress-nginx-controller-admission unchanged
service/ingress-nginx-controller unchanged
deployment.apps/ingress-nginx-controller configured
ingressclass.networking.k8s.io/nginx unchanged
validatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission configured
serviceaccount/ingress-nginx-admission unchanged
clusterrole.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
role.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
rolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged

stderr:
Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-create\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-create\"},\"spec\":{\"containers\":[{\"args\":[\"create\",\"--host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc\",\"--namespace=$(POD_NAMESPACE)\",\"--secret-name=ingress-nginx-admission\"],\"env\":[{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"create\"}],\"restartPolicy\":\"OnFailure\",\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":2000},\"serviceAccountName\":\"ingress-nginx-admission\"}}}}\n"},"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"template":{"metadata":{"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"$setElementOrder/containers":[{"name":"create"}],"containers":[{"image":"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660","name":"create","securityContext":null}],"nodeSelector":null,"securityContext":{"runAsNonRoot":true,"runAsUser":2000}}}}}
to:
Resource: "batch/v1, Resource=jobs", GroupVersionKind: "batch/v1, Kind=Job"
Name: "ingress-nginx-admission-create", Namespace: "ingress-nginx"
for: "/etc/kubernetes/addons/ingress-deploy.yaml": Job.batch "ingress-nginx-admission-create" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-create", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app.kubernetes.io/component":"admission-webhook", "app.kubernetes.io/instance":"ingress-nginx", "app.kubernetes.io/name":"ingress-nginx", "controller-uid":"8d577ee3-188a-46d7-a283-795857a59e12", "job-name":"ingress-nginx-admission-create"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:"create", Image:"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660", Command:[]string(nil), Args:[]string{"create", "--host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc", "--namespace=$(POD_NAMESPACE)", "--secret-name=ingress-nginx-admission"}, WorkingDir:"", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar{core.EnvVar{Name:"POD_NAMESPACE", Value:"", ValueFrom:(*core.EnvVarSource)(0xc00e638980)}}, Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*core.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:"OnFailure", TerminationGracePeriodSeconds:(*int64)(0xc00e1b1a40), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"ingress-nginx-admission", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", SecurityContext:(*core.PodSecurityContext)(0xc00efd6000), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:"", Subdomain:"", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable
Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-patch\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-patch\"},\"spec\":{\"containers\":[{\"args\":[\"patch\",\"--webhook-name=ingress-nginx-admission\",\"--namespace=$(POD_NAMESPACE)\",\"--patch-mutating=false\",\"--secret-name=ingress-nginx-admission\",\"--patch-failure-policy=Fail\"],\"env\":[{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"patch\"}],\"restartPolicy\":\"OnFailure\",\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":2000},\"serviceAccountName\":\"ingress-nginx-admission\"}}}}\n"},"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"template":{"metadata":{"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"$setElementOrder/containers":[{"name":"patch"}],"containers":[{"image":"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660","name":"patch","securityContext":null}],"nodeSelector":null,"securityContext":{"runAsNonRoot":true,"runAsUser":2000}}}}}
to:
Resource: "batch/v1, Resource=jobs", GroupVersionKind: "batch/v1, Kind=Job"
Name: "ingress-nginx-admission-patch", Namespace: "ingress-nginx"
for: "/etc/kubernetes/addons/ingress-deploy.yaml": Job.batch "ingress-nginx-admission-patch" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-patch", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app.kubernetes.io/component":"admission-webhook", "app.kubernetes.io/instance":"ingress-nginx", "app.kubernetes.io/name":"ingress-nginx", "controller-uid":"9961db6d-58a8-4d49-a390-8d0d0f4d7d10", "job-name":"ingress-nginx-admission-patch"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:"patch", Image:"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660", Command:[]string(nil), Args:[]string{"patch", "--webhook-name=ingress-nginx-admission", "--namespace=$(POD_NAMESPACE)", "--patch-mutating=false", "--secret-name=ingress-nginx-admission", "--patch-failure-policy=Fail"}, WorkingDir:"", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar{core.EnvVar{Name:"POD_NAMESPACE", Value:"", ValueFrom:(*core.EnvVarSource)(0xc00f04a740)}}, Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*core.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:"OnFailure", TerminationGracePeriodSeconds:(*int64)(0xc00e165090), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"ingress-nginx-admission", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", SecurityContext:(*core.PodSecurityContext)(0xc00efd7b00), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:"", Subdomain:"", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable
I0125 22:15:22.049538   10760 ssh_runner.go:152] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml
W0125 22:15:22.646338   10760 addons.go:369] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:
namespace/ingress-nginx unchanged
serviceaccount/ingress-nginx unchanged
configmap/ingress-nginx-controller unchanged
configmap/tcp-services unchanged
configmap/udp-services unchanged
clusterrole.rbac.authorization.k8s.io/ingress-nginx unchanged
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged
role.rbac.authorization.k8s.io/ingress-nginx unchanged
rolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged
service/ingress-nginx-controller-admission unchanged
service/ingress-nginx-controller unchanged
deployment.apps/ingress-nginx-controller configured
ingressclass.networking.k8s.io/nginx unchanged
validatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission configured
serviceaccount/ingress-nginx-admission unchanged
clusterrole.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
role.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
rolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged

stderr:
Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-create\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-create\"},\"spec\":{\"containers\":[{\"args\":[\"create\",\"--host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc\",\"--namespace=$(POD_NAMESPACE)\",\"--secret-name=ingress-nginx-admission\"],\"env\":[{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"create\"}],\"restartPolicy\":\"OnFailure\",\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":2000},\"serviceAccountName\":\"ingress-nginx-admission\"}}}}\n"},"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"template":{"metadata":{"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"$setElementOrder/containers":[{"name":"create"}],"containers":[{"image":"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660","name":"create","securityContext":null}],"nodeSelector":null,"securityContext":{"runAsNonRoot":true,"runAsUser":2000}}}}}
to:
Resource: "batch/v1, Resource=jobs", GroupVersionKind: "batch/v1, Kind=Job"
Name: "ingress-nginx-admission-create", Namespace: "ingress-nginx"
for: "/etc/kubernetes/addons/ingress-deploy.yaml": Job.batch "ingress-nginx-admission-create" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-create", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app.kubernetes.io/component":"admission-webhook", "app.kubernetes.io/instance":"ingress-nginx", "app.kubernetes.io/name":"ingress-nginx", "controller-uid":"8d577ee3-188a-46d7-a283-795857a59e12", "job-name":"ingress-nginx-admission-create"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:"create", Image:"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660", Command:[]string(nil), Args:[]string{"create", "--host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc", "--namespace=$(POD_NAMESPACE)", "--secret-name=ingress-nginx-admission"}, WorkingDir:"", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar{core.EnvVar{Name:"POD_NAMESPACE", Value:"", ValueFrom:(*core.EnvVarSource)(0xc009f1a5e0)}}, Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*core.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:"OnFailure", TerminationGracePeriodSeconds:(*int64)(0xc00e59eab0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"ingress-nginx-admission", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", SecurityContext:(*core.PodSecurityContext)(0xc0107b3280), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:"", Subdomain:"", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable
Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-patch\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-patch\"},\"spec\":{\"containers\":[{\"args\":[\"patch\",\"--webhook-name=ingress-nginx-admission\",\"--namespace=$(POD_NAMESPACE)\",\"--patch-mutating=false\",\"--secret-name=ingress-nginx-admission\",\"--patch-failure-policy=Fail\"],\"env\":[{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"patch\"}],\"restartPolicy\":\"OnFailure\",\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":2000},\"serviceAccountName\":\"ingress-nginx-admission\"}}}}\n"},"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"template":{"metadata":{"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"$setElementOrder/containers":[{"name":"patch"}],"containers":[{"image":"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660","name":"patch","securityContext":null}],"nodeSelector":null,"securityContext":{"runAsNonRoot":true,"runAsUser":2000}}}}}
to:
Resource: "batch/v1, Resource=jobs", GroupVersionKind: "batch/v1, Kind=Job"
Name: "ingress-nginx-admission-patch", Namespace: "ingress-nginx"
for: "/etc/kubernetes/addons/ingress-deploy.yaml": Job.batch "ingress-nginx-admission-patch" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-patch", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app.kubernetes.io/component":"admission-webhook", "app.kubernetes.io/instance":"ingress-nginx", "app.kubernetes.io/name":"ingress-nginx", "controller-uid":"9961db6d-58a8-4d49-a390-8d0d0f4d7d10", "job-name":"ingress-nginx-admission-patch"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:"patch", Image:"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660", Command:[]string(nil), Args:[]string{"patch", "--webhook-name=ingress-nginx-admission", "--namespace=$(POD_NAMESPACE)", "--patch-mutating=false", "--secret-name=ingress-nginx-admission", "--patch-failure-policy=Fail"}, WorkingDir:"", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar{core.EnvVar{Name:"POD_NAMESPACE", Value:"", ValueFrom:(*core.EnvVarSource)(0xc010254900)}}, Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*core.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:"OnFailure", TerminationGracePeriodSeconds:(*int64)(0xc00dd3b180), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"ingress-nginx-admission", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", SecurityContext:(*core.PodSecurityContext)(0xc00f5f3180), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:"", Subdomain:"", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable
I0125 22:15:22.646338   10760 addons.go:386] Verifying addon ingress=true in "minikube"
I0125 22:15:22.650904   10760 out.go:176] * Verifying ingress addon...
I0125 22:31:53.513487   10760 kapi.go:75] Waiting for pod with label "app.kubernetes.io/name=ingress-nginx" in ns "ingress-nginx" ...
I0125 22:31:53.580057   10760 kapi.go:86] Found 4 Pods for label selector app.kubernetes.io/name=ingress-nginx
I0125 22:31:53.580057   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:31:54.102504   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:31:54.614178   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:31:55.107547   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:31:55.607380   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:31:56.093292   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:31:56.623690   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:31:57.098563   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:31:57.604242   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:31:58.092958   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:31:58.612169   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:31:59.098192   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:31:59.598014   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:00.120668   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:00.603478   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:01.099989   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:01.599929   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:02.105003   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:02.609746   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:03.102345   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:03.600416   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:04.091862   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:04.595471   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:05.091003   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:05.631856   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:06.102094   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:06.596393   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:07.099832   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:07.601181   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:08.101051   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:08.591845   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:09.097448   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:09.593656   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:10.095892   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:10.600944   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:11.095471   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:11.602943   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:12.099834   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:12.596183   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:13.113425   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:13.593437   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:14.107460   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:14.591332   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:15.098995   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:15.596611   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:16.100032   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:16.597295   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:17.102954   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:17.617149   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:18.093257   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:18.592704   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:19.096213   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:19.593951   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:20.117667   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:20.605642   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:21.097890   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:21.598680   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:22.119396   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:22.595755   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:23.095599   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:23.601866   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:24.095384   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:24.598305   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:25.092897   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:25.611213   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:26.095344   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:26.607137   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:27.095604   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:27.601327   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:28.096778   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:28.615976   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:29.110934   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:29.599462   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:30.093733   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:30.595010   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:31.093561   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:31.598885   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:32.115824   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:32.599458   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:33.099309   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:33.598710   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:34.094959   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:34.600657   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:35.095159   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:35.616536   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:36.098352   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:36.600368   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:37.100448   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:37.598639   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:38.092116   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:38.593610   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:39.117010   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:39.596606   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:40.098826   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:40.600194   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:41.103695   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:41.601320   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:42.090436   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:42.595651   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:43.100595   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:43.592997   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:44.121194   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:44.594953   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:45.095390   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:45.597537   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:46.091666   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:46.603441   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:47.092554   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:47.590770   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:48.090792   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:48.592357   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:49.095182   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:49.601762   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:50.108465   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:50.592762   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:51.095792   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:51.596092   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:52.090282   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:52.591677   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:53.113505   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:53.590521   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:54.107896   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:54.597031   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:55.092567   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:55.597837   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:56.094302   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:56.599437   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:57.095622   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:57.599162   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:58.092602   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:58.597558   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:59.092241   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:32:59.589625   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:00.093420   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:00.606235   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:01.092869   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:01.593363   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:02.093901   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:02.602374   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:03.097871   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:03.596162   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:04.102280   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:04.600556   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:05.090679   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:05.604902   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:06.095944   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:06.592738   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:07.106240   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:07.596914   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:08.098794   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:08.607558   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:09.092222   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:09.591497   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:10.100030   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:10.591511   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:11.098847   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:11.591406   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:12.098971   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:12.596317   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:13.090302   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:13.597191   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:14.090693   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:14.601035   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:15.097096   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:15.593862   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:16.093459   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:16.591239   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:17.098879   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:17.595516   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:18.090911   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:18.591572   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:19.090319   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:19.587864   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:20.103333   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:20.592472   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:21.089387   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:21.598843   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:22.088654   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:22.590850   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:23.097939   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:23.596431   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:24.097120   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:24.597545   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:25.097414   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:25.594830   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:26.095333   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:26.597575   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:27.093809   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:27.602514   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:28.089696   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:28.601926   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:29.095470   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:29.594140   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:30.096024   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:30.598234   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:31.089339   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:31.588844   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:32.099341   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:32.598969   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:33.090323   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:33.593408   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:34.095336   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:34.594189   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:35.092277   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:35.591588   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:36.097012   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:36.598053   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:37.090559   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:37.593421   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:38.095492   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:38.590446   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:39.087853   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:39.589388   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:40.097542   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:40.592602   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:41.091403   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:41.590329   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:42.096314   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:42.589932   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:43.089851   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:43.590239   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:44.089914   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:44.594743   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:45.094812   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:45.587899   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:46.091898   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:46.597718   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:47.097939   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:47.594275   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:48.088007   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:48.592036   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:49.092929   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:49.594234   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:50.088534   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:50.597503   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:51.088390   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:51.599292   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:52.101430   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:52.590185   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:53.095192   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:53.598892   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:54.086061   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:54.589647   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:55.097923   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:55.596759   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:56.092549   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:56.596443   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:57.090200   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:57.596942   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:58.094366   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:58.595855   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:59.087361   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:33:59.588214   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:00.092379   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:00.597312   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:01.092493   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:01.600847   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:02.094432   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:02.592783   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:03.093084   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:03.599255   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:04.094009   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:04.599375   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:05.098070   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:05.597543   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:06.098442   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:06.589964   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:07.091383   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:07.596375   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:08.103432   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:08.597353   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:09.088988   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:09.594135   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:10.094521   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:10.595878   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:11.089250   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:11.594094   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:12.090318   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:12.596475   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:13.093242   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:13.587662   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:14.093832   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:14.586613   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:15.089594   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:15.591123   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:16.089654   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:16.589177   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:17.091560   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:17.592679   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:18.088047   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:18.592764   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:19.088383   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:19.589168   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:20.091039   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:20.591539   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:21.087208   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:21.597476   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:22.087252   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:22.595659   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:23.088927   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:23.601838   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:24.098432   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:24.598526   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:25.088999   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:25.592246   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:26.092713   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:26.592460   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:27.089880   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:27.616724   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:28.102125   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:28.599458   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:29.104689   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:29.615887   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:30.108811   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:30.606441   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:31.104119   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:31.593971   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:32.093229   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:32.603792   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:33.101443   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:33.594049   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:34.103478   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:34.595433   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:35.095895   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:35.600657   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:36.106574   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:36.602024   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:37.106339   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:37.590878   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:38.104020   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:38.630027   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:39.091929   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:39.593714   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:40.101635   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:40.594486   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:41.093089   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:41.590558   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:42.092126   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:42.591325   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:43.093774   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:43.596364   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:44.100334   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:44.590768   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:45.087769   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:45.593416   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:46.090745   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:46.599880   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:47.092936   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:47.606070   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:48.104719   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:48.598356   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:49.096218   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:49.594847   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:50.099493   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:50.597327   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:51.090075   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:51.594725   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:52.095093   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:52.588175   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:53.095602   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:53.592589   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:54.098562   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:54.601634   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:55.089869   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:55.593437   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:56.097874   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:56.595319   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:57.088692   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:57.598187   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:58.098429   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:58.602309   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:59.088999   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:34:59.591855   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:00.095304   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:00.596676   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:01.094725   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:01.595245   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:02.087687   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:02.598633   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:03.091508   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:03.591062   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:04.092939   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:04.592351   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:05.093984   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:05.597973   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:06.090192   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:06.598182   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:07.093847   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:07.593253   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:08.109329   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:08.591913   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:09.091844   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:09.591527   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:10.090820   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:10.615618   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:11.128317   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:11.597978   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:12.089079   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:12.610430   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:13.100534   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:13.611870   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:14.097730   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:14.598376   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:15.100453   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:15.592994   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:16.100578   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:16.595881   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:17.087849   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:17.593854   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:18.090461   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:18.598693   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:19.106482   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:19.589321   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:20.103935   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:20.597990   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:21.099305   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:21.598543   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:22.096416   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:22.587555   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:23.093932   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:23.626656   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:24.100489   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:24.593410   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:25.096342   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:25.590064   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:26.092070   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:26.588290   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:27.089742   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:27.595975   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:28.089523   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:28.589419   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:29.092687   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:29.596389   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:30.092732   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:30.598314   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:31.088156   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:31.610002   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:32.091921   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:32.596188   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:33.093169   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:33.589495   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:34.098753   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:34.592237   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:35.091053   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:35.591125   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:36.091017   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:36.595063   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:37.091392   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:37.591827   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:38.096221   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:38.589000   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:39.095803   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:39.598530   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:40.091168   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:40.594854   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:41.088651   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:41.592919   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:42.097256   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:42.591303   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:43.092377   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:43.596793   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:44.095599   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:44.590885   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:45.089130   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:45.596441   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:46.090783   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:46.595604   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:47.094417   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:47.593919   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:48.093874   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:48.594498   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:49.096563   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:49.594322   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:50.096467   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:50.598182   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:51.097353   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:51.594658   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:52.096019   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:52.596259   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:53.100142   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:53.594759   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:54.096050   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:54.592938   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:55.099930   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:55.591423   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:56.093746   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:56.591817   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:57.090212   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:57.592664   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:58.090253   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:58.589648   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:59.098227   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:35:59.591444   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:00.092822   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:00.594274   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:01.089706   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:01.590261   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:02.094755   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:02.589758   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:03.091949   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:03.596171   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:04.097151   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:04.587497   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:05.094504   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:05.594311   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:06.095090   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:06.602274   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:07.092112   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:07.589698   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:08.089367   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:08.595651   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:09.091551   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:09.594137   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:10.094815   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:10.598367   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:11.097902   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:11.596182   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:12.091533   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:12.596196   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:13.096499   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:13.589449   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:14.091965   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:14.587499   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:15.093170   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:15.590358   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:16.094869   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:16.590085   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:17.093674   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:17.589354   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:18.096918   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:18.594139   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:19.094733   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:19.589522   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:20.091518   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:20.597441   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:21.092283   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:21.593032   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:22.091501   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:22.591084   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:23.092729   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:23.590926   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:24.089155   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:24.592096   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:25.093729   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:25.589683   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:26.092081   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:26.597264   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:27.095584   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:27.590389   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:28.091958   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:28.591709   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:29.091881   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:29.595769   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:30.091605   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:30.597879   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:31.093135   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:31.595827   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:32.087070   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:32.588414   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:33.090545   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:33.591497   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:34.088411   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:34.588046   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:35.096721   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:35.601250   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:36.088617   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:36.598305   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:37.095932   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:37.602806   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:38.096891   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:38.596499   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:39.088820   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:39.593028   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:40.089042   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:40.596011   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:41.098988   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:41.594594   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:42.086997   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:42.592091   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:43.090868   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:43.594568   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:44.091330   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:44.593434   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:45.089241   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:45.594056   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:46.103642   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:46.595075   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:47.096475   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:47.596700   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:48.089210   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:48.588333   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:49.090629   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:49.591690   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:50.095793   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:50.589470   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:51.093078   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:51.600610   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:52.092560   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:52.589619   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:53.097637   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:53.589575   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:54.088986   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:54.586741   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:55.093306   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:55.591061   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:56.096634   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:56.595277   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:57.099254   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:57.600661   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:58.099189   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:58.594130   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:59.088593   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:36:59.588290   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:00.090578   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:00.608335   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:01.094581   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:01.588602   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:02.087149   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:02.587830   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:03.088283   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:03.590488   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:04.094366   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:04.593183   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:05.089419   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:05.600575   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:06.093257   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:06.595453   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:07.097149   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:07.599001   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:08.113167   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:08.600622   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:09.091840   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:09.600033   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:10.089400   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:10.595902   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:11.089435   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:11.595484   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:12.109001   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:12.595809   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:13.093531   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:13.597140   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:14.116570   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:14.593084   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:15.091884   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:15.592659   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:16.091956   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:16.592578   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:17.087900   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:17.595273   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:18.091896   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:18.601364   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:19.100605   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:19.595596   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:20.095292   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:20.588420   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:21.096089   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:21.593663   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:22.099468   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:22.588985   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:23.095763   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:23.593056   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:24.101834   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:24.600226   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:25.095559   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:25.590165   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:26.089329   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:26.590147   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:27.092925   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:27.601629   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:28.092523   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:28.586856   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:29.091929   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:29.597152   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:30.101598   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:30.596402   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:31.097674   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:31.595846   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:32.098434   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:32.598412   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:33.090309   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:33.597942   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:34.088026   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:34.595305   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:35.097291   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:35.594286   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:36.094750   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:36.595935   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:37.100141   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:37.595201   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:38.092997   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:38.586790   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:39.094109   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:39.591103   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:40.097358   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:40.588220   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:41.102050   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:41.588389   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:42.094865   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:42.596256   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:43.092335   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:43.599393   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:44.092363   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:44.599557   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:45.100498   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:45.591892   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:46.092648   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:46.586123   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:47.101122   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:47.600306   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:48.096886   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:48.599024   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:49.089407   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:49.590242   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:50.092038   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:50.594698   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:51.089938   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:51.592184   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:52.097157   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:52.596074   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:53.096666   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:53.597839   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:53.604945   10760 kapi.go:96] waiting for pod "app.kubernetes.io/name=ingress-nginx", current state: Pending: [<nil>]
I0125 22:37:53.604945   10760 kapi.go:108] duration metric: took 6m0.0914583s to wait for app.kubernetes.io/name=ingress-nginx ...
W0125 22:37:53.626072   10760 out.go:241] ! Enabling 'ingress' returned an error: running callbacks: [sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml: Process exited with status 1
stdout:
namespace/ingress-nginx unchanged
serviceaccount/ingress-nginx unchanged
configmap/ingress-nginx-controller unchanged
configmap/tcp-services unchanged
configmap/udp-services unchanged
clusterrole.rbac.authorization.k8s.io/ingress-nginx unchanged
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged
role.rbac.authorization.k8s.io/ingress-nginx unchanged
rolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged
service/ingress-nginx-controller-admission unchanged
service/ingress-nginx-controller unchanged
deployment.apps/ingress-nginx-controller configured
ingressclass.networking.k8s.io/nginx unchanged
validatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission configured
serviceaccount/ingress-nginx-admission unchanged
clusterrole.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
role.rbac.authorization.k8s.io/ingress-nginx-admission unchanged
rolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged

stderr:
Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-create\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-create\"},\"spec\":{\"containers\":[{\"args\":[\"create\",\"--host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc\",\"--namespace=$(POD_NAMESPACE)\",\"--secret-name=ingress-nginx-admission\"],\"env\":[{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"create\"}],\"restartPolicy\":\"OnFailure\",\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":2000},\"serviceAccountName\":\"ingress-nginx-admission\"}}}}\n"},"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"template":{"metadata":{"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"$setElementOrder/containers":[{"name":"create"}],"containers":[{"image":"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660","name":"create","securityContext":null}],"nodeSelector":null,"securityContext":{"runAsNonRoot":true,"runAsUser":2000}}}}}
to:
Resource: "batch/v1, Resource=jobs", GroupVersionKind: "batch/v1, Kind=Job"
Name: "ingress-nginx-admission-create", Namespace: "ingress-nginx"
for: "/etc/kubernetes/addons/ingress-deploy.yaml": Job.batch "ingress-nginx-admission-create" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-create", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app.kubernetes.io/component":"admission-webhook", "app.kubernetes.io/instance":"ingress-nginx", "app.kubernetes.io/name":"ingress-nginx", "controller-uid":"8d577ee3-188a-46d7-a283-795857a59e12", "job-name":"ingress-nginx-admission-create"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:"create", Image:"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660", Command:[]string(nil), Args:[]string{"create", "--host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc", "--namespace=$(POD_NAMESPACE)", "--secret-name=ingress-nginx-admission"}, WorkingDir:"", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar{core.EnvVar{Name:"POD_NAMESPACE", Value:"", ValueFrom:(*core.EnvVarSource)(0xc009f1a5e0)}}, Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*core.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:"OnFailure", TerminationGracePeriodSeconds:(*int64)(0xc00e59eab0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"ingress-nginx-admission", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", SecurityContext:(*core.PodSecurityContext)(0xc0107b3280), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:"", Subdomain:"", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable
Error from server (Invalid): error when applying patch:
{"metadata":{"annotations":{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"batch/v1\",\"kind\":\"Job\",\"metadata\":{\"annotations\":{},\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-patch\",\"namespace\":\"ingress-nginx\"},\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"app.kubernetes.io/component\":\"admission-webhook\",\"app.kubernetes.io/instance\":\"ingress-nginx\",\"app.kubernetes.io/name\":\"ingress-nginx\"},\"name\":\"ingress-nginx-admission-patch\"},\"spec\":{\"containers\":[{\"args\":[\"patch\",\"--webhook-name=ingress-nginx-admission\",\"--namespace=$(POD_NAMESPACE)\",\"--patch-mutating=false\",\"--secret-name=ingress-nginx-admission\",\"--patch-failure-policy=Fail\"],\"env\":[{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"patch\"}],\"restartPolicy\":\"OnFailure\",\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":2000},\"serviceAccountName\":\"ingress-nginx-admission\"}}}}\n"},"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"template":{"metadata":{"labels":{"app.kubernetes.io/part-of":null,"app.kubernetes.io/version":null}},"spec":{"$setElementOrder/containers":[{"name":"patch"}],"containers":[{"image":"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660","name":"patch","securityContext":null}],"nodeSelector":null,"securityContext":{"runAsNonRoot":true,"runAsUser":2000}}}}}
to:
Resource: "batch/v1, Resource=jobs", GroupVersionKind: "batch/v1, Kind=Job"
Name: "ingress-nginx-admission-patch", Namespace: "ingress-nginx"
for: "/etc/kubernetes/addons/ingress-deploy.yaml": Job.batch "ingress-nginx-admission-patch" is invalid: spec.template: Invalid value: core.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"ingress-nginx-admission-patch", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app.kubernetes.io/component":"admission-webhook", "app.kubernetes.io/instance":"ingress-nginx", "app.kubernetes.io/name":"ingress-nginx", "controller-uid":"9961db6d-58a8-4d49-a390-8d0d0f4d7d10", "job-name":"ingress-nginx-admission-patch"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:core.PodSpec{Volumes:[]core.Volume(nil), InitContainers:[]core.Container(nil), Containers:[]core.Container{core.Container{Name:"patch", Image:"k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660", Command:[]string(nil), Args:[]string{"patch", "--webhook-name=ingress-nginx-admission", "--namespace=$(POD_NAMESPACE)", "--patch-mutating=false", "--secret-name=ingress-nginx-admission", "--patch-failure-policy=Fail"}, WorkingDir:"", Ports:[]core.ContainerPort(nil), EnvFrom:[]core.EnvFromSource(nil), Env:[]core.EnvVar{core.EnvVar{Name:"POD_NAMESPACE", Value:"", ValueFrom:(*core.EnvVarSource)(0xc010254900)}}, Resources:core.ResourceRequirements{Limits:core.ResourceList(nil), Requests:core.ResourceList(nil)}, VolumeMounts:[]core.VolumeMount(nil), VolumeDevices:[]core.VolumeDevice(nil), LivenessProbe:(*core.Probe)(nil), ReadinessProbe:(*core.Probe)(nil), StartupProbe:(*core.Probe)(nil), Lifecycle:(*core.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*core.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]core.EphemeralContainer(nil), RestartPolicy:"OnFailure", TerminationGracePeriodSeconds:(*int64)(0xc00dd3b180), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"ingress-nginx-admission", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", SecurityContext:(*core.PodSecurityContext)(0xc00f5f3180), ImagePullSecrets:[]core.LocalObjectReference(nil), Hostname:"", Subdomain:"", SetHostnameAsFQDN:(*bool)(nil), Affinity:(*core.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]core.Toleration(nil), HostAliases:[]core.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), PreemptionPolicy:(*core.PreemptionPolicy)(nil), DNSConfig:(*core.PodDNSConfig)(nil), ReadinessGates:[]core.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), Overhead:core.ResourceList(nil), EnableServiceLinks:(*bool)(nil), TopologySpreadConstraints:[]core.TopologySpreadConstraint(nil)}}: field is immutable
 waiting for app.kubernetes.io/name=ingress-nginx pods: timed out waiting for the condition]
I0125 22:37:53.634155   10760 out.go:176] * Enabled addons: storage-provisioner
I0125 22:37:53.638310   10760 addons.go:417] enableAddons completed in 24m20.7977612s
I0125 22:37:53.708673   10760 config.go:176] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.22.3
I0125 22:37:53.725184   10760 config.go:176] Loaded profile config "stancluster01": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.22.3
I0125 22:37:53.725693   10760 profile.go:147] Saving config to C:\Users\bm10'\.minikube\profiles\minikube\config.json ...
I0125 22:37:53.729556   10760 out.go:176] * Starting worker node minikube-m02 in cluster minikube
I0125 22:37:53.730721   10760 cache.go:118] Beginning downloading kic base image for docker with docker
I0125 22:37:53.732460   10760 out.go:176] * Pulling base image ...
I0125 22:37:53.737813   10760 image.go:75] Checking for gcr.io/k8s-minikube/kicbase:v0.0.28@sha256:4780f1897569d2bf77aafb3d133a08d42b4fe61127f06fcfc90c2c5d902d893c in local docker daemon
I0125 22:37:53.737813   10760 preload.go:132] Checking if preload exists for k8s version v1.22.3 and runtime docker
I0125 22:37:53.738365   10760 cache.go:57] Caching tarball of preloaded images
I0125 22:37:53.747563   10760 preload.go:174] Found C:\Users\bm10'\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v13-v1.22.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0125 22:37:53.747563   10760 cache.go:60] Finished verifying existence of preloaded tar for  v1.22.3 on docker
I0125 22:37:53.748074   10760 profile.go:147] Saving config to C:\Users\bm10'\.minikube\profiles\minikube\config.json ...
I0125 22:37:54.099247   10760 image.go:79] Found gcr.io/k8s-minikube/kicbase:v0.0.28@sha256:4780f1897569d2bf77aafb3d133a08d42b4fe61127f06fcfc90c2c5d902d893c in local docker daemon, skipping pull
I0125 22:37:54.099247   10760 cache.go:140] gcr.io/k8s-minikube/kicbase:v0.0.28@sha256:4780f1897569d2bf77aafb3d133a08d42b4fe61127f06fcfc90c2c5d902d893c exists in daemon, skipping load
I0125 22:37:54.099756   10760 cache.go:206] Successfully downloaded all kic artifacts
I0125 22:37:54.102500   10760 start.go:313] acquiring machines lock for minikube-m02: {Name:mk1c1ed23f34c149d3d7a32a39e7b3be0bb4ce7c Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0125 22:37:54.103915   10760 start.go:317] acquired machines lock for "minikube-m02" in 544.8µs
I0125 22:37:54.105026   10760 start.go:93] Skipping create...Using existing machine configuration
I0125 22:37:54.105026   10760 fix.go:55] fixHost starting: m02
I0125 22:37:54.149875   10760 cli_runner.go:115] Run: docker container inspect minikube-m02 --format={{.State.Status}}
I0125 22:37:54.369131   10760 fix.go:108] recreateIfNeeded on minikube-m02: state=Stopped err=<nil>
W0125 22:37:54.370392   10760 fix.go:134] unexpected machine state, will restart: <nil>
I0125 22:37:54.371992   10760 out.go:176] * Restarting existing docker container for "minikube-m02" ...
I0125 22:37:54.377954   10760 cli_runner.go:115] Run: docker start minikube-m02
I0125 22:37:58.085556   10760 cli_runner.go:168] Completed: docker start minikube-m02: (3.7075019s)
I0125 22:37:58.092748   10760 cli_runner.go:115] Run: docker container inspect minikube-m02 --format={{.State.Status}}
I0125 22:37:58.233292   10760 kic.go:420] container "minikube-m02" state is running.
I0125 22:37:58.241391   10760 cli_runner.go:115] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m02
I0125 22:37:58.414901   10760 profile.go:147] Saving config to C:\Users\bm10'\.minikube\profiles\minikube\config.json ...
I0125 22:37:58.416610   10760 machine.go:88] provisioning docker machine ...
I0125 22:37:58.418341   10760 ubuntu.go:169] provisioning hostname "minikube-m02"
I0125 22:37:58.424461   10760 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0125 22:37:58.571218   10760 main.go:130] libmachine: Using SSH client type: native
I0125 22:37:58.597001   10760 main.go:130] libmachine: &{{{<nil> 0 [] [] []} docker [0xa29f80] 0xa2ce40 <nil>  [] 0s} 127.0.0.1 61751 <nil> <nil>}
I0125 22:37:58.597546   10760 main.go:130] libmachine: About to run SSH command:
sudo hostname minikube-m02 && echo "minikube-m02" | sudo tee /etc/hostname
I0125 22:37:58.615940   10760 main.go:130] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I0125 22:38:02.010308   10760 main.go:130] libmachine: SSH cmd err, output: <nil>: minikube-m02

I0125 22:38:02.014303   10760 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0125 22:38:02.155956   10760 main.go:130] libmachine: Using SSH client type: native
I0125 22:38:02.157636   10760 main.go:130] libmachine: &{{{<nil> 0 [] [] []} docker [0xa29f80] 0xa2ce40 <nil>  [] 0s} 127.0.0.1 61751 <nil> <nil>}
I0125 22:38:02.157636   10760 main.go:130] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube-m02' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube-m02/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube-m02' | sudo tee -a /etc/hosts; 
			fi
		fi
I0125 22:38:02.356128   10760 main.go:130] libmachine: SSH cmd err, output: <nil>: 
I0125 22:38:02.357332   10760 ubuntu.go:175] set auth options {CertDir:C:\Users\bm10'\.minikube CaCertPath:C:\Users\bm10'\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\bm10'\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\bm10'\.minikube\machines\server.pem ServerKeyPath:C:\Users\bm10'\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\bm10'\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\bm10'\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\bm10'\.minikube}
I0125 22:38:02.358435   10760 ubuntu.go:177] setting up certificates
I0125 22:38:02.359450   10760 provision.go:83] configureAuth start
I0125 22:38:02.364175   10760 cli_runner.go:115] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m02
I0125 22:38:02.553361   10760 provision.go:138] copyHostCerts
I0125 22:38:02.556423   10760 exec_runner.go:144] found C:\Users\bm10'\.minikube/ca.pem, removing ...
I0125 22:38:02.556935   10760 exec_runner.go:207] rm: C:\Users\bm10'\.minikube\ca.pem
I0125 22:38:02.557032   10760 exec_runner.go:151] cp: C:\Users\bm10'\.minikube\certs\ca.pem --> C:\Users\bm10'\.minikube/ca.pem (1074 bytes)
I0125 22:38:02.559274   10760 exec_runner.go:144] found C:\Users\bm10'\.minikube/cert.pem, removing ...
I0125 22:38:02.559274   10760 exec_runner.go:207] rm: C:\Users\bm10'\.minikube\cert.pem
I0125 22:38:02.559274   10760 exec_runner.go:151] cp: C:\Users\bm10'\.minikube\certs\cert.pem --> C:\Users\bm10'\.minikube/cert.pem (1119 bytes)
I0125 22:38:02.561320   10760 exec_runner.go:144] found C:\Users\bm10'\.minikube/key.pem, removing ...
I0125 22:38:02.561320   10760 exec_runner.go:207] rm: C:\Users\bm10'\.minikube\key.pem
I0125 22:38:02.561320   10760 exec_runner.go:151] cp: C:\Users\bm10'\.minikube\certs\key.pem --> C:\Users\bm10'\.minikube/key.pem (1675 bytes)
I0125 22:38:02.563071   10760 provision.go:112] generating server cert: C:\Users\bm10'\.minikube\machines\server.pem ca-key=C:\Users\bm10'\.minikube\certs\ca.pem private-key=C:\Users\bm10'\.minikube\certs\ca-key.pem org=bm10'.minikube-m02 san=[192.168.49.3 127.0.0.1 localhost 127.0.0.1 minikube minikube-m02]
I0125 22:38:02.714642   10760 provision.go:172] copyRemoteCerts
I0125 22:38:02.725524   10760 ssh_runner.go:152] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0125 22:38:02.734735   10760 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0125 22:38:02.924258   10760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61751 SSHKeyPath:C:\Users\bm10'\.minikube\machines\minikube-m02\id_rsa Username:docker}
I0125 22:38:03.086574   10760 ssh_runner.go:319] scp C:\Users\bm10'\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1074 bytes)
I0125 22:38:03.198026   10760 ssh_runner.go:319] scp C:\Users\bm10'\.minikube\machines\server.pem --> /etc/docker/server.pem (1208 bytes)
I0125 22:38:03.294915   10760 ssh_runner.go:319] scp C:\Users\bm10'\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0125 22:38:03.383826   10760 provision.go:86] duration metric: configureAuth took 1.0243759s
I0125 22:38:03.384362   10760 ubuntu.go:193] setting minikube options for container-runtime
I0125 22:38:03.384908   10760 config.go:176] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.22.3
I0125 22:38:03.388329   10760 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0125 22:38:03.530424   10760 main.go:130] libmachine: Using SSH client type: native
I0125 22:38:03.530424   10760 main.go:130] libmachine: &{{{<nil> 0 [] [] []} docker [0xa29f80] 0xa2ce40 <nil>  [] 0s} 127.0.0.1 61751 <nil> <nil>}
I0125 22:38:03.531030   10760 main.go:130] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0125 22:38:03.679270   10760 main.go:130] libmachine: SSH cmd err, output: <nil>: overlay

I0125 22:38:03.679270   10760 ubuntu.go:71] root file system type: overlay
I0125 22:38:03.680894   10760 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0125 22:38:03.685270   10760 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0125 22:38:03.834201   10760 main.go:130] libmachine: Using SSH client type: native
I0125 22:38:03.835766   10760 main.go:130] libmachine: &{{{<nil> 0 [] [] []} docker [0xa29f80] 0xa2ce40 <nil>  [] 0s} 127.0.0.1 61751 <nil> <nil>}
I0125 22:38:03.835766   10760 main.go:130] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure

Environment="NO_PROXY=192.168.49.2"


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0125 22:38:04.041297   10760 main.go:130] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure

Environment=NO_PROXY=192.168.49.2


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0125 22:38:04.045248   10760 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0125 22:38:04.192709   10760 main.go:130] libmachine: Using SSH client type: native
I0125 22:38:04.192827   10760 main.go:130] libmachine: &{{{<nil> 0 [] [] []} docker [0xa29f80] 0xa2ce40 <nil>  [] 0s} 127.0.0.1 61751 <nil> <nil>}
I0125 22:38:04.192827   10760 main.go:130] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0125 22:38:04.357948   10760 main.go:130] libmachine: SSH cmd err, output: <nil>: 
I0125 22:38:04.357948   10760 machine.go:91] provisioned docker machine in 5.9407983s
I0125 22:38:04.371669   10760 start.go:267] post-start starting for "minikube-m02" (driver="docker")
I0125 22:38:04.371669   10760 start.go:277] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0125 22:38:04.379566   10760 ssh_runner.go:152] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0125 22:38:04.382883   10760 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0125 22:38:04.526449   10760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61751 SSHKeyPath:C:\Users\bm10'\.minikube\machines\minikube-m02\id_rsa Username:docker}
I0125 22:38:04.649171   10760 ssh_runner.go:152] Run: cat /etc/os-release
I0125 22:38:04.656338   10760 main.go:130] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0125 22:38:04.656338   10760 main.go:130] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0125 22:38:04.656338   10760 main.go:130] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0125 22:38:04.656957   10760 info.go:137] Remote host: Ubuntu 20.04.2 LTS
I0125 22:38:04.657536   10760 filesync.go:126] Scanning C:\Users\bm10'\.minikube\addons for local assets ...
I0125 22:38:04.657536   10760 filesync.go:126] Scanning C:\Users\bm10'\.minikube\files for local assets ...
I0125 22:38:04.657536   10760 start.go:270] post-start completed in 285.8667ms
I0125 22:38:04.664310   10760 ssh_runner.go:152] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0125 22:38:04.667527   10760 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0125 22:38:04.800407   10760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61751 SSHKeyPath:C:\Users\bm10'\.minikube\machines\minikube-m02\id_rsa Username:docker}
I0125 22:38:04.905427   10760 fix.go:57] fixHost completed within 10.8004006s
I0125 22:38:04.905427   10760 start.go:80] releasing machines lock for "minikube-m02", held for 10.8015121s
I0125 22:38:04.910933   10760 cli_runner.go:115] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m02
I0125 22:38:05.047055   10760 out.go:176] * Found network options:
I0125 22:38:05.048944   10760 out.go:176]   - NO_PROXY=192.168.49.2
W0125 22:38:05.051130   10760 proxy.go:118] fail to check proxy env: Error ip not in block
I0125 22:38:05.052789   10760 out.go:176]   - no_proxy=192.168.49.2
W0125 22:38:05.052789   10760 proxy.go:118] fail to check proxy env: Error ip not in block
W0125 22:38:05.052789   10760 proxy.go:118] fail to check proxy env: Error ip not in block
I0125 22:38:05.053390   10760 ssh_runner.go:152] Run: curl -sS -m 2 https://k8s.gcr.io/
I0125 22:38:05.059993   10760 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0125 22:38:05.062818   10760 ssh_runner.go:152] Run: sudo systemctl is-active --quiet service containerd
I0125 22:38:05.067762   10760 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0125 22:38:05.199423   10760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61751 SSHKeyPath:C:\Users\bm10'\.minikube\machines\minikube-m02\id_rsa Username:docker}
I0125 22:38:05.212290   10760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61751 SSHKeyPath:C:\Users\bm10'\.minikube\machines\minikube-m02\id_rsa Username:docker}
I0125 22:38:07.410260   10760 ssh_runner.go:192] Completed: sudo systemctl is-active --quiet service containerd: (2.3474423s)
I0125 22:38:07.410458   10760 ssh_runner.go:192] Completed: curl -sS -m 2 https://k8s.gcr.io/: (2.3570677s)
W0125 22:38:07.411002   10760 start.go:664] [curl -sS -m 2 https://k8s.gcr.io/] failed: curl -sS -m 2 https://k8s.gcr.io/: Process exited with status 28
stdout:

stderr:
curl: (28) Operation timed out after 2000 milliseconds with 0 out of 0 bytes received
W0125 22:38:07.411528   10760 out.go:241] ! This container is having trouble accessing https://k8s.gcr.io
W0125 22:38:07.412076   10760 out.go:241] * To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I0125 22:38:07.417358   10760 ssh_runner.go:152] Run: sudo systemctl cat docker.service
I0125 22:38:07.442852   10760 cruntime.go:255] skipping containerd shutdown because we are bound to it
I0125 22:38:07.449592   10760 ssh_runner.go:152] Run: sudo systemctl is-active --quiet service crio
I0125 22:38:07.478206   10760 ssh_runner.go:152] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/dockershim.sock
image-endpoint: unix:///var/run/dockershim.sock
" | sudo tee /etc/crictl.yaml"
I0125 22:38:07.527283   10760 ssh_runner.go:152] Run: sudo systemctl unmask docker.service
I0125 22:38:07.711239   10760 ssh_runner.go:152] Run: sudo systemctl enable docker.socket
I0125 22:38:07.806805   10760 ssh_runner.go:152] Run: sudo systemctl cat docker.service
I0125 22:38:07.833456   10760 ssh_runner.go:152] Run: sudo systemctl daemon-reload
I0125 22:38:07.987914   10760 ssh_runner.go:152] Run: sudo systemctl start docker
I0125 22:38:08.014687   10760 ssh_runner.go:152] Run: docker version --format {{.Server.Version}}
I0125 22:38:08.918321   10760 ssh_runner.go:152] Run: docker version --format {{.Server.Version}}
I0125 22:38:09.036320   10760 out.go:203] * Preparing Kubernetes v1.22.3 on Docker 20.10.8 ...
I0125 22:38:09.039149   10760 out.go:176]   - env NO_PROXY=192.168.49.2
I0125 22:38:09.046288   10760 cli_runner.go:115] Run: docker exec -t minikube-m02 dig +short host.docker.internal
I0125 22:38:09.394192   10760 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0125 22:38:09.400912   10760 ssh_runner.go:152] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0125 22:38:09.406777   10760 ssh_runner.go:152] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0125 22:38:09.431535   10760 certs.go:54] Setting up C:\Users\bm10'\.minikube\profiles\minikube for IP: 192.168.49.3
I0125 22:38:09.432621   10760 certs.go:182] skipping minikubeCA CA generation: C:\Users\bm10'\.minikube\ca.key
I0125 22:38:09.433154   10760 certs.go:182] skipping proxyClientCA CA generation: C:\Users\bm10'\.minikube\proxy-client-ca.key
I0125 22:38:09.433760   10760 certs.go:388] found cert: C:\Users\bm10'\.minikube\certs\C:\Users\bm10'\.minikube\certs\ca-key.pem (1679 bytes)
I0125 22:38:09.433760   10760 certs.go:388] found cert: C:\Users\bm10'\.minikube\certs\C:\Users\bm10'\.minikube\certs\ca.pem (1074 bytes)
I0125 22:38:09.434285   10760 certs.go:388] found cert: C:\Users\bm10'\.minikube\certs\C:\Users\bm10'\.minikube\certs\cert.pem (1119 bytes)
I0125 22:38:09.434285   10760 certs.go:388] found cert: C:\Users\bm10'\.minikube\certs\C:\Users\bm10'\.minikube\certs\key.pem (1675 bytes)
I0125 22:38:09.439209   10760 ssh_runner.go:319] scp C:\Users\bm10'\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0125 22:38:09.484193   10760 ssh_runner.go:319] scp C:\Users\bm10'\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0125 22:38:09.535299   10760 ssh_runner.go:319] scp C:\Users\bm10'\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0125 22:38:09.584138   10760 ssh_runner.go:319] scp C:\Users\bm10'\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0125 22:38:09.630009   10760 ssh_runner.go:319] scp C:\Users\bm10'\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0125 22:38:09.687460   10760 ssh_runner.go:152] Run: openssl version
I0125 22:38:09.708009   10760 ssh_runner.go:152] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0125 22:38:09.737984   10760 ssh_runner.go:152] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0125 22:38:09.748433   10760 certs.go:431] hashing: -rw-r--r-- 1 root root 1111 Dec 21 17:29 /usr/share/ca-certificates/minikubeCA.pem
I0125 22:38:09.753817   10760 ssh_runner.go:152] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0125 22:38:09.770994   10760 ssh_runner.go:152] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0125 22:38:09.795598   10760 ssh_runner.go:152] Run: docker info --format {{.CgroupDriver}}
I0125 22:38:11.283820   10760 ssh_runner.go:192] Completed: docker info --format {{.CgroupDriver}}: (1.4882227s)
I0125 22:38:11.285506   10760 cni.go:93] Creating CNI manager for ""
I0125 22:38:11.286078   10760 cni.go:154] 3 nodes found, recommending kindnet
I0125 22:38:11.286603   10760 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0125 22:38:11.286603   10760 kubeadm.go:153] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.3 APIServerPort:8443 KubernetesVersion:v1.22.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube-m02 DNSDomain:cluster.local CRISocket:/var/run/dockershim.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NoTaintMaster:true NodeIP:192.168.49.3 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[]}
I0125 22:38:11.288258   10760 kubeadm.go:157] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta2
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.3
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: "minikube-m02"
  kubeletExtraArgs:
    node-ip: 192.168.49.3
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.22.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0125 22:38:11.289330   10760 kubeadm.go:909] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.22.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --cni-conf-dir=/etc/cni/net.mk --config=/var/lib/kubelet/config.yaml --container-runtime=docker --hostname-override=minikube-m02 --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.3

[Install]
 config:
{KubernetesVersion:v1.22.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: ExtraOptions:[{Component:kubelet Key:cni-conf-dir Value:/etc/cni/net.mk}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0125 22:38:11.295116   10760 ssh_runner.go:152] Run: sudo ls /var/lib/minikube/binaries/v1.22.3
I0125 22:38:11.316274   10760 binaries.go:44] Found k8s binaries, skipping transfer
I0125 22:38:11.321410   10760 ssh_runner.go:152] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system
I0125 22:38:11.337787   10760 ssh_runner.go:319] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I0125 22:38:11.371591   10760 ssh_runner.go:319] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0125 22:38:11.413206   10760 ssh_runner.go:152] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0125 22:38:11.419296   10760 ssh_runner.go:152] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0125 22:38:11.442820   10760 config.go:176] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.22.3
I0125 22:38:11.443341   10760 host.go:66] Checking if "minikube" exists ...
I0125 22:38:11.444460   10760 start.go:244] JoinCluster: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.28@sha256:4780f1897569d2bf77aafb3d133a08d42b4fe61127f06fcfc90c2c5d902d893c Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.22.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: ExtraOptions:[{Component:kubelet Key:cni-conf-dir Value:/etc/cni/net.mk}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.22.3 ControlPlane:true Worker:true} {Name:m02 IP:192.168.49.3 Port:0 KubernetesVersion:v1.22.3 ControlPlane:false Worker:true} {Name:m03 IP:192.168.49.4 Port:0 KubernetesVersion:v1.22.3 ControlPlane:false Worker:true}] Addons:map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false helm-tiller:false ingress:true ingress-dns:true istio:false istio-provisioner:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[IngressController:ingress-nginx/controller:v1.0.4@sha256:545cff00370f28363dad31e3b59a94ba377854d3a11f18988f5f9e56841ef9ef IngressDNS:k8s-minikube/minikube-ingress-dns:0.0.2@sha256:4abe27f9fc03fedab1d655e2020e6b165faf3bf6de1088ce6cf215a75b78f05f KubeWebhookCertgenCreate:k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660 KubeWebhookCertgenPatch:k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\bm10':/minikube-host}
I0125 22:38:11.445066   10760 ssh_runner.go:152] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.22.3:$PATH" kubeadm token create --print-join-command --ttl=0"
I0125 22:38:11.449485   10760 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0125 22:38:11.621466   10760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52288 SSHKeyPath:C:\Users\bm10'\.minikube\machines\minikube\id_rsa Username:docker}
I0125 22:38:12.409743   10760 start.go:257] removing existing worker node "m02" before attempting to rejoin cluster: &{Name:m02 IP:192.168.49.3 Port:0 KubernetesVersion:v1.22.3 ControlPlane:false Worker:true}
I0125 22:38:12.410341   10760 host.go:66] Checking if "minikube" exists ...
I0125 22:38:12.417633   10760 ssh_runner.go:152] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl drain minikube-m02 --force --grace-period=1 --skip-wait-for-delete-timeout=1 --disable-eviction --ignore-daemonsets --delete-emptydir-data --delete-local-data
I0125 22:38:12.422313   10760 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0125 22:38:12.599839   10760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52288 SSHKeyPath:C:\Users\bm10'\.minikube\machines\minikube\id_rsa Username:docker}
I0125 22:38:13.588153   10760 ssh_runner.go:192] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl drain minikube-m02 --force --grace-period=1 --skip-wait-for-delete-timeout=1 --disable-eviction --ignore-daemonsets --delete-emptydir-data --delete-local-data: (1.1705197s)
I0125 22:38:13.588153   10760 node.go:109] successfully drained node "m02"
I0125 22:38:13.614097   10760 node.go:125] successfully deleted node "m02"
I0125 22:38:13.614097   10760 start.go:261] successfully removed existing worker node "m02" from cluster: &{Name:m02 IP:192.168.49.3 Port:0 KubernetesVersion:v1.22.3 ControlPlane:false Worker:true}
I0125 22:38:13.614607   10760 start.go:265] trying to join worker node "m02" to cluster: &{Name:m02 IP:192.168.49.3 Port:0 KubernetesVersion:v1.22.3 ControlPlane:false Worker:true}
I0125 22:38:13.614607   10760 ssh_runner.go:152] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.22.3:$PATH" kubeadm join control-plane.minikube.internal:8443 --token sjsb9o.notix276kbmyqvis --discovery-token-ca-cert-hash sha256:f12ef4c14017fbf957bbc12398b5455d46d61ce61d1b9bfc467884045bd43df3 --ignore-preflight-errors=all --cri-socket /var/run/dockershim.sock --node-name=minikube-m02"
I0125 22:38:16.840415   10760 ssh_runner.go:192] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.22.3:$PATH" kubeadm join control-plane.minikube.internal:8443 --token sjsb9o.notix276kbmyqvis --discovery-token-ca-cert-hash sha256:f12ef4c14017fbf957bbc12398b5455d46d61ce61d1b9bfc467884045bd43df3 --ignore-preflight-errors=all --cri-socket /var/run/dockershim.sock --node-name=minikube-m02": (3.2252961s)
I0125 22:38:16.841026   10760 ssh_runner.go:152] Run: /bin/bash -c "sudo systemctl daemon-reload && sudo systemctl enable kubelet && sudo systemctl start kubelet"
I0125 22:38:17.152599   10760 start.go:246] JoinCluster complete in 5.7081388s
I0125 22:38:17.152599   10760 cni.go:93] Creating CNI manager for ""
I0125 22:38:17.152599   10760 cni.go:154] 3 nodes found, recommending kindnet
I0125 22:38:17.161501   10760 ssh_runner.go:152] Run: stat /opt/cni/bin/portmap
I0125 22:38:17.171004   10760 cni.go:187] applying CNI manifest using /var/lib/minikube/binaries/v1.22.3/kubectl ...
I0125 22:38:17.171004   10760 ssh_runner.go:319] scp memory --> /var/tmp/minikube/cni.yaml (2429 bytes)
I0125 22:38:17.209367   10760 ssh_runner.go:152] Run: sudo /var/lib/minikube/binaries/v1.22.3/kubectl apply --kubeconfig=/var/lib/minikube/kubeconfig -f /var/tmp/minikube/cni.yaml
I0125 22:38:17.535400   10760 start.go:229] Will wait 6m0s for node &{Name:m02 IP:192.168.49.3 Port:0 KubernetesVersion:v1.22.3 ControlPlane:false Worker:true}
I0125 22:38:17.537029   10760 out.go:176] * Verifying Kubernetes components...
I0125 22:38:17.544903   10760 ssh_runner.go:152] Run: sudo systemctl is-active --quiet service kubelet
I0125 22:38:17.570787   10760 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0125 22:38:17.778867   10760 kubeadm.go:547] duration metric: took 242.9388ms to wait for : map[apiserver:true system_pods:true] ...
I0125 22:38:17.779440   10760 node_conditions.go:102] verifying NodePressure condition ...
I0125 22:38:17.787426   10760 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0125 22:38:17.787962   10760 node_conditions.go:123] node cpu capacity is 12
I0125 22:38:17.789036   10760 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0125 22:38:17.789036   10760 node_conditions.go:123] node cpu capacity is 12
I0125 22:38:17.789036   10760 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0125 22:38:17.789036   10760 node_conditions.go:123] node cpu capacity is 12
I0125 22:38:17.789036   10760 node_conditions.go:105] duration metric: took 9.5961ms to run NodePressure ...
I0125 22:38:17.789036   10760 start.go:234] waiting for startup goroutines ...
I0125 22:38:17.791305   10760 config.go:176] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.22.3
I0125 22:38:17.791825   10760 config.go:176] Loaded profile config "stancluster01": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.22.3
I0125 22:38:17.791825   10760 profile.go:147] Saving config to C:\Users\bm10'\.minikube\profiles\minikube\config.json ...
I0125 22:38:17.795108   10760 out.go:176] * Starting worker node minikube-m03 in cluster minikube
I0125 22:38:17.796178   10760 cache.go:118] Beginning downloading kic base image for docker with docker
I0125 22:38:17.797336   10760 out.go:176] * Pulling base image ...
I0125 22:38:17.798796   10760 preload.go:132] Checking if preload exists for k8s version v1.22.3 and runtime docker
I0125 22:38:17.798796   10760 image.go:75] Checking for gcr.io/k8s-minikube/kicbase:v0.0.28@sha256:4780f1897569d2bf77aafb3d133a08d42b4fe61127f06fcfc90c2c5d902d893c in local docker daemon
I0125 22:38:17.798796   10760 cache.go:57] Caching tarball of preloaded images
I0125 22:38:17.799333   10760 preload.go:174] Found C:\Users\bm10'\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v13-v1.22.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0125 22:38:17.799333   10760 cache.go:60] Finished verifying existence of preloaded tar for  v1.22.3 on docker
I0125 22:38:17.799942   10760 profile.go:147] Saving config to C:\Users\bm10'\.minikube\profiles\minikube\config.json ...
I0125 22:38:17.985263   10760 image.go:79] Found gcr.io/k8s-minikube/kicbase:v0.0.28@sha256:4780f1897569d2bf77aafb3d133a08d42b4fe61127f06fcfc90c2c5d902d893c in local docker daemon, skipping pull
I0125 22:38:17.985263   10760 cache.go:140] gcr.io/k8s-minikube/kicbase:v0.0.28@sha256:4780f1897569d2bf77aafb3d133a08d42b4fe61127f06fcfc90c2c5d902d893c exists in daemon, skipping load
I0125 22:38:17.985263   10760 cache.go:206] Successfully downloaded all kic artifacts
I0125 22:38:17.985263   10760 start.go:313] acquiring machines lock for minikube-m03: {Name:mk2f262c11ef53d083ed323129f6467cda5add18 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0125 22:38:17.985263   10760 start.go:317] acquired machines lock for "minikube-m03" in 0s
I0125 22:38:17.986269   10760 start.go:93] Skipping create...Using existing machine configuration
I0125 22:38:17.986269   10760 fix.go:55] fixHost starting: m03
I0125 22:38:18.004793   10760 cli_runner.go:115] Run: docker container inspect minikube-m03 --format={{.State.Status}}
I0125 22:38:18.155021   10760 fix.go:108] recreateIfNeeded on minikube-m03: state=Stopped err=<nil>
W0125 22:38:18.155021   10760 fix.go:134] unexpected machine state, will restart: <nil>
I0125 22:38:18.157955   10760 out.go:176] * Restarting existing docker container for "minikube-m03" ...
I0125 22:38:18.164384   10760 cli_runner.go:115] Run: docker start minikube-m03
I0125 22:38:19.115987   10760 cli_runner.go:115] Run: docker container inspect minikube-m03 --format={{.State.Status}}
I0125 22:38:19.284295   10760 kic.go:420] container "minikube-m03" state is running.
I0125 22:38:19.292601   10760 cli_runner.go:115] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m03
I0125 22:38:19.438428   10760 profile.go:147] Saving config to C:\Users\bm10'\.minikube\profiles\minikube\config.json ...
I0125 22:38:19.441581   10760 machine.go:88] provisioning docker machine ...
I0125 22:38:19.441581   10760 ubuntu.go:169] provisioning hostname "minikube-m03"
I0125 22:38:19.448006   10760 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0125 22:38:19.705005   10760 main.go:130] libmachine: Using SSH client type: native
I0125 22:38:19.729049   10760 main.go:130] libmachine: &{{{<nil> 0 [] [] []} docker [0xa29f80] 0xa2ce40 <nil>  [] 0s} 127.0.0.1 61775 <nil> <nil>}
I0125 22:38:19.729049   10760 main.go:130] libmachine: About to run SSH command:
sudo hostname minikube-m03 && echo "minikube-m03" | sudo tee /etc/hostname
I0125 22:38:19.734741   10760 main.go:130] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I0125 22:38:22.938680   10760 main.go:130] libmachine: SSH cmd err, output: <nil>: minikube-m03

I0125 22:38:22.942547   10760 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0125 22:38:23.094669   10760 main.go:130] libmachine: Using SSH client type: native
I0125 22:38:23.094669   10760 main.go:130] libmachine: &{{{<nil> 0 [] [] []} docker [0xa29f80] 0xa2ce40 <nil>  [] 0s} 127.0.0.1 61775 <nil> <nil>}
I0125 22:38:23.094669   10760 main.go:130] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube-m03' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube-m03/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube-m03' | sudo tee -a /etc/hosts; 
			fi
		fi
I0125 22:38:23.254445   10760 main.go:130] libmachine: SSH cmd err, output: <nil>: 
I0125 22:38:23.254445   10760 ubuntu.go:175] set auth options {CertDir:C:\Users\bm10'\.minikube CaCertPath:C:\Users\bm10'\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\bm10'\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\bm10'\.minikube\machines\server.pem ServerKeyPath:C:\Users\bm10'\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\bm10'\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\bm10'\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\bm10'\.minikube}
I0125 22:38:23.254445   10760 ubuntu.go:177] setting up certificates
I0125 22:38:23.254993   10760 provision.go:83] configureAuth start
I0125 22:38:23.260840   10760 cli_runner.go:115] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m03
I0125 22:38:23.475357   10760 provision.go:138] copyHostCerts
I0125 22:38:23.475850   10760 exec_runner.go:144] found C:\Users\bm10'\.minikube/ca.pem, removing ...
I0125 22:38:23.475850   10760 exec_runner.go:207] rm: C:\Users\bm10'\.minikube\ca.pem
I0125 22:38:23.476261   10760 exec_runner.go:151] cp: C:\Users\bm10'\.minikube\certs\ca.pem --> C:\Users\bm10'\.minikube/ca.pem (1074 bytes)
I0125 22:38:23.478368   10760 exec_runner.go:144] found C:\Users\bm10'\.minikube/cert.pem, removing ...
I0125 22:38:23.478368   10760 exec_runner.go:207] rm: C:\Users\bm10'\.minikube\cert.pem
I0125 22:38:23.479024   10760 exec_runner.go:151] cp: C:\Users\bm10'\.minikube\certs\cert.pem --> C:\Users\bm10'\.minikube/cert.pem (1119 bytes)
I0125 22:38:23.480335   10760 exec_runner.go:144] found C:\Users\bm10'\.minikube/key.pem, removing ...
I0125 22:38:23.480335   10760 exec_runner.go:207] rm: C:\Users\bm10'\.minikube\key.pem
I0125 22:38:23.482765   10760 exec_runner.go:151] cp: C:\Users\bm10'\.minikube\certs\key.pem --> C:\Users\bm10'\.minikube/key.pem (1675 bytes)
I0125 22:38:23.483349   10760 provision.go:112] generating server cert: C:\Users\bm10'\.minikube\machines\server.pem ca-key=C:\Users\bm10'\.minikube\certs\ca.pem private-key=C:\Users\bm10'\.minikube\certs\ca-key.pem org=bm10'.minikube-m03 san=[192.168.49.4 127.0.0.1 localhost 127.0.0.1 minikube minikube-m03]
I0125 22:38:23.651432   10760 provision.go:172] copyRemoteCerts
I0125 22:38:23.658894   10760 ssh_runner.go:152] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0125 22:38:23.662890   10760 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0125 22:38:23.817057   10760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61775 SSHKeyPath:C:\Users\bm10'\.minikube\machines\minikube-m03\id_rsa Username:docker}
I0125 22:38:23.924917   10760 ssh_runner.go:319] scp C:\Users\bm10'\.minikube\machines\server.pem --> /etc/docker/server.pem (1208 bytes)
I0125 22:38:23.975637   10760 ssh_runner.go:319] scp C:\Users\bm10'\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0125 22:38:24.028234   10760 ssh_runner.go:319] scp C:\Users\bm10'\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1074 bytes)
I0125 22:38:24.073392   10760 provision.go:86] duration metric: configureAuth took 818.3994ms
I0125 22:38:24.073392   10760 ubuntu.go:193] setting minikube options for container-runtime
I0125 22:38:24.073956   10760 config.go:176] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.22.3
I0125 22:38:24.078067   10760 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0125 22:38:24.246334   10760 main.go:130] libmachine: Using SSH client type: native
I0125 22:38:24.246781   10760 main.go:130] libmachine: &{{{<nil> 0 [] [] []} docker [0xa29f80] 0xa2ce40 <nil>  [] 0s} 127.0.0.1 61775 <nil> <nil>}
I0125 22:38:24.246781   10760 main.go:130] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0125 22:38:24.397322   10760 main.go:130] libmachine: SSH cmd err, output: <nil>: overlay

I0125 22:38:24.397322   10760 ubuntu.go:71] root file system type: overlay
I0125 22:38:24.397860   10760 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0125 22:38:24.401808   10760 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0125 22:38:24.537492   10760 main.go:130] libmachine: Using SSH client type: native
I0125 22:38:24.538043   10760 main.go:130] libmachine: &{{{<nil> 0 [] [] []} docker [0xa29f80] 0xa2ce40 <nil>  [] 0s} 127.0.0.1 61775 <nil> <nil>}
I0125 22:38:24.538043   10760 main.go:130] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure

Environment="NO_PROXY=192.168.49.2"
Environment="NO_PROXY=192.168.49.2,192.168.49.3"


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0125 22:38:24.698385   10760 main.go:130] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure

Environment=NO_PROXY=192.168.49.2
Environment=NO_PROXY=192.168.49.2,192.168.49.3


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0125 22:38:24.701714   10760 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0125 22:38:24.850419   10760 main.go:130] libmachine: Using SSH client type: native
I0125 22:38:24.850419   10760 main.go:130] libmachine: &{{{<nil> 0 [] [] []} docker [0xa29f80] 0xa2ce40 <nil>  [] 0s} 127.0.0.1 61775 <nil> <nil>}
I0125 22:38:24.850419   10760 main.go:130] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0125 22:38:24.987052   10760 main.go:130] libmachine: SSH cmd err, output: <nil>: 
I0125 22:38:24.987112   10760 machine.go:91] provisioned docker machine in 5.5455309s
I0125 22:38:24.987112   10760 start.go:267] post-start starting for "minikube-m03" (driver="docker")
I0125 22:38:24.987112   10760 start.go:277] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0125 22:38:24.992567   10760 ssh_runner.go:152] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0125 22:38:24.996675   10760 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0125 22:38:25.135706   10760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61775 SSHKeyPath:C:\Users\bm10'\.minikube\machines\minikube-m03\id_rsa Username:docker}
I0125 22:38:25.253160   10760 ssh_runner.go:152] Run: cat /etc/os-release
I0125 22:38:25.260198   10760 main.go:130] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0125 22:38:25.260198   10760 main.go:130] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0125 22:38:25.260198   10760 main.go:130] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0125 22:38:25.260198   10760 info.go:137] Remote host: Ubuntu 20.04.2 LTS
I0125 22:38:25.260198   10760 filesync.go:126] Scanning C:\Users\bm10'\.minikube\addons for local assets ...
I0125 22:38:25.260781   10760 filesync.go:126] Scanning C:\Users\bm10'\.minikube\files for local assets ...
I0125 22:38:25.260781   10760 start.go:270] post-start completed in 273.6694ms
I0125 22:38:25.266810   10760 ssh_runner.go:152] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0125 22:38:25.270557   10760 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0125 22:38:25.408021   10760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61775 SSHKeyPath:C:\Users\bm10'\.minikube\machines\minikube-m03\id_rsa Username:docker}
I0125 22:38:25.524504   10760 fix.go:57] fixHost completed within 7.5382355s
I0125 22:38:25.524504   10760 start.go:80] releasing machines lock for "minikube-m03", held for 7.539241s
I0125 22:38:25.528467   10760 cli_runner.go:115] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m03
I0125 22:38:25.670532   10760 out.go:176] * Found network options:
I0125 22:38:25.672424   10760 out.go:176]   - NO_PROXY=192.168.49.2,192.168.49.3
W0125 22:38:25.675293   10760 proxy.go:118] fail to check proxy env: Error ip not in block
W0125 22:38:25.675293   10760 proxy.go:118] fail to check proxy env: Error ip not in block
I0125 22:38:25.677153   10760 out.go:176]   - no_proxy=192.168.49.2,192.168.49.3
W0125 22:38:25.677710   10760 proxy.go:118] fail to check proxy env: Error ip not in block
W0125 22:38:25.677710   10760 proxy.go:118] fail to check proxy env: Error ip not in block
W0125 22:38:25.677710   10760 proxy.go:118] fail to check proxy env: Error ip not in block
W0125 22:38:25.677710   10760 proxy.go:118] fail to check proxy env: Error ip not in block
I0125 22:38:25.678788   10760 ssh_runner.go:152] Run: curl -sS -m 2 https://k8s.gcr.io/
I0125 22:38:25.683148   10760 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0125 22:38:25.685402   10760 ssh_runner.go:152] Run: sudo systemctl is-active --quiet service containerd
I0125 22:38:25.689191   10760 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0125 22:38:25.831102   10760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61775 SSHKeyPath:C:\Users\bm10'\.minikube\machines\minikube-m03\id_rsa Username:docker}
I0125 22:38:25.844103   10760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61775 SSHKeyPath:C:\Users\bm10'\.minikube\machines\minikube-m03\id_rsa Username:docker}
I0125 22:38:25.948394   10760 ssh_runner.go:152] Run: sudo systemctl cat docker.service
I0125 22:38:28.025424   10760 ssh_runner.go:192] Completed: sudo systemctl cat docker.service: (2.0769671s)
I0125 22:38:28.025424   10760 ssh_runner.go:192] Completed: curl -sS -m 2 https://k8s.gcr.io/: (2.3466365s)
I0125 22:38:28.025424   10760 cruntime.go:255] skipping containerd shutdown because we are bound to it
W0125 22:38:28.025424   10760 start.go:664] [curl -sS -m 2 https://k8s.gcr.io/] failed: curl -sS -m 2 https://k8s.gcr.io/: Process exited with status 28
stdout:

stderr:
curl: (28) Operation timed out after 2001 milliseconds with 0 out of 0 bytes received
W0125 22:38:28.025424   10760 out.go:241] ! This container is having trouble accessing https://k8s.gcr.io
W0125 22:38:28.027544   10760 out.go:241] * To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I0125 22:38:28.036676   10760 ssh_runner.go:152] Run: sudo systemctl is-active --quiet service crio
I0125 22:38:28.080638   10760 ssh_runner.go:152] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/dockershim.sock
image-endpoint: unix:///var/run/dockershim.sock
" | sudo tee /etc/crictl.yaml"
I0125 22:38:28.162886   10760 ssh_runner.go:152] Run: sudo systemctl unmask docker.service
I0125 22:38:28.417097   10760 ssh_runner.go:152] Run: sudo systemctl enable docker.socket
I0125 22:38:28.579215   10760 ssh_runner.go:152] Run: sudo systemctl cat docker.service
I0125 22:38:28.607434   10760 ssh_runner.go:152] Run: sudo systemctl daemon-reload
I0125 22:38:28.732420   10760 ssh_runner.go:152] Run: sudo systemctl start docker
I0125 22:38:28.757282   10760 ssh_runner.go:152] Run: docker version --format {{.Server.Version}}
I0125 22:38:28.922970   10760 ssh_runner.go:152] Run: docker version --format {{.Server.Version}}
I0125 22:38:29.015155   10760 out.go:203] * Preparing Kubernetes v1.22.3 on Docker 20.10.8 ...
I0125 22:38:29.016772   10760 out.go:176]   - env NO_PROXY=192.168.49.2
I0125 22:38:29.019651   10760 out.go:176]   - env NO_PROXY=192.168.49.2,192.168.49.3
I0125 22:38:29.023524   10760 cli_runner.go:115] Run: docker exec -t minikube-m03 dig +short host.docker.internal
I0125 22:38:29.301003   10760 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0125 22:38:29.307595   10760 ssh_runner.go:152] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0125 22:38:29.313564   10760 ssh_runner.go:152] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0125 22:38:29.333836   10760 certs.go:54] Setting up C:\Users\bm10'\.minikube\profiles\minikube for IP: 192.168.49.4
I0125 22:38:29.334358   10760 certs.go:182] skipping minikubeCA CA generation: C:\Users\bm10'\.minikube\ca.key
I0125 22:38:29.334358   10760 certs.go:182] skipping proxyClientCA CA generation: C:\Users\bm10'\.minikube\proxy-client-ca.key
I0125 22:38:29.334901   10760 certs.go:388] found cert: C:\Users\bm10'\.minikube\certs\C:\Users\bm10'\.minikube\certs\ca-key.pem (1679 bytes)
I0125 22:38:29.334901   10760 certs.go:388] found cert: C:\Users\bm10'\.minikube\certs\C:\Users\bm10'\.minikube\certs\ca.pem (1074 bytes)
I0125 22:38:29.334901   10760 certs.go:388] found cert: C:\Users\bm10'\.minikube\certs\C:\Users\bm10'\.minikube\certs\cert.pem (1119 bytes)
I0125 22:38:29.334901   10760 certs.go:388] found cert: C:\Users\bm10'\.minikube\certs\C:\Users\bm10'\.minikube\certs\key.pem (1675 bytes)
I0125 22:38:29.340598   10760 ssh_runner.go:319] scp C:\Users\bm10'\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0125 22:38:29.386329   10760 ssh_runner.go:319] scp C:\Users\bm10'\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0125 22:38:29.432836   10760 ssh_runner.go:319] scp C:\Users\bm10'\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0125 22:38:29.478233   10760 ssh_runner.go:319] scp C:\Users\bm10'\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0125 22:38:29.520442   10760 ssh_runner.go:319] scp C:\Users\bm10'\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0125 22:38:29.572330   10760 ssh_runner.go:152] Run: openssl version
I0125 22:38:29.587074   10760 ssh_runner.go:152] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0125 22:38:29.612213   10760 ssh_runner.go:152] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0125 22:38:29.618794   10760 certs.go:431] hashing: -rw-r--r-- 1 root root 1111 Dec 21 17:29 /usr/share/ca-certificates/minikubeCA.pem
I0125 22:38:29.624293   10760 ssh_runner.go:152] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0125 22:38:29.637143   10760 ssh_runner.go:152] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0125 22:38:29.657470   10760 ssh_runner.go:152] Run: docker info --format {{.CgroupDriver}}
I0125 22:38:29.885303   10760 cni.go:93] Creating CNI manager for ""
I0125 22:38:29.885303   10760 cni.go:154] 3 nodes found, recommending kindnet
I0125 22:38:29.885303   10760 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0125 22:38:29.885706   10760 kubeadm.go:153] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.4 APIServerPort:8443 KubernetesVersion:v1.22.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube-m03 DNSDomain:cluster.local CRISocket:/var/run/dockershim.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NoTaintMaster:true NodeIP:192.168.49.4 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[]}
I0125 22:38:29.885706   10760 kubeadm.go:157] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta2
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.4
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: "minikube-m03"
  kubeletExtraArgs:
    node-ip: 192.168.49.4
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.22.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0125 22:38:29.885706   10760 kubeadm.go:909] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.22.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --cni-conf-dir=/etc/cni/net.mk --config=/var/lib/kubelet/config.yaml --container-runtime=docker --hostname-override=minikube-m03 --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.4

[Install]
 config:
{KubernetesVersion:v1.22.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: ExtraOptions:[{Component:kubelet Key:cni-conf-dir Value:/etc/cni/net.mk}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0125 22:38:29.891749   10760 ssh_runner.go:152] Run: sudo ls /var/lib/minikube/binaries/v1.22.3
I0125 22:38:29.909605   10760 binaries.go:44] Found k8s binaries, skipping transfer
I0125 22:38:29.915521   10760 ssh_runner.go:152] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system
I0125 22:38:29.931513   10760 ssh_runner.go:319] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I0125 22:38:29.964839   10760 ssh_runner.go:319] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0125 22:38:30.004399   10760 ssh_runner.go:152] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0125 22:38:30.010738   10760 ssh_runner.go:152] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0125 22:38:30.031420   10760 host.go:66] Checking if "minikube" exists ...
I0125 22:38:30.032541   10760 start.go:244] JoinCluster: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.28@sha256:4780f1897569d2bf77aafb3d133a08d42b4fe61127f06fcfc90c2c5d902d893c Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.22.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: ExtraOptions:[{Component:kubelet Key:cni-conf-dir Value:/etc/cni/net.mk}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.22.3 ControlPlane:true Worker:true} {Name:m02 IP:192.168.49.3 Port:0 KubernetesVersion:v1.22.3 ControlPlane:false Worker:true} {Name:m03 IP:192.168.49.4 Port:0 KubernetesVersion:v1.22.3 ControlPlane:false Worker:true}] Addons:map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false helm-tiller:false ingress:true ingress-dns:true istio:false istio-provisioner:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[IngressController:ingress-nginx/controller:v1.0.4@sha256:545cff00370f28363dad31e3b59a94ba377854d3a11f18988f5f9e56841ef9ef IngressDNS:k8s-minikube/minikube-ingress-dns:0.0.2@sha256:4abe27f9fc03fedab1d655e2020e6b165faf3bf6de1088ce6cf215a75b78f05f KubeWebhookCertgenCreate:k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660 KubeWebhookCertgenPatch:k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\bm10':/minikube-host}
I0125 22:38:30.032596   10760 ssh_runner.go:152] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.22.3:$PATH" kubeadm token create --print-join-command --ttl=0"
I0125 22:38:30.032596   10760 config.go:176] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.22.3
I0125 22:38:30.035367   10760 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0125 22:38:30.166049   10760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52288 SSHKeyPath:C:\Users\bm10'\.minikube\machines\minikube\id_rsa Username:docker}
I0125 22:38:30.376096   10760 start.go:257] removing existing worker node "m03" before attempting to rejoin cluster: &{Name:m03 IP:192.168.49.4 Port:0 KubernetesVersion:v1.22.3 ControlPlane:false Worker:true}
I0125 22:38:30.376657   10760 host.go:66] Checking if "minikube" exists ...
I0125 22:38:30.382292   10760 ssh_runner.go:152] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl drain minikube-m03 --force --grace-period=1 --skip-wait-for-delete-timeout=1 --disable-eviction --ignore-daemonsets --delete-emptydir-data --delete-local-data
I0125 22:38:30.385714   10760 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0125 22:38:30.523547   10760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52288 SSHKeyPath:C:\Users\bm10'\.minikube\machines\minikube\id_rsa Username:docker}
I0125 22:38:30.765674   10760 node.go:109] successfully drained node "m03"
I0125 22:38:30.780103   10760 node.go:125] successfully deleted node "m03"
I0125 22:38:30.780103   10760 start.go:261] successfully removed existing worker node "m03" from cluster: &{Name:m03 IP:192.168.49.4 Port:0 KubernetesVersion:v1.22.3 ControlPlane:false Worker:true}
I0125 22:38:30.780103   10760 start.go:265] trying to join worker node "m03" to cluster: &{Name:m03 IP:192.168.49.4 Port:0 KubernetesVersion:v1.22.3 ControlPlane:false Worker:true}
I0125 22:38:30.780103   10760 ssh_runner.go:152] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.22.3:$PATH" kubeadm join control-plane.minikube.internal:8443 --token l83j13.dkjky8uthat1v09u --discovery-token-ca-cert-hash sha256:f12ef4c14017fbf957bbc12398b5455d46d61ce61d1b9bfc467884045bd43df3 --ignore-preflight-errors=all --cri-socket /var/run/dockershim.sock --node-name=minikube-m03"
I0125 22:38:33.268706   10760 ssh_runner.go:192] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.22.3:$PATH" kubeadm join control-plane.minikube.internal:8443 --token l83j13.dkjky8uthat1v09u --discovery-token-ca-cert-hash sha256:f12ef4c14017fbf957bbc12398b5455d46d61ce61d1b9bfc467884045bd43df3 --ignore-preflight-errors=all --cri-socket /var/run/dockershim.sock --node-name=minikube-m03": (2.4886034s)
I0125 22:38:33.268706   10760 ssh_runner.go:152] Run: /bin/bash -c "sudo systemctl daemon-reload && sudo systemctl enable kubelet && sudo systemctl start kubelet"
I0125 22:38:33.509520   10760 start.go:246] JoinCluster complete in 3.477525s
I0125 22:38:33.509520   10760 cni.go:93] Creating CNI manager for ""
I0125 22:38:33.509520   10760 cni.go:154] 3 nodes found, recommending kindnet
I0125 22:38:33.516044   10760 ssh_runner.go:152] Run: stat /opt/cni/bin/portmap
I0125 22:38:33.521906   10760 cni.go:187] applying CNI manifest using /var/lib/minikube/binaries/v1.22.3/kubectl ...
I0125 22:38:33.521906   10760 ssh_runner.go:319] scp memory --> /var/tmp/minikube/cni.yaml (2429 bytes)
I0125 22:38:33.556867   10760 ssh_runner.go:152] Run: sudo /var/lib/minikube/binaries/v1.22.3/kubectl apply --kubeconfig=/var/lib/minikube/kubeconfig -f /var/tmp/minikube/cni.yaml
I0125 22:38:33.897330   10760 start.go:229] Will wait 6m0s for node &{Name:m03 IP:192.168.49.4 Port:0 KubernetesVersion:v1.22.3 ControlPlane:false Worker:true}
I0125 22:38:33.898436   10760 out.go:176] * Verifying Kubernetes components...
I0125 22:38:33.907370   10760 ssh_runner.go:152] Run: sudo systemctl is-active --quiet service kubelet
I0125 22:38:33.931811   10760 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0125 22:38:34.092583   10760 kubeadm.go:547] duration metric: took 195.2531ms to wait for : map[apiserver:true system_pods:true] ...
I0125 22:38:34.092583   10760 node_conditions.go:102] verifying NodePressure condition ...
I0125 22:38:34.100758   10760 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0125 22:38:34.100758   10760 node_conditions.go:123] node cpu capacity is 12
I0125 22:38:34.100758   10760 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0125 22:38:34.100758   10760 node_conditions.go:123] node cpu capacity is 12
I0125 22:38:34.100758   10760 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0125 22:38:34.100758   10760 node_conditions.go:123] node cpu capacity is 12
I0125 22:38:34.100758   10760 node_conditions.go:105] duration metric: took 8.1742ms to run NodePressure ...
I0125 22:38:34.100758   10760 start.go:234] waiting for startup goroutines ...
I0125 22:38:34.871862   10760 start.go:473] kubectl: 1.28.2, cluster: 1.22.3 (minor skew: 6)
I0125 22:38:34.873486   10760 out.go:176] 
W0125 22:38:34.874047   10760 out.go:241] ! C:\Program Files\Docker\Docker\resources\bin\kubectl.exe is version 1.28.2, which may have incompatibilites with Kubernetes 1.22.3.
I0125 22:38:34.878469   10760 out.go:176]   - Want kubectl v1.22.3? Try 'minikube kubectl -- get pods -A'
I0125 22:38:34.880848   10760 out.go:176] * Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* -- Logs begin at Thu 2024-01-25 19:12:42 UTC, end at Thu 2024-01-25 19:51:25 UTC. --
Jan 25 19:12:45 minikube dockerd[222]: time="2024-01-25T19:12:45.948331417Z" level=warning msg="Your kernel does not support cgroup blkio throttle.read_iops_device"
Jan 25 19:12:45 minikube dockerd[222]: time="2024-01-25T19:12:45.948337779Z" level=warning msg="Your kernel does not support cgroup blkio throttle.write_iops_device"
Jan 25 19:12:45 minikube dockerd[222]: time="2024-01-25T19:12:45.958001852Z" level=info msg="Loading containers: start."
Jan 25 19:12:47 minikube dockerd[222]: time="2024-01-25T19:12:47.442091497Z" level=info msg="Removing stale sandbox 8eb5bf3ee24f8eb955c076a1a1a8a52bfce376f5e5aca65ac682d4a0f99b4e9d (aa35c4afd9eacc474c370a85e1d30248d5b202e1555b4d7b62f3d442d6aa457f)"
Jan 25 19:12:47 minikube dockerd[222]: time="2024-01-25T19:12:47.452367809Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint 1c97a5e6ee9913f1c7f2e964e8761d9bd407a43827631c453492cf3fbd5beb1c 13017988bfe256d05886632e93a6a98edc452435c9af0a3b74d0586f54caa89e], retrying...."
Jan 25 19:12:47 minikube dockerd[222]: time="2024-01-25T19:12:47.634524699Z" level=info msg="Removing stale sandbox 9e5d4156eef7b84b6ffeb40b992e8962be99f3325a6158446659b52742544397 (8e351047d026a09b4d84237f2794f879d27242be44ab5a7a46d7ff07266de4d2)"
Jan 25 19:12:47 minikube dockerd[222]: time="2024-01-25T19:12:47.640523402Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint 1c97a5e6ee9913f1c7f2e964e8761d9bd407a43827631c453492cf3fbd5beb1c 27b880358916e66761fb72ed36105e5a23889c377254d3a5c7d2b2d12ad36e58], retrying...."
Jan 25 19:12:47 minikube dockerd[222]: time="2024-01-25T19:12:47.821145166Z" level=info msg="Removing stale sandbox 56a9b34a9f34e97d0723d3ea1f63b7265cd6147ed2564768028f59d6b33a64bc (658fbc56d177ef524bc3aa166c13766fd591e6800f737095290d01d50163d430)"
Jan 25 19:12:47 minikube dockerd[222]: time="2024-01-25T19:12:47.825741782Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint 1c97a5e6ee9913f1c7f2e964e8761d9bd407a43827631c453492cf3fbd5beb1c 7fd4d56e5d7afcd04fc4e71c01c1693e939f51ad3ce14130270629315173945b], retrying...."
Jan 25 19:12:48 minikube dockerd[222]: time="2024-01-25T19:12:48.012391699Z" level=info msg="Removing stale sandbox 617a06cacdf23de0fc1f2b1c83f59bd6dd7a1ab651502d0b521c03f676b027bb (336654d63edf6e771cc5b0173bcc13e7321e0f0abc399efdcb451b2a643a6453)"
Jan 25 19:12:48 minikube dockerd[222]: time="2024-01-25T19:12:48.024089270Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint 74e206177ccd33d3c685689788d52d17d573001ee69ae33ac4e3201150820ba1 65c222753a8f3a60f1c1fe13996c99b239e8bfd9faa6b93fe66f0dc958fda013], retrying...."
Jan 25 19:12:48 minikube dockerd[222]: time="2024-01-25T19:12:48.226897497Z" level=info msg="Removing stale sandbox 6a961f288fba439b55b1ede75ecf05f25645292f61a1406dca792cc6253ce90e (1a2fcb264b04de8eed583d598a8f4faace65485af80e8d38ff0edc60631c763a)"
Jan 25 19:12:48 minikube dockerd[222]: time="2024-01-25T19:12:48.234129082Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint 1c97a5e6ee9913f1c7f2e964e8761d9bd407a43827631c453492cf3fbd5beb1c cbc5d4bc87d7bd7992ccdf586a5dfb2edd922d03fab80d8c9cc93920726833cc], retrying...."
Jan 25 19:12:48 minikube dockerd[222]: time="2024-01-25T19:12:48.427770138Z" level=info msg="Removing stale sandbox 740b594e7d444ada4804d65befbe2d6a5ebcd6067a5c477d03b7aa433f703a77 (62d76d0006e1ab0ec9082df428e808faf80200b2ba5b1d7e3ac271c926c61d47)"
Jan 25 19:12:48 minikube dockerd[222]: time="2024-01-25T19:12:48.432540989Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint 1c97a5e6ee9913f1c7f2e964e8761d9bd407a43827631c453492cf3fbd5beb1c e148c126649fdd91ae920f024c9b21be37c669d3964478951a6f817cc561236d], retrying...."
Jan 25 19:12:48 minikube dockerd[222]: time="2024-01-25T19:12:48.599321576Z" level=info msg="Removing stale sandbox 9bd97e18216a754a9e824ac2e54a22a2bf348f7c60c507068e983372c2fd2f7b (bf17a92a29fca0dde906b0e8e1b4a78b1944de990eb662a664e29233f533dd9a)"
Jan 25 19:12:48 minikube dockerd[222]: time="2024-01-25T19:12:48.604963025Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint 1c97a5e6ee9913f1c7f2e964e8761d9bd407a43827631c453492cf3fbd5beb1c f184f6c454b6c5d213b2600212dda0915c00742330d5d601118415aeb17e6ef6], retrying...."
Jan 25 19:12:48 minikube dockerd[222]: time="2024-01-25T19:12:48.775390080Z" level=info msg="Removing stale sandbox 9d7a55f539c587f2d5b283e5856f4f3a645eb9276bca4429f684de453ebeef44 (6d56cda67062b4ecb25cb56397ff87c694c4749dcfa8247d0604fa7a2ae1a22e)"
Jan 25 19:12:48 minikube dockerd[222]: time="2024-01-25T19:12:48.786177800Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint 74e206177ccd33d3c685689788d52d17d573001ee69ae33ac4e3201150820ba1 23ae8c9484ec70e6c69985b62555d6ee48bdfb0378936e738211274bd88bb721], retrying...."
Jan 25 19:12:48 minikube dockerd[222]: time="2024-01-25T19:12:48.960851406Z" level=info msg="Removing stale sandbox c344a75e18dd28fc22ea7927602e323592113c588b7b1a30018c5eb6563c543e (dc88077d503492bf5900af8cdc095cf6f124a6d6ad685820d712878ef7f3c461)"
Jan 25 19:12:48 minikube dockerd[222]: time="2024-01-25T19:12:48.972642196Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint 74e206177ccd33d3c685689788d52d17d573001ee69ae33ac4e3201150820ba1 11eeb61e8145850472f12ca23fb44117ef6f9ccb02873d11faa7970ba092be0a], retrying...."
Jan 25 19:12:49 minikube dockerd[222]: time="2024-01-25T19:12:49.154738834Z" level=info msg="Removing stale sandbox c94086493fbc4944a7945591bc4dc0a37749ec9cfaf1284ee70fb5d898cd8547 (cc4357685acee86cc12c6a72c6f10c9e2412c78240936638ecde787a8da5169f)"
Jan 25 19:12:49 minikube dockerd[222]: time="2024-01-25T19:12:49.167395346Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint 74e206177ccd33d3c685689788d52d17d573001ee69ae33ac4e3201150820ba1 38cc7c108475f0b17a9a0ee73aab948ad75ae305f953a83ed79d71292579c25e], retrying...."
Jan 25 19:12:49 minikube dockerd[222]: time="2024-01-25T19:12:49.359937142Z" level=info msg="Removing stale sandbox 25a23326a636fdf2e33be1f2cb1143fc197fbf2037e1b6aeb2e774cbc895b926 (684ef364a5bfea77a9ce1aab5fa94237b2223d62a5d6ddc0382eeb9076c72c99)"
Jan 25 19:12:49 minikube dockerd[222]: time="2024-01-25T19:12:49.365233178Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint 1c97a5e6ee9913f1c7f2e964e8761d9bd407a43827631c453492cf3fbd5beb1c 094d2c462930fa34e623d0bcc6b64d2bcd8b3274e1ee1e14e05e1a1d297580be], retrying...."
Jan 25 19:12:49 minikube dockerd[222]: time="2024-01-25T19:12:49.442110401Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Jan 25 19:12:49 minikube dockerd[222]: time="2024-01-25T19:12:49.533431285Z" level=info msg="Loading containers: done."
Jan 25 19:12:49 minikube dockerd[222]: time="2024-01-25T19:12:49.851031490Z" level=info msg="Docker daemon" commit=75249d8 graphdriver(s)=overlay2 version=20.10.8
Jan 25 19:12:49 minikube dockerd[222]: time="2024-01-25T19:12:49.852512737Z" level=info msg="Daemon has completed initialization"
Jan 25 19:12:50 minikube systemd[1]: Started Docker Application Container Engine.
Jan 25 19:12:50 minikube dockerd[222]: time="2024-01-25T19:12:50.228382339Z" level=info msg="API listen on [::]:2376"
Jan 25 19:12:50 minikube dockerd[222]: time="2024-01-25T19:12:50.233782501Z" level=info msg="API listen on /var/run/docker.sock"
Jan 25 19:14:00 minikube dockerd[222]: time="2024-01-25T19:14:00.151904547Z" level=info msg="ignoring event" container=da84a87eff8d36c3c5fede5f7d6ac05fe6bd0c6d89eaf55c6c55d60a610d325f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 25 19:19:28 minikube dockerd[222]: time="2024-01-25T19:19:28.187418839Z" level=warning msg="reference for unknown type: " digest="sha256:b3aba22b1da80e7acfc52b115cae1d4c687172cbf2b742d5b502419c25ff340e" remote="registry.k8s.io/ingress-nginx/controller@sha256:b3aba22b1da80e7acfc52b115cae1d4c687172cbf2b742d5b502419c25ff340e"
Jan 25 19:22:27 minikube dockerd[222]: time="2024-01-25T19:22:27.147870739Z" level=info msg="Download failed, retrying (1/5): unexpected EOF"
Jan 25 19:24:39 minikube dockerd[222]: time="2024-01-25T19:24:39.391747154Z" level=info msg="Download failed, retrying (1/5): unexpected EOF"
Jan 25 19:24:53 minikube dockerd[222]: time="2024-01-25T19:24:53.279811069Z" level=info msg="Download failed, retrying (2/5): Get \"https://prod-registry-k8s-io-eu-central-1.s3.dualstack.eu-central-1.amazonaws.com/containers/images/sha256:d338f1210194347a4176f3bb61435385f7a7b05cd4e290add010d9a78c71eb9a\": dial tcp [2a05:d050:404f:8540:34db:ab7a::]:443: connect: cannot assign requested address"
Jan 25 19:25:17 minikube dockerd[222]: time="2024-01-25T19:25:17.920218705Z" level=info msg="Download failed, retrying (2/5): unexpected EOF"
Jan 25 19:26:10 minikube dockerd[222]: time="2024-01-25T19:26:10.372280284Z" level=info msg="Download failed, retrying (1/5): unexpected EOF"
Jan 25 19:28:59 minikube dockerd[222]: time="2024-01-25T19:28:59.085324645Z" level=info msg="Download failed, retrying (2/5): unexpected EOF"
Jan 25 19:29:25 minikube dockerd[222]: time="2024-01-25T19:29:25.301007029Z" level=info msg="Download failed, retrying (3/5): Get \"https://prod-registry-k8s-io-eu-central-1.s3.dualstack.eu-central-1.amazonaws.com/containers/images/sha256:1b862f95e61b4263ab1a25f4c07b91fcda0f5d158adb062bfb5c55179abf33f2\": net/http: TLS handshake timeout"
Jan 25 19:31:34 minikube dockerd[222]: time="2024-01-25T19:31:34.756076731Z" level=info msg="Download failed, retrying (1/5): unexpected EOF"
Jan 25 19:33:56 minikube dockerd[222]: time="2024-01-25T19:33:56.045939627Z" level=info msg="Download failed, retrying (4/5): unexpected EOF"
Jan 25 19:34:41 minikube dockerd[222]: time="2024-01-25T19:34:41.884026089Z" level=info msg="Download failed, retrying (2/5): unexpected EOF"
Jan 25 19:35:09 minikube dockerd[222]: time="2024-01-25T19:35:09.345489021Z" level=info msg="Download failed, retrying (3/5): Get \"https://prod-registry-k8s-io-eu-central-1.s3.dualstack.eu-central-1.amazonaws.com/containers/images/sha256:105b8b703448e8dffc56b226d49e2c7ab8e336c5d801899f653e0f05e9264446\": net/http: TLS handshake timeout"
Jan 25 19:35:56 minikube dockerd[222]: time="2024-01-25T19:35:56.988005921Z" level=info msg="Download failed, retrying (3/5): unexpected EOF"
Jan 25 19:36:52 minikube dockerd[222]: time="2024-01-25T19:36:52.196669571Z" level=info msg="Download failed, retrying (5/5): unexpected EOF"
Jan 25 19:37:29 minikube dockerd[222]: time="2024-01-25T19:37:29.567281545Z" level=error msg="Download failed after 6 attempts: Get \"https://prod-registry-k8s-io-eu-central-1.s3.dualstack.eu-central-1.amazonaws.com/containers/images/sha256:1b862f95e61b4263ab1a25f4c07b91fcda0f5d158adb062bfb5c55179abf33f2\": net/http: TLS handshake timeout"
Jan 25 19:38:24 minikube dockerd[222]: time="2024-01-25T19:38:24.995783305Z" level=info msg="Download failed, retrying (4/5): unexpected EOF"
Jan 25 19:39:06 minikube dockerd[222]: time="2024-01-25T19:39:06.131111173Z" level=info msg="Download failed, retrying (4/5): unexpected EOF"
Jan 25 19:41:17 minikube dockerd[222]: time="2024-01-25T19:41:17.687548959Z" level=info msg="Download failed, retrying (1/5): unexpected EOF"
Jan 25 19:41:26 minikube dockerd[222]: time="2024-01-25T19:41:26.777365549Z" level=info msg="Download failed, retrying (5/5): unexpected EOF"
Jan 25 19:42:50 minikube dockerd[222]: time="2024-01-25T19:42:50.512873814Z" level=info msg="Download failed, retrying (5/5): unexpected EOF"
Jan 25 19:44:14 minikube dockerd[222]: time="2024-01-25T19:44:14.308001908Z" level=info msg="Download failed, retrying (2/5): unexpected EOF"
Jan 25 19:47:00 minikube dockerd[222]: time="2024-01-25T19:47:00.571498810Z" level=info msg="Download failed, retrying (3/5): unexpected EOF"
Jan 25 19:47:45 minikube dockerd[222]: time="2024-01-25T19:47:45.899322598Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://prod-registry-k8s-io-eu-central-1.s3.dualstack.eu-central-1.amazonaws.com/containers/images/sha256:1b862f95e61b4263ab1a25f4c07b91fcda0f5d158adb062bfb5c55179abf33f2\": net/http: TLS handshake timeout"
Jan 25 19:47:45 minikube dockerd[222]: time="2024-01-25T19:47:45.955109837Z" level=info msg="Layer sha256:9ed37a3e817fad35cc3da6f254622ca226650fc537dd48f16c2bb199b3270fb7 cleaned up"
Jan 25 19:47:45 minikube dockerd[222]: time="2024-01-25T19:47:45.955172475Z" level=info msg="Layer sha256:8e39c8972428e5f5e8fb871757ff7053f297808d5ce8531c2a56c4544ca7933b cleaned up"
Jan 25 19:47:46 minikube dockerd[222]: time="2024-01-25T19:47:46.118147252Z" level=info msg="Layer sha256:5af4f8f59b764c64c6def53f52ada809fe38d528441d08d01c206dfb3fc3b691 cleaned up"
Jan 25 19:47:46 minikube dockerd[222]: time="2024-01-25T19:47:46.950922708Z" level=info msg="ignoring event" container=abd143c06dd2707c2c5557ff302b67d00eff5f2b909870b0ee93e99b79cf1896 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                                  CREATED             STATE               NAME                      ATTEMPT             POD ID
a9f5c01f4c5f4       a9f76bcccfb5f                                                                          31 minutes ago      Running             controller                0                   5438ce47a26e5
ad07d83721c97       6e38f40d628db                                                                          37 minutes ago      Running             storage-provisioner       20                  ecdce05d51da0
04a01adf8ca22       postgres@sha256:49c276fa02e3d61bd9b8db81dfb4784fe814f50f778dce5980a03817438293e3       37 minutes ago      Running             postgres                  1                   7d0cf58de587c
aa24af39ae423       e81a1755dc17e                                                                          37 minutes ago      Running             node-back-end             1                   d1d17f557fbab
99187b085a197       6120bd723dced                                                                          37 minutes ago      Running             kube-proxy                9                   a449bd637e2e9
b2d3ab11cddbb       5b6309474b5c5                                                                          37 minutes ago      Running             frontend-react            1                   66fa78da355f1
24faa035d0e51       8d147537fb7d1                                                                          37 minutes ago      Running             coredns                   9                   a9dfd38a2f0f7
da84a87eff8d3       6e38f40d628db                                                                          38 minutes ago      Exited              storage-provisioner       19                  ecdce05d51da0
7bb82774b9d67       6de166512aa22                                                                          38 minutes ago      Running             kindnet-cni               8                   de5d9f87edf56
c2844a63dfc45       53224b502ea4d                                                                          38 minutes ago      Running             kube-apiserver            9                   19b4c2847528a
14df4ac85e957       0aa9c7e31d307                                                                          38 minutes ago      Running             kube-scheduler            9                   eb5b063d18aa4
1b6a069f54bdc       0048118155842                                                                          38 minutes ago      Running             etcd                      9                   4ab73a07a2b35
7103b6bf75189       05c905cef780c                                                                          38 minutes ago      Running             kube-controller-manager   9                   62622deecc439
cf72528f59fbb       postgres@sha256:49c276fa02e3d61bd9b8db81dfb4784fe814f50f778dce5980a03817438293e3       28 hours ago        Exited              postgres                  0                   dc88077d50349
6ffd79e52cd92       mo1074/react@sha256:2b5e7f29b0618730f145e9c6c7344f144fa46ddb26c8f9883dd36e8b7c1b6fbc   28 hours ago        Exited              frontend-react            0                   6d56cda67062b
428e55a5fd467       mo1074/node@sha256:96b4eb46f40603ed71f2dc4cf3044d58ac36b24fcf2be925d35f4f231fec7804    29 hours ago        Exited              node-back-end             0                   336654d63edf6
12b48bf940c3d       8d147537fb7d1                                                                          29 hours ago        Exited              coredns                   8                   cc4357685acee
5598fdb8f3970       6de166512aa22                                                                          29 hours ago        Exited              kindnet-cni               7                   62d76d0006e1a
2b7a56aea1bd3       6120bd723dced                                                                          29 hours ago        Exited              kube-proxy                8                   684ef364a5bfe
58d871a33e1ce       0aa9c7e31d307                                                                          29 hours ago        Exited              kube-scheduler            8                   bf17a92a29fca
ba2861f29cefa       53224b502ea4d                                                                          29 hours ago        Exited              kube-apiserver            8                   8e351047d026a
97deec103a40c       0048118155842                                                                          29 hours ago        Exited              etcd                      8                   658fbc56d177e
df5d1cd5aca38       05c905cef780c                                                                          29 hours ago        Exited              kube-controller-manager   8                   1a2fcb264b04d

* 
* ==> coredns [12b48bf940c3] <==
* .:53
[INFO] plugin/reload: Running configuration MD5 = 64bc18231851429e450c3543694a16cc
CoreDNS-1.8.4
linux/amd64, go1.16.4, 053c4d5

* 
* ==> coredns [24faa035d0e5] <==
* [INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration MD5 = 64bc18231851429e450c3543694a16cc
CoreDNS-1.8.4
linux/amd64, go1.16.4, 053c4d5
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane,master
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=76b94fb3c4e8ac5062daf70d60cf03ddcc0a741b
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/updated_at=2024_01_22T13_15_44_0700
                    minikube.k8s.io/version=v1.24.0
                    node-role.kubernetes.io/control-plane=
                    node-role.kubernetes.io/master=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Mon, 22 Jan 2024 10:15:40 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Thu, 25 Jan 2024 19:51:18 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Thu, 25 Jan 2024 19:48:41 +0000   Tue, 23 Jan 2024 16:03:49 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Thu, 25 Jan 2024 19:48:41 +0000   Tue, 23 Jan 2024 16:03:49 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Thu, 25 Jan 2024 19:48:41 +0000   Tue, 23 Jan 2024 16:03:49 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Thu, 25 Jan 2024 19:48:41 +0000   Tue, 23 Jan 2024 16:03:49 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                12
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3673200Ki
  pods:               110
Allocatable:
  cpu:                12
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3673200Ki
  pods:               110
System Info:
  Machine ID:                 bba0be70c47c400ea3cf7733f1c0b4c1
  System UUID:                bba0be70c47c400ea3cf7733f1c0b4c1
  Boot ID:                    99f11296-2f3d-4c84-877d-dd99d6885459
  Kernel Version:             5.15.90.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 20.04.2 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://20.10.8
  Kubelet Version:            v1.22.3
  Kube-Proxy Version:         v1.22.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (12 in total)
  Namespace                   Name                                         CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                         ------------  ----------  ---------------  -------------  ---
  default                     frontend-699c74878d-mw9tc                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         30h
  default                     node-back-end-7bb94dffbc-7vl5t               0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         30h
  default                     postgres-deployment-674cfbd48-xmjvk          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         30h
  ingress-nginx               ingress-nginx-controller-5f66978484-xz587    100m (0%!)(MISSING)     0 (0%!)(MISSING)      90Mi (2%!)(MISSING)        0 (0%!)(MISSING)         32m
  kube-system                 coredns-78fcd69978-fxjn2                     100m (0%!)(MISSING)     0 (0%!)(MISSING)      70Mi (1%!)(MISSING)        170Mi (4%!)(MISSING)     3d9h
  kube-system                 etcd-minikube                                100m (0%!)(MISSING)     0 (0%!)(MISSING)      100Mi (2%!)(MISSING)       0 (0%!)(MISSING)         3d9h
  kube-system                 kindnet-5tdfc                                100m (0%!)(MISSING)     100m (0%!)(MISSING)   50Mi (1%!)(MISSING)        50Mi (1%!)(MISSING)      3d9h
  kube-system                 kube-apiserver-minikube                      250m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d9h
  kube-system                 kube-controller-manager-minikube             200m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d9h
  kube-system                 kube-proxy-nc2gp                             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d9h
  kube-system                 kube-scheduler-minikube                      100m (0%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d9h
  kube-system                 storage-provisioner                          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d9h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                950m (7%!)(MISSING)   100m (0%!)(MISSING)
  memory             310Mi (8%!)(MISSING)  220Mi (6%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                From        Message
  ----    ------                   ----               ----        -------
  Normal  Starting                 37m                kube-proxy  
  Normal  Starting                 38m                kubelet     Starting kubelet.
  Normal  NodeAllocatableEnforced  38m                kubelet     Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  38m (x8 over 38m)  kubelet     Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    38m (x8 over 38m)  kubelet     Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     38m (x7 over 38m)  kubelet     Node minikube status is now: NodeHasSufficientPID


Name:               minikube-m02
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube-m02
                    kubernetes.io/os=linux
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Thu, 25 Jan 2024 19:38:16 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube-m02
  AcquireTime:     <unset>
  RenewTime:       Thu, 25 Jan 2024 19:51:20 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Thu, 25 Jan 2024 19:48:30 +0000   Thu, 25 Jan 2024 19:38:16 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Thu, 25 Jan 2024 19:48:30 +0000   Thu, 25 Jan 2024 19:38:16 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Thu, 25 Jan 2024 19:48:30 +0000   Thu, 25 Jan 2024 19:38:16 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Thu, 25 Jan 2024 19:48:30 +0000   Thu, 25 Jan 2024 19:38:26 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.3
  Hostname:    minikube-m02
Capacity:
  cpu:                12
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3673200Ki
  pods:               110
Allocatable:
  cpu:                12
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3673200Ki
  pods:               110
System Info:
  Machine ID:                 bba0be70c47c400ea3cf7733f1c0b4c1
  System UUID:                bba0be70c47c400ea3cf7733f1c0b4c1
  Boot ID:                    99f11296-2f3d-4c84-877d-dd99d6885459
  Kernel Version:             5.15.90.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 20.04.2 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://20.10.8
  Kubelet Version:            v1.22.3
  Kube-Proxy Version:         v1.22.3
PodCIDR:                      10.244.1.0/24
PodCIDRs:                     10.244.1.0/24
Non-terminated Pods:          (2 in total)
  Namespace                   Name                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                ------------  ----------  ---------------  -------------  ---
  kube-system                 kindnet-llqkw       100m (0%!)(MISSING)     100m (0%!)(MISSING)   50Mi (1%!)(MISSING)        50Mi (1%!)(MISSING)      3d9h
  kube-system                 kube-proxy-kxmdl    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d9h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests   Limits
  --------           --------   ------
  cpu                100m (0%!)(MISSING)  100m (0%!)(MISSING)
  memory             50Mi (1%!)(MISSING)  50Mi (1%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)     0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)     0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)     0 (0%!)(MISSING)
Events:
  Type     Reason                   Age                From        Message
  ----     ------                   ----               ----        -------
  Normal   Starting                 13m                kube-proxy  
  Normal   Starting                 13m                kubelet     Starting kubelet.
  Normal   NodeNotReady             13m                kubelet     Node minikube-m02 status is now: NodeNotReady
  Normal   NodeAllocatableEnforced  13m                kubelet     Updated Node Allocatable limit across pods
  Warning  Rebooted                 13m                kubelet     Node minikube-m02 has been rebooted, boot id: 99f11296-2f3d-4c84-877d-dd99d6885459
  Normal   NodeReady                13m                kubelet     Node minikube-m02 status is now: NodeReady
  Normal   NodeHasSufficientPID     13m (x3 over 13m)  kubelet     Node minikube-m02 status is now: NodeHasSufficientPID
  Normal   NodeHasNoDiskPressure    13m (x3 over 13m)  kubelet     Node minikube-m02 status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientMemory  13m (x3 over 13m)  kubelet     Node minikube-m02 status is now: NodeHasSufficientMemory
  Normal   Starting                 13m                kubelet     Starting kubelet.
  Normal   NodeHasSufficientMemory  13m                kubelet     Node minikube-m02 status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    13m                kubelet     Node minikube-m02 status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     13m                kubelet     Node minikube-m02 status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced  13m                kubelet     Updated Node Allocatable limit across pods
  Normal   NodeReady                12m                kubelet     Node minikube-m02 status is now: NodeReady


Name:               minikube-m03
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube-m03
                    kubernetes.io/os=linux
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Thu, 25 Jan 2024 19:38:32 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube-m03
  AcquireTime:     <unset>
  RenewTime:       Thu, 25 Jan 2024 19:51:19 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Thu, 25 Jan 2024 19:48:46 +0000   Thu, 25 Jan 2024 19:38:32 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Thu, 25 Jan 2024 19:48:46 +0000   Thu, 25 Jan 2024 19:38:32 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Thu, 25 Jan 2024 19:48:46 +0000   Thu, 25 Jan 2024 19:38:32 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Thu, 25 Jan 2024 19:48:46 +0000   Thu, 25 Jan 2024 19:38:43 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.4
  Hostname:    minikube-m03
Capacity:
  cpu:                12
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3673200Ki
  pods:               110
Allocatable:
  cpu:                12
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3673200Ki
  pods:               110
System Info:
  Machine ID:                 bba0be70c47c400ea3cf7733f1c0b4c1
  System UUID:                bba0be70c47c400ea3cf7733f1c0b4c1
  Boot ID:                    99f11296-2f3d-4c84-877d-dd99d6885459
  Kernel Version:             5.15.90.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 20.04.2 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://20.10.8
  Kubelet Version:            v1.22.3
  Kube-Proxy Version:         v1.22.3
PodCIDR:                      10.244.2.0/24
PodCIDRs:                     10.244.2.0/24
Non-terminated Pods:          (2 in total)
  Namespace                   Name                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                ------------  ----------  ---------------  -------------  ---
  kube-system                 kindnet-6gvnv       100m (0%!)(MISSING)     100m (0%!)(MISSING)   50Mi (1%!)(MISSING)        50Mi (1%!)(MISSING)      3d9h
  kube-system                 kube-proxy-ct2gp    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d9h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests   Limits
  --------           --------   ------
  cpu                100m (0%!)(MISSING)  100m (0%!)(MISSING)
  memory             50Mi (1%!)(MISSING)  50Mi (1%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)     0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)     0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)     0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                From        Message
  ----    ------                   ----               ----        -------
  Normal  Starting                 12m                kube-proxy  
  Normal  Starting                 12m                kubelet     Starting kubelet.
  Normal  NodeHasSufficientMemory  12m (x2 over 12m)  kubelet     Node minikube-m03 status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    12m (x2 over 12m)  kubelet     Node minikube-m03 status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     12m (x2 over 12m)  kubelet     Node minikube-m03 status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  12m                kubelet     Updated Node Allocatable limit across pods
  Normal  NodeReady                12m                kubelet     Node minikube-m03 status is now: NodeReady

* 
* ==> dmesg <==
* [Jan25 19:07]   #2  #3  #4  #5  #6  #7  #8  #9 #10 #11
[  +0.021213] PCI: Fatal: No config space access function found
[  +0.031716] PCI: System does not support PCI
[  +0.028746] kvm: no hardware support
[  +2.113673] FS-Cache: Duplicate cookie detected
[  +0.000699] FS-Cache: O-cookie c=00000005 [p=00000002 fl=222 nc=0 na=1]
[  +0.000723] FS-Cache: O-cookie d=00000000d2c0397d{9P.session} n=00000000ff46a603
[  +0.000918] FS-Cache: O-key=[10] '34323934393337353136'
[  +0.000643] FS-Cache: N-cookie c=00000006 [p=00000002 fl=2 nc=0 na=1]
[  +0.000700] FS-Cache: N-cookie d=00000000d2c0397d{9P.session} n=000000008758514a
[  +0.001639] FS-Cache: N-key=[10] '34323934393337353136'
[Jan25 19:08] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001308] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001055] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001380] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.682992] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001068] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001216] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001633] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +1.171149] WSL (1) ERROR: ConfigApplyWindowsLibPath:2431: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000007]  failed 2
[  +0.049002] WSL (1) WARNING: /usr/share/zoneinfo/Europe/Istanbul not found. Is the tzdata package installed?
[  +0.153866] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.002347] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001334] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001430] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +1.504845] WSL (2) ERROR: UtilCreateProcessAndWait:662: /bin/mount failed with 2
[  +0.003229] WSL (1) ERROR: UtilCreateProcessAndWait:684: /bin/mount failed with status 0xff00

[  +0.002148] WSL (1) ERROR: ConfigMountFsTab:2483: Processing fstab with mount -a failed.
[  +0.012067] WSL (1) ERROR: ConfigApplyWindowsLibPath:2431: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000009]  failed 2
[  +0.148761] WSL (1) WARNING: /usr/share/zoneinfo/Europe/Istanbul not found. Is the tzdata package installed?
[  +0.089501] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001872] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001580] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001343] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +1.797097] netlink: 'init': attribute type 4 has an invalid length.
[  +1.010112] kmem.limit_in_bytes is deprecated and will be removed. Please report your usecase to linux-mm@kvack.org if you depend on this functionality.

* 
* ==> etcd [1b6a069f54bd] <==
* {"level":"info","ts":"2024-01-25T19:13:38.329Z","caller":"traceutil/trace.go:171","msg":"trace[1742583489] range","detail":"{range_begin:/registry/leases/kube-node-lease/minikube-m03; range_end:; response_count:1; response_revision:122725; }","duration":"194.857142ms","start":"2024-01-25T19:13:38.134Z","end":"2024-01-25T19:13:38.329Z","steps":["trace[1742583489] 'agreement among raft nodes before linearized reading'  (duration: 194.222791ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-25T19:13:38.329Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"183.570901ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/deployments/default/postgres-deployment\" ","response":"range_response_count:1 size:3603"}
{"level":"warn","ts":"2024-01-25T19:13:38.329Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"193.175765ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/deployments/default/node-back-end\" ","response":"range_response_count:1 size:3408"}
{"level":"info","ts":"2024-01-25T19:13:38.329Z","caller":"traceutil/trace.go:171","msg":"trace[1084138738] range","detail":"{range_begin:/registry/deployments/default/postgres-deployment; range_end:; response_count:1; response_revision:122725; }","duration":"184.179113ms","start":"2024-01-25T19:13:38.145Z","end":"2024-01-25T19:13:38.329Z","steps":["trace[1084138738] 'agreement among raft nodes before linearized reading'  (duration: 183.442541ms)"],"step_count":1}
{"level":"info","ts":"2024-01-25T19:13:38.329Z","caller":"traceutil/trace.go:171","msg":"trace[1799094591] range","detail":"{range_begin:/registry/deployments/default/node-back-end; range_end:; response_count:1; response_revision:122725; }","duration":"193.21592ms","start":"2024-01-25T19:13:38.136Z","end":"2024-01-25T19:13:38.329Z","steps":["trace[1799094591] 'agreement among raft nodes before linearized reading'  (duration: 193.137493ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-25T19:13:38.330Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"388.275763ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/replicaset-controller\" ","response":"range_response_count:1 size:261"}
{"level":"info","ts":"2024-01-25T19:13:38.330Z","caller":"traceutil/trace.go:171","msg":"trace[86170942] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/replicaset-controller; range_end:; response_count:1; response_revision:122725; }","duration":"388.347617ms","start":"2024-01-25T19:13:37.941Z","end":"2024-01-25T19:13:38.330Z","steps":["trace[86170942] 'agreement among raft nodes before linearized reading'  (duration: 388.177768ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-25T19:13:38.330Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-25T19:13:37.941Z","time spent":"388.40213ms","remote":"127.0.0.1:60312","response type":"/etcdserverpb.KV/Range","request count":0,"request size":61,"response count":1,"response size":285,"request content":"key:\"/registry/serviceaccounts/kube-system/replicaset-controller\" "}
{"level":"info","ts":"2024-01-25T19:13:38.330Z","caller":"traceutil/trace.go:171","msg":"trace[425360262] range","detail":"{range_begin:/registry/deployments/kube-system/coredns; range_end:; response_count:1; response_revision:122725; }","duration":"185.932125ms","start":"2024-01-25T19:13:38.143Z","end":"2024-01-25T19:13:38.329Z","steps":["trace[425360262] 'agreement among raft nodes before linearized reading'  (duration: 185.754842ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-25T19:13:38.744Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"111.797322ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128026742279231390 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/services/endpoints/default/service\" mod_revision:106156 > success:<request_put:<key:\"/registry/services/endpoints/default/service\" value_size:315 >> failure:<request_range:<key:\"/registry/services/endpoints/default/service\" > >>","response":"size:18"}
{"level":"info","ts":"2024-01-25T19:13:38.745Z","caller":"traceutil/trace.go:171","msg":"trace[223688671] transaction","detail":"{read_only:false; response_revision:122732; number_of_response:1; }","duration":"106.470251ms","start":"2024-01-25T19:13:38.639Z","end":"2024-01-25T19:13:38.745Z","steps":["trace[223688671] 'process raft request'  (duration: 106.354784ms)"],"step_count":1}
{"level":"info","ts":"2024-01-25T19:13:38.746Z","caller":"traceutil/trace.go:171","msg":"trace[2115206179] transaction","detail":"{read_only:false; response_revision:122728; number_of_response:1; }","duration":"206.325681ms","start":"2024-01-25T19:13:38.540Z","end":"2024-01-25T19:13:38.746Z","steps":["trace[2115206179] 'process raft request'  (duration: 88.353385ms)","trace[2115206179] 'get key's previous created_revision and leaseID' {req_type:put; key:/registry/services/endpoints/default/service; req_size:364; } (duration: 111.494244ms)"],"step_count":2}
{"level":"info","ts":"2024-01-25T19:13:38.747Z","caller":"traceutil/trace.go:171","msg":"trace[1725571491] transaction","detail":"{read_only:false; response_revision:122729; number_of_response:1; }","duration":"202.062526ms","start":"2024-01-25T19:13:38.545Z","end":"2024-01-25T19:13:38.747Z","steps":["trace[1725571491] 'process raft request'  (duration: 199.643434ms)"],"step_count":1}
{"level":"info","ts":"2024-01-25T19:13:38.747Z","caller":"traceutil/trace.go:171","msg":"trace[340316622] transaction","detail":"{read_only:false; response_revision:122730; number_of_response:1; }","duration":"118.715231ms","start":"2024-01-25T19:13:38.628Z","end":"2024-01-25T19:13:38.747Z","steps":["trace[340316622] 'process raft request'  (duration: 116.452743ms)"],"step_count":1}
{"level":"info","ts":"2024-01-25T19:13:38.747Z","caller":"traceutil/trace.go:171","msg":"trace[1622496909] transaction","detail":"{read_only:false; response_revision:122731; number_of_response:1; }","duration":"114.998538ms","start":"2024-01-25T19:13:38.632Z","end":"2024-01-25T19:13:38.747Z","steps":["trace[1622496909] 'process raft request'  (duration: 112.621625ms)"],"step_count":1}
{"level":"info","ts":"2024-01-25T19:13:39.140Z","caller":"traceutil/trace.go:171","msg":"trace[1678850897] transaction","detail":"{read_only:false; response_revision:122735; number_of_response:1; }","duration":"109.436175ms","start":"2024-01-25T19:13:39.031Z","end":"2024-01-25T19:13:39.140Z","steps":["trace[1678850897] 'process raft request'  (duration: 23.912641ms)","trace[1678850897] 'compare'  (duration: 85.291219ms)"],"step_count":2}
{"level":"info","ts":"2024-01-25T19:13:39.429Z","caller":"traceutil/trace.go:171","msg":"trace[2126099829] linearizableReadLoop","detail":"{readStateIndex:147565; appliedIndex:147565; }","duration":"186.00176ms","start":"2024-01-25T19:13:39.243Z","end":"2024-01-25T19:13:39.429Z","steps":["trace[2126099829] 'read index received'  (duration: 185.984215ms)","trace[2126099829] 'applied index is now lower than readState.Index'  (duration: 14.529µs)"],"step_count":2}
{"level":"warn","ts":"2024-01-25T19:13:39.439Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"101.604286ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/configmaps/ingress-nginx/tcp-services\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-01-25T19:13:39.440Z","caller":"traceutil/trace.go:171","msg":"trace[1780696105] range","detail":"{range_begin:/registry/configmaps/ingress-nginx/tcp-services; range_end:; response_count:0; response_revision:122736; }","duration":"101.816082ms","start":"2024-01-25T19:13:39.338Z","end":"2024-01-25T19:13:39.440Z","steps":["trace[1780696105] 'agreement among raft nodes before linearized reading'  (duration: 95.694964ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-25T19:13:39.441Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"197.99897ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-01-25T19:13:39.441Z","caller":"traceutil/trace.go:171","msg":"trace[621563527] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:122736; }","duration":"198.144515ms","start":"2024-01-25T19:13:39.243Z","end":"2024-01-25T19:13:39.441Z","steps":["trace[621563527] 'agreement among raft nodes before linearized reading'  (duration: 190.450678ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-25T19:13:39.441Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"106.471128ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-01-25T19:13:39.441Z","caller":"traceutil/trace.go:171","msg":"trace[415408054] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:122736; }","duration":"106.53357ms","start":"2024-01-25T19:13:39.334Z","end":"2024-01-25T19:13:39.441Z","steps":["trace[415408054] 'agreement among raft nodes before linearized reading'  (duration: 98.90521ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-25T19:13:40.543Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"102.989281ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128026742279231428 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/roles/ingress-nginx/ingress-nginx\" mod_revision:108983 > success:<request_put:<key:\"/registry/roles/ingress-nginx/ingress-nginx\" value_size:2087 >> failure:<request_range:<key:\"/registry/roles/ingress-nginx/ingress-nginx\" > >>","response":"size:18"}
{"level":"info","ts":"2024-01-25T19:13:40.544Z","caller":"traceutil/trace.go:171","msg":"trace[1091416924] transaction","detail":"{read_only:false; response_revision:122744; number_of_response:1; }","duration":"105.490519ms","start":"2024-01-25T19:13:40.438Z","end":"2024-01-25T19:13:40.544Z","steps":["trace[1091416924] 'compare'  (duration: 102.675817ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-25T19:13:40.544Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"105.219128ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/storage-provisioner\" ","response":"range_response_count:1 size:914"}
{"level":"info","ts":"2024-01-25T19:13:40.545Z","caller":"traceutil/trace.go:171","msg":"trace[652541892] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/storage-provisioner; range_end:; response_count:1; response_revision:122745; }","duration":"105.341367ms","start":"2024-01-25T19:13:40.439Z","end":"2024-01-25T19:13:40.545Z","steps":["trace[652541892] 'agreement among raft nodes before linearized reading'  (duration: 105.073453ms)"],"step_count":1}
{"level":"info","ts":"2024-01-25T19:13:40.546Z","caller":"traceutil/trace.go:171","msg":"trace[157744808] linearizableReadLoop","detail":"{readStateIndex:147574; appliedIndex:147573; }","duration":"104.016452ms","start":"2024-01-25T19:13:40.439Z","end":"2024-01-25T19:13:40.543Z","steps":["trace[157744808] 'read index received'  (duration: 8.806979ms)","trace[157744808] 'applied index is now lower than readState.Index'  (duration: 95.206016ms)"],"step_count":2}
{"level":"warn","ts":"2024-01-25T19:13:41.328Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"188.258156ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128026742279231448 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/services/endpoints/ingress-nginx/ingress-nginx-controller\" mod_revision:122052 > success:<request_put:<key:\"/registry/services/endpoints/ingress-nginx/ingress-nginx-controller\" value_size:671 >> failure:<request_range:<key:\"/registry/services/endpoints/ingress-nginx/ingress-nginx-controller\" > >>","response":"size:18"}
{"level":"info","ts":"2024-01-25T19:13:41.330Z","caller":"traceutil/trace.go:171","msg":"trace[1728366533] transaction","detail":"{read_only:false; response_revision:122751; number_of_response:1; }","duration":"190.832839ms","start":"2024-01-25T19:13:41.139Z","end":"2024-01-25T19:13:41.330Z","steps":["trace[1728366533] 'get key's previous created_revision and leaseID' {req_type:put; key:/registry/services/endpoints/ingress-nginx/ingress-nginx-controller; req_size:743; } (duration: 187.990282ms)"],"step_count":1}
{"level":"info","ts":"2024-01-25T19:13:41.331Z","caller":"traceutil/trace.go:171","msg":"trace[254502572] transaction","detail":"{read_only:false; response_revision:122752; number_of_response:1; }","duration":"176.65446ms","start":"2024-01-25T19:13:41.154Z","end":"2024-01-25T19:13:41.331Z","steps":["trace[254502572] 'process raft request'  (duration: 176.023394ms)"],"step_count":1}
{"level":"info","ts":"2024-01-25T19:13:41.331Z","caller":"traceutil/trace.go:171","msg":"trace[637539430] linearizableReadLoop","detail":"{readStateIndex:147581; appliedIndex:147580; }","duration":"180.372422ms","start":"2024-01-25T19:13:41.151Z","end":"2024-01-25T19:13:41.331Z","steps":["trace[637539430] 'read index received'  (duration: 96.885215ms)","trace[637539430] 'applied index is now lower than readState.Index'  (duration: 83.485553ms)"],"step_count":2}
{"level":"warn","ts":"2024-01-25T19:13:41.331Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"180.594287ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/ranges/servicenodeports\" ","response":"range_response_count:1 size:305"}
{"level":"info","ts":"2024-01-25T19:13:41.332Z","caller":"traceutil/trace.go:171","msg":"trace[1590688721] range","detail":"{range_begin:/registry/ranges/servicenodeports; range_end:; response_count:1; response_revision:122752; }","duration":"181.263026ms","start":"2024-01-25T19:13:41.151Z","end":"2024-01-25T19:13:41.332Z","steps":["trace[1590688721] 'agreement among raft nodes before linearized reading'  (duration: 180.568316ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-25T19:13:41.335Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"177.622795ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/service-controller\" ","response":"range_response_count:1 size:252"}
{"level":"info","ts":"2024-01-25T19:13:41.335Z","caller":"traceutil/trace.go:171","msg":"trace[2103835350] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/service-controller; range_end:; response_count:1; response_revision:122752; }","duration":"177.789181ms","start":"2024-01-25T19:13:41.157Z","end":"2024-01-25T19:13:41.335Z","steps":["trace[2103835350] 'agreement among raft nodes before linearized reading'  (duration: 177.541746ms)"],"step_count":1}
{"level":"info","ts":"2024-01-25T19:23:16.511Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":122988}
{"level":"info","ts":"2024-01-25T19:23:16.589Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":122988,"took":"72.250304ms"}
{"level":"info","ts":"2024-01-25T19:28:16.541Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":123287}
{"level":"info","ts":"2024-01-25T19:28:16.543Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":123287,"took":"1.133959ms"}
{"level":"info","ts":"2024-01-25T19:33:16.551Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":123537}
{"level":"info","ts":"2024-01-25T19:33:16.553Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":123537,"took":"1.919846ms"}
{"level":"info","ts":"2024-01-25T19:38:16.561Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":123787}
{"level":"info","ts":"2024-01-25T19:38:16.579Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":123787,"took":"13.13435ms"}
{"level":"warn","ts":"2024-01-25T19:41:13.732Z","caller":"etcdserver/v3_server.go:815","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":8128026742279240934,"retry-timeout":"500ms"}
{"level":"info","ts":"2024-01-25T19:41:13.925Z","caller":"traceutil/trace.go:171","msg":"trace[1931264562] linearizableReadLoop","detail":"{readStateIndex:149505; appliedIndex:149505; }","duration":"685.465887ms","start":"2024-01-25T19:41:13.231Z","end":"2024-01-25T19:41:13.916Z","steps":["trace[1931264562] 'read index received'  (duration: 685.45666ms)","trace[1931264562] 'applied index is now lower than readState.Index'  (duration: 7.925µs)"],"step_count":2}
{"level":"warn","ts":"2024-01-25T19:41:13.938Z","caller":"etcdserver/v3_server.go:792","msg":"ignored out-of-date read index response; local node read indexes queueing up and waiting to be in sync with leader","sent-request-id":8128026742279240936,"received-request-id":8128026742279240934}
{"level":"warn","ts":"2024-01-25T19:41:13.959Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"113.55387ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/validatingwebhookconfigurations/\" range_end:\"/registry/validatingwebhookconfigurations0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"warn","ts":"2024-01-25T19:41:13.958Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"688.789543ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-01-25T19:41:13.960Z","caller":"traceutil/trace.go:171","msg":"trace[311115551] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:124330; }","duration":"729.363904ms","start":"2024-01-25T19:41:13.231Z","end":"2024-01-25T19:41:13.960Z","steps":["trace[311115551] 'agreement among raft nodes before linearized reading'  (duration: 685.756592ms)"],"step_count":1}
{"level":"info","ts":"2024-01-25T19:41:13.960Z","caller":"traceutil/trace.go:171","msg":"trace[171836195] range","detail":"{range_begin:/registry/validatingwebhookconfigurations/; range_end:/registry/validatingwebhookconfigurations0; response_count:0; response_revision:124330; }","duration":"135.047128ms","start":"2024-01-25T19:41:13.825Z","end":"2024-01-25T19:41:13.960Z","steps":["trace[171836195] 'agreement among raft nodes before linearized reading'  (duration: 113.494739ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-25T19:41:13.962Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-25T19:41:13.231Z","time spent":"729.635644ms","remote":"127.0.0.1:60252","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2024-01-25T19:43:16.577Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":124081}
{"level":"info","ts":"2024-01-25T19:43:16.615Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":124081,"took":"34.518734ms"}
{"level":"info","ts":"2024-01-25T19:47:54.269Z","caller":"etcdserver/server.go:1368","msg":"triggering snapshot","local-member-id":"aec36adc501070cc","local-member-applied-index":150017,"local-member-snapshot-index":140016,"local-member-snapshot-count":10000}
{"level":"info","ts":"2024-01-25T19:47:54.297Z","caller":"etcdserver/server.go:2363","msg":"saved snapshot","snapshot-index":150017}
{"level":"info","ts":"2024-01-25T19:47:54.297Z","caller":"etcdserver/server.go:2393","msg":"compacted Raft logs","compact-index":145017}
{"level":"info","ts":"2024-01-25T19:48:16.323Z","caller":"fileutil/purge.go:77","msg":"purged","path":"/var/lib/minikube/etcd/member/snap/0000000000000005-00000000000186aa.snap"}
{"level":"info","ts":"2024-01-25T19:48:16.587Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":124457}
{"level":"info","ts":"2024-01-25T19:48:16.601Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":124457,"took":"13.47683ms"}

* 
* ==> etcd [97deec103a40] <==
* {"level":"info","ts":"2024-01-24T18:59:49.127Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":117171,"took":"715.503µs"}
{"level":"info","ts":"2024-01-24T19:04:49.135Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":117481}
{"level":"info","ts":"2024-01-24T19:04:49.136Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":117481,"took":"698.247µs"}
{"level":"info","ts":"2024-01-24T19:09:49.150Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":117752}
{"level":"info","ts":"2024-01-24T19:09:49.151Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":117752,"took":"666.071µs"}
{"level":"info","ts":"2024-01-24T19:14:49.159Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":118023}
{"level":"info","ts":"2024-01-24T19:14:49.161Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":118023,"took":"1.286322ms"}
{"level":"info","ts":"2024-01-24T19:19:49.168Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":118295}
{"level":"info","ts":"2024-01-24T19:19:49.169Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":118295,"took":"632.017µs"}
{"level":"info","ts":"2024-01-24T19:24:49.178Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":118592}
{"level":"info","ts":"2024-01-24T19:24:49.179Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":118592,"took":"735.931µs"}
{"level":"info","ts":"2024-01-24T19:29:49.187Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":118865}
{"level":"info","ts":"2024-01-24T19:29:49.188Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":118865,"took":"550.355µs"}
{"level":"info","ts":"2024-01-24T19:34:49.199Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":119136}
{"level":"info","ts":"2024-01-24T19:34:49.200Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":119136,"took":"646.235µs"}
{"level":"warn","ts":"2024-01-24T19:37:38.410Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"391.388164ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-01-24T19:37:38.432Z","caller":"traceutil/trace.go:171","msg":"trace[1181270689] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:119563; }","duration":"548.331809ms","start":"2024-01-24T19:37:37.883Z","end":"2024-01-24T19:37:38.431Z","steps":["trace[1181270689] 'range keys from in-memory index tree'  (duration: 379.817147ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-24T19:37:38.500Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-24T19:37:37.883Z","time spent":"616.260939ms","remote":"127.0.0.1:43592","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2024-01-24T19:39:49.207Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":119407}
{"level":"info","ts":"2024-01-24T19:39:49.208Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":119407,"took":"590.287µs"}
{"level":"info","ts":"2024-01-24T19:44:49.215Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":119681}
{"level":"info","ts":"2024-01-24T19:44:49.218Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":119681,"took":"2.6538ms"}
{"level":"info","ts":"2024-01-24T19:49:49.224Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":119977}
{"level":"info","ts":"2024-01-24T19:49:49.225Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":119977,"took":"1.022301ms"}
{"level":"info","ts":"2024-01-24T19:54:49.234Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":120248}
{"level":"info","ts":"2024-01-24T19:54:49.235Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":120248,"took":"816.093µs"}
{"level":"info","ts":"2024-01-24T19:58:52.099Z","caller":"traceutil/trace.go:171","msg":"trace[297096086] transaction","detail":"{read_only:false; response_revision:120741; number_of_response:1; }","duration":"101.168523ms","start":"2024-01-24T19:58:51.998Z","end":"2024-01-24T19:58:52.099Z","steps":["trace[297096086] 'process raft request'  (duration: 94.857579ms)"],"step_count":1}
{"level":"info","ts":"2024-01-24T19:59:49.244Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":120520}
{"level":"info","ts":"2024-01-24T19:59:49.245Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":120520,"took":"848.836µs"}
{"level":"warn","ts":"2024-01-24T20:00:33.892Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"100.73021ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128026716190496312 username:\"kube-apiserver-etcd-client\" auth_revision:1 > lease_grant:<ttl:3660-second id:70cc8d3bf76b5637>","response":"size:43"}
{"level":"info","ts":"2024-01-24T20:00:33.893Z","caller":"traceutil/trace.go:171","msg":"trace[1193839012] transaction","detail":"{read_only:false; response_revision:120844; number_of_response:1; }","duration":"105.181997ms","start":"2024-01-24T20:00:33.788Z","end":"2024-01-24T20:00:33.893Z","steps":["trace[1193839012] 'process raft request'  (duration: 104.891046ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-24T20:00:37.985Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"103.920451ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-01-24T20:00:37.985Z","caller":"traceutil/trace.go:171","msg":"trace[45559751] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:120851; }","duration":"104.103328ms","start":"2024-01-24T20:00:37.881Z","end":"2024-01-24T20:00:37.985Z","steps":["trace[45559751] 'range keys from in-memory index tree'  (duration: 103.771219ms)"],"step_count":1}
{"level":"info","ts":"2024-01-24T20:04:49.254Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":120792}
{"level":"info","ts":"2024-01-24T20:04:49.259Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":120792,"took":"5.026579ms"}
{"level":"info","ts":"2024-01-24T20:09:49.281Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":121088}
{"level":"info","ts":"2024-01-24T20:09:49.282Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":121088,"took":"1.055175ms"}
{"level":"info","ts":"2024-01-24T20:11:28.532Z","caller":"traceutil/trace.go:171","msg":"trace[1732793504] linearizableReadLoop","detail":"{readStateIndex:146015; appliedIndex:146015; }","duration":"112.410466ms","start":"2024-01-24T20:11:28.387Z","end":"2024-01-24T20:11:28.500Z","steps":["trace[1732793504] 'read index received'  (duration: 112.39732ms)","trace[1732793504] 'applied index is now lower than readState.Index'  (duration: 11.412µs)"],"step_count":2}
{"level":"warn","ts":"2024-01-24T20:11:28.554Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"134.709044ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/\" range_end:\"/registry/services/endpoints0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2024-01-24T20:11:28.575Z","caller":"traceutil/trace.go:171","msg":"trace[610365274] range","detail":"{range_begin:/registry/services/endpoints/; range_end:/registry/services/endpoints0; response_count:0; response_revision:121449; }","duration":"167.228564ms","start":"2024-01-24T20:11:28.387Z","end":"2024-01-24T20:11:28.554Z","steps":["trace[610365274] 'agreement among raft nodes before linearized reading'  (duration: 112.635356ms)","trace[610365274] 'count revisions from in-memory index tree'  (duration: 22.029941ms)"],"step_count":2}
{"level":"info","ts":"2024-01-24T20:14:49.290Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":121359}
{"level":"info","ts":"2024-01-24T20:14:49.292Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":121359,"took":"1.314597ms"}
{"level":"info","ts":"2024-01-24T20:19:49.320Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":121632}
{"level":"info","ts":"2024-01-24T20:19:49.323Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":121632,"took":"1.711482ms"}
{"level":"info","ts":"2024-01-24T20:24:49.328Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":121906}
{"level":"info","ts":"2024-01-24T20:24:49.330Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":121906,"took":"777.213µs"}
{"level":"warn","ts":"2024-01-24T20:26:10.993Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"332.106082ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128026716190504209 > lease_revoke:<id:70cc8d3bf76b74c2>","response":"size:30"}
{"level":"warn","ts":"2024-01-24T20:26:10.994Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"317.903523ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/leases/ingress-nginx/ingress-nginx-leader\" ","response":"range_response_count:1 size:504"}
{"level":"info","ts":"2024-01-24T20:26:10.993Z","caller":"traceutil/trace.go:171","msg":"trace[457603441] linearizableReadLoop","detail":"{readStateIndex:147050; appliedIndex:147049; }","duration":"317.471723ms","start":"2024-01-24T20:26:10.676Z","end":"2024-01-24T20:26:10.993Z","steps":["trace[457603441] 'read index received'  (duration: 173.010259ms)","trace[457603441] 'applied index is now lower than readState.Index'  (duration: 144.458057ms)"],"step_count":2}
{"level":"info","ts":"2024-01-24T20:26:10.994Z","caller":"traceutil/trace.go:171","msg":"trace[141153667] range","detail":"{range_begin:/registry/leases/ingress-nginx/ingress-nginx-leader; range_end:; response_count:1; response_revision:122303; }","duration":"318.027837ms","start":"2024-01-24T20:26:10.676Z","end":"2024-01-24T20:26:10.994Z","steps":["trace[141153667] 'agreement among raft nodes before linearized reading'  (duration: 317.796793ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-24T20:26:10.996Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"118.688838ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-01-24T20:26:10.996Z","caller":"traceutil/trace.go:171","msg":"trace[1688170892] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:122303; }","duration":"118.870159ms","start":"2024-01-24T20:26:10.877Z","end":"2024-01-24T20:26:10.996Z","steps":["trace[1688170892] 'agreement among raft nodes before linearized reading'  (duration: 118.611844ms)"],"step_count":1}
{"level":"warn","ts":"2024-01-24T20:26:10.997Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-24T20:26:10.676Z","time spent":"319.4136ms","remote":"127.0.0.1:58850","response type":"/etcdserverpb.KV/Range","request count":0,"request size":53,"response count":1,"response size":528,"request content":"key:\"/registry/leases/ingress-nginx/ingress-nginx-leader\" "}
{"level":"info","ts":"2024-01-24T20:29:49.357Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":122221}
{"level":"info","ts":"2024-01-24T20:29:49.358Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":122221,"took":"908.155µs"}
{"level":"info","ts":"2024-01-24T20:31:19.582Z","caller":"traceutil/trace.go:171","msg":"trace[1436502463] linearizableReadLoop","detail":"{readStateIndex:147432; appliedIndex:147432; }","duration":"284.164206ms","start":"2024-01-24T20:31:19.298Z","end":"2024-01-24T20:31:19.582Z","steps":["trace[1436502463] 'read index received'  (duration: 284.143107ms)","trace[1436502463] 'applied index is now lower than readState.Index'  (duration: 17.332µs)"],"step_count":2}
{"level":"warn","ts":"2024-01-24T20:31:19.922Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"338.556578ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/leases/\" range_end:\"/registry/leases0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"warn","ts":"2024-01-24T20:31:19.930Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"623.152324ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-01-24T20:31:19.933Z","caller":"traceutil/trace.go:171","msg":"trace[885912928] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:122623; }","duration":"632.454469ms","start":"2024-01-24T20:31:19.298Z","end":"2024-01-24T20:31:19.931Z","steps":["trace[885912928] 'agreement among raft nodes before linearized reading'  (duration: 284.532818ms)","trace[885912928] 'range keys from in-memory index tree'  (duration: 338.580844ms)"],"step_count":2}
{"level":"warn","ts":"2024-01-24T20:31:19.933Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-01-24T20:31:19.298Z","time spent":"634.737079ms","remote":"127.0.0.1:43592","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}

* 
* ==> kernel <==
*  19:51:25 up 43 min,  0 users,  load average: 0.37, 0.37, 0.36
Linux minikube 5.15.90.1-microsoft-standard-WSL2 #1 SMP Fri Jan 27 02:56:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 20.04.2 LTS"

* 
* ==> kube-apiserver [ba2861f29cef] <==
* I0124 14:54:52.526379       1 shared_informer.go:247] Caches are synced for cluster_authentication_trust_controller 
I0124 14:54:52.616117       1 apf_controller.go:317] Running API Priority and Fairness config worker
I0124 14:54:52.527460       1 cache.go:39] Caches are synced for autoregister controller
I0124 14:54:52.615969       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0124 14:54:52.616777       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0124 14:54:52.620142       1 controller.go:611] quota admission added evaluator for: leases.coordination.k8s.io
I0124 14:54:52.626925       1 shared_informer.go:247] Caches are synced for node_authorizer 
E0124 14:54:52.736567       1 controller.go:152] Unable to remove old endpoints from kubernetes service: no master IPs were listed in storage, refusing to erase all endpoints for the kubernetes service
I0124 14:54:53.414888       1 controller.go:132] OpenAPI AggregationController: action for item : Nothing (removed from the queue).
I0124 14:54:53.414948       1 controller.go:132] OpenAPI AggregationController: action for item k8s_internal_local_delegation_chain_0000000000: Nothing (removed from the queue).
I0124 14:54:53.426342       1 storage_scheduling.go:148] all system priority classes are created successfully or already exist.
I0124 14:54:57.723943       1 controller.go:611] quota admission added evaluator for: daemonsets.apps
I0124 14:54:59.029787       1 controller.go:611] quota admission added evaluator for: serviceaccounts
I0124 14:54:59.057009       1 controller.go:611] quota admission added evaluator for: deployments.apps
I0124 14:54:59.267990       1 controller.go:611] quota admission added evaluator for: events.events.k8s.io
I0124 14:54:59.330067       1 controller.go:611] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0124 14:54:59.346052       1 controller.go:611] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0124 14:55:06.934150       1 controller.go:611] quota admission added evaluator for: endpoints
I0124 14:55:07.020397       1 controller.go:611] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0124 14:55:07.020397       1 controller.go:611] quota admission added evaluator for: endpointslices.discovery.k8s.io
W0124 15:20:46.601076       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
W0124 15:39:16.736562       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
W0124 15:45:39.063257       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
I0124 15:46:00.284409       1 trace.go:205] Trace[1139059715]: "Get" url:/api/v1/namespaces/ingress-nginx/configmaps/ingress-controller-leader,user-agent:nginx-ingress-controller/v1.0.4 (linux/amd64) ingress-nginx/9b78b6c197b48116243922170875af4aa752ee59,audit-id:10097d07-ae93-4f2a-beb7-8e2abd49e247,client:172.17.0.2,accept:application/json, */*,protocol:HTTP/2.0 (24-Jan-2024 15:45:59.729) (total time: 550ms):
Trace[1139059715]: ---"About to write a response" 550ms (15:46:00.279)
Trace[1139059715]: [550.505926ms] [550.505926ms] END
I0124 16:15:11.461843       1 rest.go:379] Transition to LoadBalancer type service with ExternalTrafficPolicy=Local
I0124 16:15:11.709919       1 controller.go:611] quota admission added evaluator for: replicasets.apps
W0124 16:25:56.368791       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
I0124 16:30:36.370063       1 controller.go:611] quota admission added evaluator for: namespaces
I0124 16:30:36.914567       1 controller.go:611] quota admission added evaluator for: jobs.batch
I0124 17:09:49.017160       1 trace.go:205] Trace[1837367433]: "List etcd3" key:/minions,resourceVersion:,resourceVersionMatch:,limit:0,continue: (24-Jan-2024 17:09:48.022) (total time: 994ms):
Trace[1837367433]: [994.139373ms] [994.139373ms] END
I0124 17:09:49.016186       1 trace.go:205] Trace[1552064565]: "Get" url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,audit-id:dee70d22-a25a-4e76-b70a-3d16aca8da3f,client:192.168.49.2,accept:application/json, */*,protocol:HTTP/2.0 (24-Jan-2024 17:09:47.991) (total time: 949ms):
Trace[1552064565]: ---"About to write a response" 949ms (17:09:48.940)
Trace[1552064565]: [949.621129ms] [949.621129ms] END
I0124 17:09:49.006754       1 trace.go:205] Trace[499342872]: "List etcd3" key:/minions,resourceVersion:,resourceVersionMatch:,limit:0,continue: (24-Jan-2024 17:09:48.022) (total time: 927ms):
Trace[499342872]: [927.583977ms] [927.583977ms] END
I0124 17:09:49.046707       1 trace.go:205] Trace[1150028079]: "List" url:/api/v1/nodes,user-agent:kindnetd/v0.0.0 (linux/amd64) kubernetes/$Format,audit-id:8531ca7f-5684-497d-9365-c09690a59144,client:192.168.49.3,accept:application/json, */*,protocol:HTTP/2.0 (24-Jan-2024 17:09:48.022) (total time: 1023ms):
Trace[1150028079]: ---"Listing from storage done" 1023ms (17:09:49.046)
Trace[1150028079]: [1.023705947s] [1.023705947s] END
I0124 17:09:49.046707       1 trace.go:205] Trace[1951819980]: "List" url:/api/v1/nodes,user-agent:kindnetd/v0.0.0 (linux/amd64) kubernetes/$Format,audit-id:873636fe-0663-42b2-a5a6-841cfe2cdc7c,client:192.168.49.2,accept:application/json, */*,protocol:HTTP/2.0 (24-Jan-2024 17:09:48.022) (total time: 1023ms):
Trace[1951819980]: ---"Listing from storage done" 1022ms (17:09:49.045)
Trace[1951819980]: [1.023665415s] [1.023665415s] END
I0124 17:09:49.306922       1 trace.go:205] Trace[1876362464]: "GuaranteedUpdate etcd3" type:*coordination.Lease (24-Jan-2024 17:09:48.198) (total time: 1107ms):
Trace[1876362464]: ---"Transaction committed" 1107ms (17:09:49.306)
Trace[1876362464]: [1.107908304s] [1.107908304s] END
I0124 17:09:49.307190       1 trace.go:205] Trace[548999084]: "Update" url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.22.3 (linux/amd64) kubernetes/c920368,audit-id:e89f0427-be81-4001-afbe-4a8dbbf35c0b,client:192.168.49.2,accept:application/vnd.kubernetes.protobuf,application/json,protocol:HTTP/2.0 (24-Jan-2024 17:09:48.198) (total time: 1108ms):
Trace[548999084]: ---"Object stored in database" 1108ms (17:09:49.307)
Trace[548999084]: [1.108393497s] [1.108393497s] END
W0124 18:16:39.127528       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
I0124 18:18:05.946465       1 trace.go:205] Trace[1844712793]: "GuaranteedUpdate etcd3" type:*v1.Endpoints (24-Jan-2024 18:18:04.982) (total time: 910ms):
Trace[1844712793]: ---"Transaction committed" 908ms (18:18:05.893)
Trace[1844712793]: [910.649806ms] [910.649806ms] END
W0124 18:30:02.157497       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
W0124 19:04:59.643372       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
W0124 19:33:15.805183       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
W0124 19:49:52.091637       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
W0124 20:16:22.976574       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
W0124 20:30:44.355100       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted

* 
* ==> kube-apiserver [c2844a63dfc4] <==
* W0125 19:13:18.546973       1 genericapiserver.go:455] Skipping API flowcontrol.apiserver.k8s.io/v1alpha1 because it has no resources.
W0125 19:13:18.566039       1 genericapiserver.go:455] Skipping API apps/v1beta2 because it has no resources.
W0125 19:13:18.566099       1 genericapiserver.go:455] Skipping API apps/v1beta1 because it has no resources.
W0125 19:13:18.575825       1 genericapiserver.go:455] Skipping API admissionregistration.k8s.io/v1beta1 because it has no resources.
I0125 19:13:18.587029       1 plugins.go:158] Loaded 12 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,ServiceAccount,NodeRestriction,TaintNodesByCondition,Priority,DefaultTolerationSeconds,DefaultStorageClass,StorageObjectInUseProtection,RuntimeClass,DefaultIngressClass,MutatingAdmissionWebhook.
I0125 19:13:18.587088       1 plugins.go:161] Loaded 11 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,PodSecurity,Priority,PersistentVolumeClaimResize,RuntimeClass,CertificateApproval,CertificateSigning,CertificateSubjectRestriction,ValidatingAdmissionWebhook,ResourceQuota.
W0125 19:13:18.680889       1 genericapiserver.go:455] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I0125 19:13:22.141111       1 dynamic_cafile_content.go:155] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0125 19:13:22.141118       1 dynamic_cafile_content.go:155] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0125 19:13:22.141304       1 dynamic_serving_content.go:129] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I0125 19:13:22.142437       1 secure_serving.go:266] Serving securely on [::]:8443
I0125 19:13:22.142577       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0125 19:13:22.142635       1 dynamic_serving_content.go:129] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0125 19:13:22.143002       1 autoregister_controller.go:141] Starting autoregister controller
I0125 19:13:22.143290       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0125 19:13:22.143809       1 controller.go:83] Starting OpenAPI AggregationController
I0125 19:13:22.144163       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I0125 19:13:22.144209       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0125 19:13:22.144228       1 shared_informer.go:240] Waiting for caches to sync for cluster_authentication_trust_controller
I0125 19:13:22.144233       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0125 19:13:22.144301       1 dynamic_cafile_content.go:155] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0125 19:13:22.144376       1 available_controller.go:491] Starting AvailableConditionController
I0125 19:13:22.144398       1 dynamic_cafile_content.go:155] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0125 19:13:22.144401       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0125 19:13:22.145135       1 customresource_discovery_controller.go:209] Starting DiscoveryController
I0125 19:13:22.145312       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0125 19:13:22.145383       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0125 19:13:22.145610       1 crd_finalizer.go:266] Starting CRDFinalizer
I0125 19:13:22.145705       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I0125 19:13:22.145722       1 shared_informer.go:240] Waiting for caches to sync for crd-autoregister
I0125 19:13:22.146435       1 establishing_controller.go:76] Starting EstablishingController
I0125 19:13:22.146559       1 naming_controller.go:291] Starting NamingConditionController
I0125 19:13:22.147796       1 controller.go:85] Starting OpenAPI controller
I0125 19:13:22.147993       1 apf_controller.go:312] Starting API Priority and Fairness config controller
I0125 19:13:22.246631       1 shared_informer.go:247] Caches are synced for crd-autoregister 
E0125 19:13:22.338538       1 controller.go:152] Unable to remove old endpoints from kubernetes service: no master IPs were listed in storage, refusing to erase all endpoints for the kubernetes service
I0125 19:13:22.343838       1 controller.go:611] quota admission added evaluator for: leases.coordination.k8s.io
I0125 19:13:22.344502       1 shared_informer.go:247] Caches are synced for cluster_authentication_trust_controller 
I0125 19:13:22.443691       1 cache.go:39] Caches are synced for autoregister controller
I0125 19:13:22.444420       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0125 19:13:22.444700       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0125 19:13:22.448310       1 apf_controller.go:317] Running API Priority and Fairness config worker
I0125 19:13:22.457857       1 shared_informer.go:247] Caches are synced for node_authorizer 
I0125 19:13:23.139304       1 controller.go:132] OpenAPI AggregationController: action for item : Nothing (removed from the queue).
I0125 19:13:23.139386       1 controller.go:132] OpenAPI AggregationController: action for item k8s_internal_local_delegation_chain_0000000000: Nothing (removed from the queue).
I0125 19:13:23.156686       1 storage_scheduling.go:148] all system priority classes are created successfully or already exist.
I0125 19:13:30.064827       1 controller.go:611] quota admission added evaluator for: daemonsets.apps
I0125 19:13:31.633858       1 controller.go:611] quota admission added evaluator for: serviceaccounts
I0125 19:13:31.669866       1 controller.go:611] quota admission added evaluator for: deployments.apps
I0125 19:13:32.249415       1 controller.go:611] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0125 19:13:32.360244       1 controller.go:611] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0125 19:13:33.146074       1 controller.go:611] quota admission added evaluator for: events.events.k8s.io
I0125 19:13:38.036677       1 controller.go:611] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0125 19:13:38.335844       1 trace.go:205] Trace[1098509910]: "Get" url:/api/v1/namespaces/kube-system/serviceaccounts/deployment-controller,user-agent:kube-controller-manager/v1.22.3 (linux/amd64) kubernetes/c920368/kube-controller-manager,audit-id:e20d23e0-6dea-4a75-b7bc-83494d621c2c,client:192.168.49.2,accept:application/vnd.kubernetes.protobuf, */*,protocol:HTTP/2.0 (25-Jan-2024 19:13:37.742) (total time: 593ms):
Trace[1098509910]: ---"About to write a response" 593ms (19:13:38.335)
Trace[1098509910]: [593.200855ms] [593.200855ms] END
I0125 19:13:38.531208       1 controller.go:611] quota admission added evaluator for: endpoints
I0125 19:13:41.040529       1 rest.go:387] Transition to non LoadBalancer type service or LoadBalancer type service with ExternalTrafficPolicy=Global
I0125 19:13:41.650498       1 controller.go:611] quota admission added evaluator for: replicasets.apps
W0125 19:33:56.161447       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted

* 
* ==> kube-controller-manager [7103b6bf7518] <==
* I0125 19:13:37.228551       1 shared_informer.go:247] Caches are synced for namespace 
I0125 19:13:37.228734       1 shared_informer.go:247] Caches are synced for ClusterRoleAggregator 
I0125 19:13:37.535487       1 shared_informer.go:247] Caches are synced for endpoint 
I0125 19:13:37.536109       1 shared_informer.go:247] Caches are synced for ephemeral 
I0125 19:13:37.537986       1 shared_informer.go:247] Caches are synced for resource quota 
I0125 19:13:37.539010       1 shared_informer.go:247] Caches are synced for ReplicaSet 
I0125 19:13:37.541558       1 shared_informer.go:240] Waiting for caches to sync for garbage collector
I0125 19:13:37.542621       1 shared_informer.go:247] Caches are synced for attach detach 
I0125 19:13:37.548839       1 shared_informer.go:247] Caches are synced for endpoint_slice 
I0125 19:13:37.548917       1 shared_informer.go:247] Caches are synced for deployment 
I0125 19:13:37.548975       1 shared_informer.go:247] Caches are synced for persistent volume 
I0125 19:13:37.628367       1 shared_informer.go:247] Caches are synced for stateful set 
I0125 19:13:37.628568       1 shared_informer.go:247] Caches are synced for resource quota 
I0125 19:13:37.636148       1 shared_informer.go:247] Caches are synced for job 
I0125 19:13:37.638991       1 shared_informer.go:247] Caches are synced for HPA 
I0125 19:13:37.729331       1 shared_informer.go:247] Caches are synced for disruption 
I0125 19:13:37.729387       1 disruption.go:371] Sending events to api server.
I0125 19:13:37.729653       1 shared_informer.go:247] Caches are synced for GC 
I0125 19:13:37.730221       1 shared_informer.go:247] Caches are synced for taint 
I0125 19:13:37.730527       1 node_lifecycle_controller.go:1398] Initializing eviction metric for zone: 
W0125 19:13:37.730896       1 node_lifecycle_controller.go:1013] Missing timestamp for Node minikube. Assuming now as a timestamp.
W0125 19:13:37.731027       1 node_lifecycle_controller.go:1013] Missing timestamp for Node minikube-m02. Assuming now as a timestamp.
W0125 19:13:37.731107       1 node_lifecycle_controller.go:1013] Missing timestamp for Node minikube-m03. Assuming now as a timestamp.
I0125 19:13:37.731112       1 shared_informer.go:247] Caches are synced for ReplicationController 
I0125 19:13:37.732342       1 shared_informer.go:247] Caches are synced for daemon sets 
I0125 19:13:37.732424       1 shared_informer.go:247] Caches are synced for PVC protection 
I0125 19:13:37.732484       1 taint_manager.go:187] "Starting NoExecuteTaintManager"
I0125 19:13:37.737567       1 event.go:291] "Event occurred" object="minikube" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I0125 19:13:37.731287       1 node_lifecycle_controller.go:1214] Controller detected that zone  is now in state Normal.
I0125 19:13:37.737726       1 event.go:291] "Event occurred" object="minikube-m02" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube-m02 event: Registered Node minikube-m02 in Controller"
I0125 19:13:37.737868       1 event.go:291] "Event occurred" object="minikube-m03" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube-m03 event: Registered Node minikube-m03 in Controller"
I0125 19:13:37.842773       1 shared_informer.go:247] Caches are synced for garbage collector 
I0125 19:13:37.928177       1 shared_informer.go:247] Caches are synced for garbage collector 
I0125 19:13:37.928262       1 garbagecollector.go:151] Garbage collector: all resource monitors have synced. Proceeding to collect garbage
W0125 19:13:38.338329       1 endpointslice_controller.go:306] Error syncing endpoint slices for service "kube-system/kube-dns", retrying. Error: EndpointSlice informer cache is out of date
I0125 19:13:41.132084       1 event.go:291] "Event occurred" object="ingress-nginx/ingress-nginx-controller" kind="Service" apiVersion="v1" type="Normal" reason="Type" message="LoadBalancer -> NodePort"
I0125 19:13:41.734344       1 event.go:291] "Event occurred" object="ingress-nginx/ingress-nginx-controller" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ingress-nginx-controller-5f66978484 to 1"
I0125 19:13:41.842569       1 event.go:291] "Event occurred" object="ingress-nginx/ingress-nginx-controller-5f66978484" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-controller-5f66978484-44txb"
I0125 19:14:17.778476       1 event.go:291] "Event occurred" object="minikube-m03" kind="Node" apiVersion="v1" type="Normal" reason="NodeNotReady" message="Node minikube-m03 status is now: NodeNotReady"
I0125 19:14:17.829559       1 event.go:291] "Event occurred" object="kube-system/kindnet-6gvnv" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0125 19:14:17.842089       1 event.go:291] "Event occurred" object="kube-system/kube-proxy-ct2gp" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0125 19:14:17.854160       1 event.go:291] "Event occurred" object="minikube-m02" kind="Node" apiVersion="v1" type="Normal" reason="NodeNotReady" message="Node minikube-m02 status is now: NodeNotReady"
I0125 19:14:17.945564       1 event.go:291] "Event occurred" object="ingress-nginx/ingress-nginx-controller-9d88bfc9f-f2wpf" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0125 19:14:17.957515       1 event.go:291] "Event occurred" object="kube-system/kindnet-llqkw" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0125 19:14:18.040603       1 event.go:291] "Event occurred" object="kube-system/kube-proxy-kxmdl" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
I0125 19:19:23.080512       1 taint_manager.go:106] "NoExecuteTaintManager is deleting pod" pod="ingress-nginx/ingress-nginx-controller-9d88bfc9f-f2wpf"
I0125 19:19:23.080525       1 taint_manager.go:106] "NoExecuteTaintManager is deleting pod" pod="ingress-nginx/ingress-nginx-controller-5f66978484-44txb"
I0125 19:19:23.083336       1 event.go:291] "Event occurred" object="ingress-nginx/ingress-nginx-controller-9d88bfc9f-f2wpf" kind="Pod" apiVersion="" type="Normal" reason="TaintManagerEviction" message="Marking for deletion Pod ingress-nginx/ingress-nginx-controller-9d88bfc9f-f2wpf"
I0125 19:19:23.083403       1 event.go:291] "Event occurred" object="ingress-nginx/ingress-nginx-controller-5f66978484-44txb" kind="Pod" apiVersion="" type="Normal" reason="TaintManagerEviction" message="Marking for deletion Pod ingress-nginx/ingress-nginx-controller-5f66978484-44txb"
I0125 19:19:23.255130       1 event.go:291] "Event occurred" object="ingress-nginx/ingress-nginx-controller-5f66978484" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-controller-5f66978484-xz587"
I0125 19:19:23.260101       1 event.go:291] "Event occurred" object="ingress-nginx/ingress-nginx-controller-9d88bfc9f" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-controller-9d88bfc9f-pp96h"
I0125 19:19:33.165594       1 taint_manager.go:106] "NoExecuteTaintManager is deleting pod" pod="kube-system/kube-ingress-dns-minikube"
I0125 19:19:33.166037       1 event.go:291] "Event occurred" object="kube-system/kube-ingress-dns-minikube" kind="Pod" apiVersion="" type="Normal" reason="TaintManagerEviction" message="Marking for deletion Pod kube-system/kube-ingress-dns-minikube"
I0125 19:19:43.638221       1 event.go:291] "Event occurred" object="ingress-nginx/ingress-nginx-controller" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set ingress-nginx-controller-9d88bfc9f to 0"
I0125 19:19:43.657164       1 event.go:291] "Event occurred" object="ingress-nginx/ingress-nginx-controller-9d88bfc9f" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: ingress-nginx-controller-9d88bfc9f-pp96h"
W0125 19:38:16.698781       1 actual_state_of_world.go:534] Failed to update statusUpdateNeeded field in actual state of world: Failed to set statusUpdateNeeded to needed true, because nodeName="minikube-m02" does not exist
I0125 19:38:16.721497       1 range_allocator.go:373] Set node minikube-m02 PodCIDR to [10.244.1.0/24]
W0125 19:38:32.901419       1 actual_state_of_world.go:534] Failed to update statusUpdateNeeded field in actual state of world: Failed to set statusUpdateNeeded to needed true, because nodeName="minikube-m03" does not exist
I0125 19:38:32.917778       1 range_allocator.go:373] Set node minikube-m03 PodCIDR to [10.244.2.0/24]
I0125 19:38:35.137112       1 event.go:291] "Event occurred" object="kube-system/kube-ingress-dns-minikube" kind="Pod" apiVersion="" type="Normal" reason="TaintManagerEviction" message="Cancelling deletion of Pod kube-system/kube-ingress-dns-minikube"

* 
* ==> kube-controller-manager [df5d1cd5aca3] <==
* I0124 16:41:25.818171       1 event.go:291] "Event occurred" object="ingress-nginx/ingress-nginx-admission-patch" kind="Job" apiVersion="batch/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: ingress-nginx-admission-patch--1-8slhx"
I0124 16:41:25.818233       1 event.go:291] "Event occurred" object="ingress-nginx/ingress-nginx-admission-patch" kind="Job" apiVersion="batch/v1" type="Warning" reason="BackoffLimitExceeded" message="Job has reached the specified backoff limit"
I0124 16:41:25.818581       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0124 16:41:25.830230       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0124 16:41:26.819078       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0124 16:41:27.845935       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0124 16:41:27.852344       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0124 16:41:41.531234       1 event.go:291] "Event occurred" object="ingress-nginx/ingress-nginx-controller-9d88bfc9f" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-controller-9d88bfc9f-55xb4"
I0124 16:46:10.832968       1 event.go:291] "Event occurred" object="ingress-nginx/ingress-nginx-controller-9d88bfc9f" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-controller-9d88bfc9f-wgzgq"
I0124 16:55:30.941685       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0124 16:56:02.928922       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0124 16:56:15.936007       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0124 17:03:12.936676       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0124 17:03:24.926043       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0124 17:04:11.926278       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0124 17:04:26.939027       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0124 17:07:38.181445       1 event.go:291] "Event occurred" object="ingress-nginx/ingress-nginx-controller-9d88bfc9f" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-controller-9d88bfc9f-wvrjl"
I0124 17:09:28.936536       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0124 17:09:41.918855       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0124 17:12:41.934504       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0124 17:12:54.920945       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0124 17:27:19.978086       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0124 17:35:00.922634       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-create
W0124 17:35:52.568425       1 endpointslice_controller.go:306] Error syncing endpoint slices for service "ingress-nginx/ingress-nginx-controller", retrying. Error: EndpointSlice informer cache is out of date
I0124 17:36:26.531658       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0124 17:36:26.578252       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0124 17:36:26.579679       1 event.go:291] "Event occurred" object="ingress-nginx/ingress-nginx-admission-create" kind="Job" apiVersion="batch/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-admission-create--1-dx2l5"
I0124 17:36:26.628728       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0124 17:36:26.673779       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0124 17:36:28.835593       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0124 17:36:29.852463       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0124 17:36:30.862054       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0124 17:36:44.955014       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0124 17:36:59.542194       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0124 17:37:13.118672       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0124 17:37:23.545706       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0124 17:38:01.387665       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0124 17:38:13.580081       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0124 17:39:21.842440       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0124 17:39:22.857188       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0124 17:40:52.807443       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0124 17:40:53.817732       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0124 17:40:53.823255       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0124 17:42:09.823890       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0124 17:42:09.836099       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0124 17:42:09.838101       1 event.go:291] "Event occurred" object="ingress-nginx/ingress-nginx-admission-create" kind="Job" apiVersion="batch/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: ingress-nginx-admission-create--1-dx2l5"
I0124 17:42:09.838171       1 event.go:291] "Event occurred" object="ingress-nginx/ingress-nginx-admission-create" kind="Job" apiVersion="batch/v1" type="Warning" reason="BackoffLimitExceeded" message="Job has reached the specified backoff limit"
I0124 17:42:09.862397       1 event.go:291] "Event occurred" object="ingress-nginx/ingress-nginx-admission-create" kind="Job" apiVersion="batch/v1" type="Warning" reason="BackoffLimitExceeded" message="Job has reached the specified backoff limit"
I0124 17:42:09.862421       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0124 17:42:09.869354       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0124 17:42:10.829035       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0124 17:42:11.838695       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0124 17:42:11.842954       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0124 18:35:10.014739       1 event.go:291] "Event occurred" object="ingress-nginx/ingress-nginx-controller-9d88bfc9f" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-controller-9d88bfc9f-xh9vh"
W0124 18:50:33.079689       1 endpointslice_controller.go:306] Error syncing endpoint slices for service "ingress-nginx/ingress-nginx-controller-admission", retrying. Error: EndpointSlice informer cache is out of date
I0124 18:57:19.723256       1 event.go:291] "Event occurred" object="ingress-nginx/ingress-nginx-controller-9d88bfc9f" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-controller-9d88bfc9f-94mmz"
I0124 19:18:16.374453       1 event.go:291] "Event occurred" object="ingress-nginx/ingress-nginx-controller-9d88bfc9f" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-controller-9d88bfc9f-5hn2n"
W0124 19:41:52.383438       1 endpointslice_controller.go:306] Error syncing endpoint slices for service "ingress-nginx/ingress-nginx-controller-admission", retrying. Error: EndpointSlice informer cache is out of date
W0124 19:41:52.383403       1 endpointslice_controller.go:306] Error syncing endpoint slices for service "ingress-nginx/ingress-nginx-controller", retrying. Error: EndpointSlice informer cache is out of date
I0124 20:00:33.487354       1 event.go:291] "Event occurred" object="ingress-nginx/ingress-nginx-controller-9d88bfc9f" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-controller-9d88bfc9f-f2wpf"

* 
* ==> kube-proxy [2b7a56aea1bd] <==
* E0124 14:54:58.821354       1 proxier.go:649] "Failed to read builtin modules file. You can ignore this message when kube-proxy is running inside container without mounting /lib/modules" err="open /lib/modules/5.15.90.1-microsoft-standard-WSL2/modules.builtin: no such file or directory" filePath="/lib/modules/5.15.90.1-microsoft-standard-WSL2/modules.builtin"
I0124 14:54:58.947779       1 proxier.go:659] "Failed to load kernel module with modprobe. You can ignore this message when kube-proxy is running inside container without mounting /lib/modules" moduleName="ip_vs"
I0124 14:54:58.950236       1 proxier.go:659] "Failed to load kernel module with modprobe. You can ignore this message when kube-proxy is running inside container without mounting /lib/modules" moduleName="ip_vs_rr"
I0124 14:54:58.952300       1 proxier.go:659] "Failed to load kernel module with modprobe. You can ignore this message when kube-proxy is running inside container without mounting /lib/modules" moduleName="ip_vs_wrr"
I0124 14:54:59.014821       1 proxier.go:659] "Failed to load kernel module with modprobe. You can ignore this message when kube-proxy is running inside container without mounting /lib/modules" moduleName="ip_vs_sh"
I0124 14:54:59.017709       1 proxier.go:659] "Failed to load kernel module with modprobe. You can ignore this message when kube-proxy is running inside container without mounting /lib/modules" moduleName="nf_conntrack"
I0124 14:54:59.123364       1 node.go:172] Successfully retrieved node IP: 192.168.49.2
I0124 14:54:59.123474       1 server_others.go:140] Detected node IP 192.168.49.2
W0124 14:54:59.123602       1 server_others.go:565] Unknown proxy mode "", assuming iptables proxy
I0124 14:54:59.257594       1 server_others.go:206] kube-proxy running in dual-stack mode, IPv4-primary
I0124 14:54:59.257660       1 server_others.go:212] Using iptables Proxier.
I0124 14:54:59.257698       1 server_others.go:219] creating dualStackProxier for iptables.
W0124 14:54:59.257717       1 server_others.go:495] detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, , defaulting to no-op detect-local for IPv6
I0124 14:54:59.259972       1 server.go:649] Version: v1.22.3
I0124 14:54:59.263309       1 config.go:224] Starting endpoint slice config controller
I0124 14:54:59.263565       1 shared_informer.go:240] Waiting for caches to sync for endpoint slice config
I0124 14:54:59.263917       1 config.go:315] Starting service config controller
I0124 14:54:59.263998       1 shared_informer.go:240] Waiting for caches to sync for service config
I0124 14:54:59.363975       1 shared_informer.go:247] Caches are synced for endpoint slice config 
I0124 14:54:59.364573       1 shared_informer.go:247] Caches are synced for service config 

* 
* ==> kube-proxy [99187b085a19] <==
* E0125 19:13:32.041303       1 proxier.go:649] "Failed to read builtin modules file. You can ignore this message when kube-proxy is running inside container without mounting /lib/modules" err="open /lib/modules/5.15.90.1-microsoft-standard-WSL2/modules.builtin: no such file or directory" filePath="/lib/modules/5.15.90.1-microsoft-standard-WSL2/modules.builtin"
I0125 19:13:32.338344       1 proxier.go:659] "Failed to load kernel module with modprobe. You can ignore this message when kube-proxy is running inside container without mounting /lib/modules" moduleName="ip_vs"
I0125 19:13:32.348157       1 proxier.go:659] "Failed to load kernel module with modprobe. You can ignore this message when kube-proxy is running inside container without mounting /lib/modules" moduleName="ip_vs_rr"
I0125 19:13:32.356028       1 proxier.go:659] "Failed to load kernel module with modprobe. You can ignore this message when kube-proxy is running inside container without mounting /lib/modules" moduleName="ip_vs_wrr"
I0125 19:13:32.435828       1 proxier.go:659] "Failed to load kernel module with modprobe. You can ignore this message when kube-proxy is running inside container without mounting /lib/modules" moduleName="ip_vs_sh"
I0125 19:13:32.444547       1 proxier.go:659] "Failed to load kernel module with modprobe. You can ignore this message when kube-proxy is running inside container without mounting /lib/modules" moduleName="nf_conntrack"
I0125 19:13:32.647051       1 node.go:172] Successfully retrieved node IP: 192.168.49.2
I0125 19:13:32.647218       1 server_others.go:140] Detected node IP 192.168.49.2
W0125 19:13:32.647301       1 server_others.go:565] Unknown proxy mode "", assuming iptables proxy
I0125 19:13:32.966486       1 server_others.go:206] kube-proxy running in dual-stack mode, IPv4-primary
I0125 19:13:32.966658       1 server_others.go:212] Using iptables Proxier.
I0125 19:13:32.966677       1 server_others.go:219] creating dualStackProxier for iptables.
W0125 19:13:32.967370       1 server_others.go:495] detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, , defaulting to no-op detect-local for IPv6
I0125 19:13:32.971627       1 server.go:649] Version: v1.22.3
I0125 19:13:33.031861       1 config.go:224] Starting endpoint slice config controller
I0125 19:13:33.034167       1 shared_informer.go:240] Waiting for caches to sync for endpoint slice config
I0125 19:13:33.034205       1 config.go:315] Starting service config controller
I0125 19:13:33.034725       1 shared_informer.go:240] Waiting for caches to sync for service config
I0125 19:13:33.142415       1 shared_informer.go:247] Caches are synced for endpoint slice config 
I0125 19:13:33.235632       1 shared_informer.go:247] Caches are synced for service config 

* 
* ==> kube-scheduler [14df4ac85e95] <==
* I0125 19:13:14.438512       1 serving.go:347] Generated self-signed cert in-memory
W0125 19:13:22.445339       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0125 19:13:22.447202       1 authentication.go:345] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0125 19:13:22.447286       1 authentication.go:346] Continuing without authentication configuration. This may treat all requests as anonymous.
W0125 19:13:22.447300       1 authentication.go:347] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0125 19:13:22.655151       1 configmap_cafile_content.go:201] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0125 19:13:22.655471       1 shared_informer.go:240] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0125 19:13:22.655680       1 secure_serving.go:200] Serving securely on 127.0.0.1:10259
I0125 19:13:22.655887       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0125 19:13:22.856640       1 shared_informer.go:247] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file 

* 
* ==> kube-scheduler [58d871a33e1c] <==
* I0124 14:54:45.350521       1 serving.go:347] Generated self-signed cert in-memory
W0124 14:54:52.625257       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0124 14:54:52.625336       1 authentication.go:345] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system": RBAC: [clusterrole.rbac.authorization.k8s.io "system:basic-user" not found, clusterrole.rbac.authorization.k8s.io "system:volume-scheduler" not found]
W0124 14:54:52.625359       1 authentication.go:346] Continuing without authentication configuration. This may treat all requests as anonymous.
W0124 14:54:52.625370       1 authentication.go:347] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0124 14:54:52.931856       1 configmap_cafile_content.go:201] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0124 14:54:52.932046       1 shared_informer.go:240] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0124 14:54:52.932567       1 secure_serving.go:200] Serving securely on 127.0.0.1:10259
I0124 14:54:52.933273       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0124 14:54:53.116186       1 shared_informer.go:247] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file 
I0124 16:15:12.245477       1 trace.go:205] Trace[1130741048]: "Scheduling" namespace:ingress-nginx,name:ingress-nginx-controller-9d88bfc9f-pj2xz (24-Jan-2024 16:15:11.917) (total time: 321ms):
Trace[1130741048]: ---"Computing predicates done" 197ms (16:15:12.118)
Trace[1130741048]: ---"Prioritizing done" 120ms (16:15:12.238)
Trace[1130741048]: [321.583913ms] [321.583913ms] END
I0124 16:30:37.215576       1 trace.go:205] Trace[1262092148]: "Scheduling" namespace:ingress-nginx,name:ingress-nginx-admission-create--1-rgcxj (24-Jan-2024 16:30:37.030) (total time: 177ms):
Trace[1262092148]: ---"Computing predicates done" 89ms (16:30:37.120)
Trace[1262092148]: ---"Prioritizing done" 87ms (16:30:37.208)
Trace[1262092148]: [177.637725ms] [177.637725ms] END
I0124 20:00:33.598643       1 trace.go:205] Trace[1157507198]: "Scheduling" namespace:ingress-nginx,name:ingress-nginx-controller-9d88bfc9f-f2wpf (24-Jan-2024 20:00:33.491) (total time: 102ms):
Trace[1157507198]: ---"Prioritizing done" 83ms (20:00:33.594)
Trace[1157507198]: [102.716293ms] [102.716293ms] END

* 
* ==> kubelet <==
* -- Logs begin at Thu 2024-01-25 19:12:42 UTC, end at Thu 2024-01-25 19:51:26 UTC. --
Jan 25 19:13:25 minikube kubelet[1192]: I0125 19:13:25.136019    1192 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="de5d9f87edf56bf8207fdb586dd3771ca6539d816a813d262466877e00751dde"
Jan 25 19:13:25 minikube kubelet[1192]: I0125 19:13:25.730316    1192 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="ecdce05d51da02ca8d51ed5fe21e6424ee6ba7aa61c0f87892d2eaaa35e926ca"
Jan 25 19:13:29 minikube kubelet[1192]: I0125 19:13:29.543722    1192 docker_sandbox.go:401] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for kube-system/coredns-78fcd69978-fxjn2 through plugin: invalid network status for"
Jan 25 19:13:29 minikube kubelet[1192]: I0125 19:13:29.632657    1192 docker_sandbox.go:401] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/frontend-699c74878d-mw9tc through plugin: invalid network status for"
Jan 25 19:13:29 minikube kubelet[1192]: I0125 19:13:29.653007    1192 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="66fa78da355f128674b78aadf4d8f55d006647022ff6e463273fe719e74adb14"
Jan 25 19:13:29 minikube kubelet[1192]: I0125 19:13:29.659661    1192 docker_sandbox.go:401] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for kube-system/coredns-78fcd69978-fxjn2 through plugin: invalid network status for"
Jan 25 19:13:29 minikube kubelet[1192]: I0125 19:13:29.738719    1192 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="a9dfd38a2f0f7be50d310204dce859c3cff2f4056f33edc07f29dba256d8b4db"
Jan 25 19:13:30 minikube kubelet[1192]: I0125 19:13:30.464245    1192 docker_sandbox.go:401] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/postgres-deployment-674cfbd48-xmjvk through plugin: invalid network status for"
Jan 25 19:13:30 minikube kubelet[1192]: I0125 19:13:30.633791    1192 docker_sandbox.go:401] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/node-back-end-7bb94dffbc-7vl5t through plugin: invalid network status for"
Jan 25 19:13:30 minikube kubelet[1192]: E0125 19:13:30.640377    1192 summary_sys_containers.go:47] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Jan 25 19:13:30 minikube kubelet[1192]: E0125 19:13:30.640900    1192 helpers.go:673] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal=allocatableMemory.available
Jan 25 19:13:31 minikube kubelet[1192]: I0125 19:13:31.141305    1192 docker_sandbox.go:401] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/frontend-699c74878d-mw9tc through plugin: invalid network status for"
Jan 25 19:13:31 minikube kubelet[1192]: I0125 19:13:31.178682    1192 docker_sandbox.go:401] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/postgres-deployment-674cfbd48-xmjvk through plugin: invalid network status for"
Jan 25 19:13:31 minikube kubelet[1192]: I0125 19:13:31.249415    1192 docker_sandbox.go:401] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for kube-system/coredns-78fcd69978-fxjn2 through plugin: invalid network status for"
Jan 25 19:13:31 minikube kubelet[1192]: I0125 19:13:31.340880    1192 docker_sandbox.go:401] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/node-back-end-7bb94dffbc-7vl5t through plugin: invalid network status for"
Jan 25 19:13:33 minikube kubelet[1192]: I0125 19:13:33.137383    1192 docker_sandbox.go:401] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for kube-system/coredns-78fcd69978-fxjn2 through plugin: invalid network status for"
Jan 25 19:13:33 minikube kubelet[1192]: I0125 19:13:33.632086    1192 docker_sandbox.go:401] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/node-back-end-7bb94dffbc-7vl5t through plugin: invalid network status for"
Jan 25 19:13:33 minikube kubelet[1192]: I0125 19:13:33.733671    1192 docker_sandbox.go:401] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/frontend-699c74878d-mw9tc through plugin: invalid network status for"
Jan 25 19:13:40 minikube kubelet[1192]: E0125 19:13:40.936305    1192 summary_sys_containers.go:47] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Jan 25 19:13:40 minikube kubelet[1192]: E0125 19:13:40.937627    1192 helpers.go:673] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal=allocatableMemory.available
Jan 25 19:13:41 minikube kubelet[1192]: I0125 19:13:41.640086    1192 docker_sandbox.go:401] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/postgres-deployment-674cfbd48-xmjvk through plugin: invalid network status for"
Jan 25 19:13:42 minikube kubelet[1192]: I0125 19:13:42.861158    1192 docker_sandbox.go:401] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/postgres-deployment-674cfbd48-xmjvk through plugin: invalid network status for"
Jan 25 19:13:50 minikube kubelet[1192]: E0125 19:13:50.967540    1192 summary_sys_containers.go:47] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Jan 25 19:13:50 minikube kubelet[1192]: E0125 19:13:50.967639    1192 helpers.go:673] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal=allocatableMemory.available
Jan 25 19:14:00 minikube kubelet[1192]: I0125 19:14:00.379080    1192 scope.go:110] "RemoveContainer" containerID="8ddf41ebeb6224d702ac682376e89a8d98ab6d4a2bc1b0930c2c93fb66087ad6"
Jan 25 19:14:00 minikube kubelet[1192]: I0125 19:14:00.379709    1192 scope.go:110] "RemoveContainer" containerID="da84a87eff8d36c3c5fede5f7d6ac05fe6bd0c6d89eaf55c6c55d60a610d325f"
Jan 25 19:14:00 minikube kubelet[1192]: E0125 19:14:00.380361    1192 pod_workers.go:836] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 10s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(6b688e41-602a-4aef-bc66-f3d564a9db43)\"" pod="kube-system/storage-provisioner" podUID=6b688e41-602a-4aef-bc66-f3d564a9db43
Jan 25 19:14:00 minikube kubelet[1192]: E0125 19:14:00.996624    1192 summary_sys_containers.go:47] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Jan 25 19:14:00 minikube kubelet[1192]: E0125 19:14:00.996762    1192 helpers.go:673] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal=allocatableMemory.available
Jan 25 19:14:12 minikube kubelet[1192]: I0125 19:14:12.266530    1192 scope.go:110] "RemoveContainer" containerID="da84a87eff8d36c3c5fede5f7d6ac05fe6bd0c6d89eaf55c6c55d60a610d325f"
Jan 25 19:18:07 minikube kubelet[1192]: W0125 19:18:07.392937    1192 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jan 25 19:19:23 minikube kubelet[1192]: I0125 19:19:23.530757    1192 topology_manager.go:200] "Topology Admit Handler"
Jan 25 19:19:23 minikube kubelet[1192]: I0125 19:19:23.539524    1192 topology_manager.go:200] "Topology Admit Handler"
Jan 25 19:19:23 minikube kubelet[1192]: I0125 19:19:23.639194    1192 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"webhook-cert\" (UniqueName: \"kubernetes.io/secret/85fde7bc-25c1-4258-9c1c-2cb6d5a77713-webhook-cert\") pod \"ingress-nginx-controller-5f66978484-xz587\" (UID: \"85fde7bc-25c1-4258-9c1c-2cb6d5a77713\") "
Jan 25 19:19:23 minikube kubelet[1192]: I0125 19:19:23.639456    1192 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-2qsgj\" (UniqueName: \"kubernetes.io/projected/85fde7bc-25c1-4258-9c1c-2cb6d5a77713-kube-api-access-2qsgj\") pod \"ingress-nginx-controller-5f66978484-xz587\" (UID: \"85fde7bc-25c1-4258-9c1c-2cb6d5a77713\") "
Jan 25 19:19:23 minikube kubelet[1192]: I0125 19:19:23.641069    1192 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"webhook-cert\" (UniqueName: \"kubernetes.io/secret/93d9cc10-1392-4e9d-85f0-96bb6c5bab6d-webhook-cert\") pod \"ingress-nginx-controller-9d88bfc9f-pp96h\" (UID: \"93d9cc10-1392-4e9d-85f0-96bb6c5bab6d\") "
Jan 25 19:19:23 minikube kubelet[1192]: I0125 19:19:23.641137    1192 reconciler.go:224] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-mxspl\" (UniqueName: \"kubernetes.io/projected/93d9cc10-1392-4e9d-85f0-96bb6c5bab6d-kube-api-access-mxspl\") pod \"ingress-nginx-controller-9d88bfc9f-pp96h\" (UID: \"93d9cc10-1392-4e9d-85f0-96bb6c5bab6d\") "
Jan 25 19:19:26 minikube kubelet[1192]: I0125 19:19:26.670849    1192 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="5438ce47a26e519511831415fa2cfea369933de976d7efca11e50d2f7c36b8a7"
Jan 25 19:19:26 minikube kubelet[1192]: I0125 19:19:26.671262    1192 docker_sandbox.go:401] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for ingress-nginx/ingress-nginx-controller-5f66978484-xz587 through plugin: invalid network status for"
Jan 25 19:19:26 minikube kubelet[1192]: I0125 19:19:26.762691    1192 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="abd143c06dd2707c2c5557ff302b67d00eff5f2b909870b0ee93e99b79cf1896"
Jan 25 19:19:26 minikube kubelet[1192]: I0125 19:19:26.762691    1192 docker_sandbox.go:401] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for ingress-nginx/ingress-nginx-controller-9d88bfc9f-pp96h through plugin: invalid network status for"
Jan 25 19:19:27 minikube kubelet[1192]: I0125 19:19:27.781886    1192 docker_sandbox.go:401] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for ingress-nginx/ingress-nginx-controller-5f66978484-xz587 through plugin: invalid network status for"
Jan 25 19:19:27 minikube kubelet[1192]: I0125 19:19:27.792972    1192 docker_sandbox.go:401] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for ingress-nginx/ingress-nginx-controller-9d88bfc9f-pp96h through plugin: invalid network status for"
Jan 25 19:23:07 minikube kubelet[1192]: W0125 19:23:07.355211    1192 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jan 25 19:28:07 minikube kubelet[1192]: W0125 19:28:07.350072    1192 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jan 25 19:33:07 minikube kubelet[1192]: W0125 19:33:07.351104    1192 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jan 25 19:38:07 minikube kubelet[1192]: W0125 19:38:07.357456    1192 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jan 25 19:43:07 minikube kubelet[1192]: W0125 19:43:07.373270    1192 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jan 25 19:47:45 minikube kubelet[1192]: E0125 19:47:45.925709    1192 remote_image.go:114] "PullImage from image service failed" err="rpc error: code = Unknown desc = Get \"https://prod-registry-k8s-io-eu-central-1.s3.dualstack.eu-central-1.amazonaws.com/containers/images/sha256:1b862f95e61b4263ab1a25f4c07b91fcda0f5d158adb062bfb5c55179abf33f2\": net/http: TLS handshake timeout" image="registry.k8s.io/ingress-nginx/controller:v1.9.5@sha256:b3aba22b1da80e7acfc52b115cae1d4c687172cbf2b742d5b502419c25ff340e"
Jan 25 19:47:45 minikube kubelet[1192]: E0125 19:47:45.925874    1192 kuberuntime_image.go:51] "Failed to pull image" err="rpc error: code = Unknown desc = Get \"https://prod-registry-k8s-io-eu-central-1.s3.dualstack.eu-central-1.amazonaws.com/containers/images/sha256:1b862f95e61b4263ab1a25f4c07b91fcda0f5d158adb062bfb5c55179abf33f2\": net/http: TLS handshake timeout" image="registry.k8s.io/ingress-nginx/controller:v1.9.5@sha256:b3aba22b1da80e7acfc52b115cae1d4c687172cbf2b742d5b502419c25ff340e"
Jan 25 19:47:45 minikube kubelet[1192]: E0125 19:47:45.931884    1192 kuberuntime_manager.go:898] container &Container{Name:controller,Image:registry.k8s.io/ingress-nginx/controller:v1.9.5@sha256:b3aba22b1da80e7acfc52b115cae1d4c687172cbf2b742d5b502419c25ff340e,Command:[],Args:[/nginx-ingress-controller --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller --election-id=ingress-nginx-leader --controller-class=k8s.io/ingress-nginx --ingress-class=nginx --configmap=$(POD_NAMESPACE)/ingress-nginx-controller --validating-webhook=:8443 --validating-webhook-certificate=/usr/local/certificates/cert --validating-webhook-key=/usr/local/certificates/key],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:http,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},ContainerPort{Name:https,HostPort:0,ContainerPort:443,Protocol:TCP,HostIP:,},ContainerPort{Name:webhook,HostPort:0,ContainerPort:8443,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:POD_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.name,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:POD_NAMESPACE,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:LD_PRELOAD,Value:/usr/local/lib/libmimalloc.so,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{cpu: {{100 -3} {<nil>} 100m DecimalSI},memory: {{94371840 0} {<nil>} 90Mi BinarySI},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:webhook-cert,ReadOnly:true,MountPath:/usr/local/certificates/,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-mxspl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthz,Port:{0 10254 },Host:,Scheme:HTTP,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,},InitialDelaySeconds:10,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,TerminationGracePeriodSeconds:nil,},ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthz,Port:{0 10254 },Host:,Scheme:HTTP,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,},InitialDelaySeconds:10,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,TerminationGracePeriodSeconds:nil,},Lifecycle:&Lifecycle{PostStart:nil,PreStop:&Handler{Exec:&ExecAction{Command:[/wait-shutdown],},HTTPGet:nil,TCPSocket:nil,},},TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[NET_BIND_SERVICE],Drop:[ALL],},Privileged:nil,SELinuxOptions:nil,RunAsUser:*101,RunAsNonRoot:*true,ReadOnlyRootFilesystem:*false,AllowPrivilegeEscalation:*false,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:&SeccompProfile{Type:RuntimeDefault,LocalhostProfile:nil,},},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,} start failed in pod ingress-nginx-controller-9d88bfc9f-pp96h_ingress-nginx(93d9cc10-1392-4e9d-85f0-96bb6c5bab6d): ErrImagePull: rpc error: code = Unknown desc = Get "https://prod-registry-k8s-io-eu-central-1.s3.dualstack.eu-central-1.amazonaws.com/containers/images/sha256:1b862f95e61b4263ab1a25f4c07b91fcda0f5d158adb062bfb5c55179abf33f2": net/http: TLS handshake timeout
Jan 25 19:47:45 minikube kubelet[1192]: E0125 19:47:45.935261    1192 pod_workers.go:836] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"controller\" with ErrImagePull: \"rpc error: code = Unknown desc = Get \\\"https://prod-registry-k8s-io-eu-central-1.s3.dualstack.eu-central-1.amazonaws.com/containers/images/sha256:1b862f95e61b4263ab1a25f4c07b91fcda0f5d158adb062bfb5c55179abf33f2\\\": net/http: TLS handshake timeout\"" pod="ingress-nginx/ingress-nginx-controller-9d88bfc9f-pp96h" podUID=93d9cc10-1392-4e9d-85f0-96bb6c5bab6d
Jan 25 19:47:47 minikube kubelet[1192]: I0125 19:47:47.526116    1192 reconciler.go:196] "operationExecutor.UnmountVolume started for volume \"webhook-cert\" (UniqueName: \"kubernetes.io/secret/93d9cc10-1392-4e9d-85f0-96bb6c5bab6d-webhook-cert\") pod \"93d9cc10-1392-4e9d-85f0-96bb6c5bab6d\" (UID: \"93d9cc10-1392-4e9d-85f0-96bb6c5bab6d\") "
Jan 25 19:47:47 minikube kubelet[1192]: I0125 19:47:47.526308    1192 reconciler.go:196] "operationExecutor.UnmountVolume started for volume \"kube-api-access-mxspl\" (UniqueName: \"kubernetes.io/projected/93d9cc10-1392-4e9d-85f0-96bb6c5bab6d-kube-api-access-mxspl\") pod \"93d9cc10-1392-4e9d-85f0-96bb6c5bab6d\" (UID: \"93d9cc10-1392-4e9d-85f0-96bb6c5bab6d\") "
Jan 25 19:47:47 minikube kubelet[1192]: I0125 19:47:47.559176    1192 operation_generator.go:866] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/93d9cc10-1392-4e9d-85f0-96bb6c5bab6d-webhook-cert" (OuterVolumeSpecName: "webhook-cert") pod "93d9cc10-1392-4e9d-85f0-96bb6c5bab6d" (UID: "93d9cc10-1392-4e9d-85f0-96bb6c5bab6d"). InnerVolumeSpecName "webhook-cert". PluginName "kubernetes.io/secret", VolumeGidValue ""
Jan 25 19:47:47 minikube kubelet[1192]: I0125 19:47:47.559181    1192 operation_generator.go:866] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/93d9cc10-1392-4e9d-85f0-96bb6c5bab6d-kube-api-access-mxspl" (OuterVolumeSpecName: "kube-api-access-mxspl") pod "93d9cc10-1392-4e9d-85f0-96bb6c5bab6d" (UID: "93d9cc10-1392-4e9d-85f0-96bb6c5bab6d"). InnerVolumeSpecName "kube-api-access-mxspl". PluginName "kubernetes.io/projected", VolumeGidValue ""
Jan 25 19:47:47 minikube kubelet[1192]: I0125 19:47:47.628403    1192 reconciler.go:319] "Volume detached for volume \"webhook-cert\" (UniqueName: \"kubernetes.io/secret/93d9cc10-1392-4e9d-85f0-96bb6c5bab6d-webhook-cert\") on node \"minikube\" DevicePath \"\""
Jan 25 19:47:47 minikube kubelet[1192]: I0125 19:47:47.628502    1192 reconciler.go:319] "Volume detached for volume \"kube-api-access-mxspl\" (UniqueName: \"kubernetes.io/projected/93d9cc10-1392-4e9d-85f0-96bb6c5bab6d-kube-api-access-mxspl\") on node \"minikube\" DevicePath \"\""
Jan 25 19:47:49 minikube kubelet[1192]: I0125 19:47:49.277463    1192 kubelet_volumes.go:160] "Cleaned up orphaned pod volumes dir" podUID=93d9cc10-1392-4e9d-85f0-96bb6c5bab6d path="/var/lib/kubelet/pods/93d9cc10-1392-4e9d-85f0-96bb6c5bab6d/volumes"
Jan 25 19:48:07 minikube kubelet[1192]: W0125 19:48:07.358221    1192 sysinfo.go:203] Nodes topology is not available, providing CPU topology

* 
* ==> storage-provisioner [ad07d83721c9] <==
* I0125 19:14:12.876586       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0125 19:14:12.941305       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0125 19:14:12.942285       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0125 19:14:30.419753       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0125 19:14:30.420147       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_a5543247-16e7-4d96-a8c3-e6f9d2685d78!
I0125 19:14:30.419971       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"71e2617b-042a-42f5-bb01-72afe5a45cb0", APIVersion:"v1", ResourceVersion:"122830", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_a5543247-16e7-4d96-a8c3-e6f9d2685d78 became leader
I0125 19:14:30.522257       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_a5543247-16e7-4d96-a8c3-e6f9d2685d78!

* 
* ==> storage-provisioner [da84a87eff8d] <==
* I0125 19:13:29.953320       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0125 19:14:00.047035       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout

